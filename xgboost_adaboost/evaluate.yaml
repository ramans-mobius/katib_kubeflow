name: Evaluation
description: Evaluates trained boosting model and generates comprehensive metrics
inputs:
  - name: trained_model
    type: Model
  - name: data_path
    type: Dataset
  - name: feature_importance
    type: String
  - name: algorithm
    type: String
    description: 'Algorithm type: xgboost or adaboost'

outputs:
  - name: metrics
    type: Metrics
  - name: evaluation_results
    type: String
    description: "Complete evaluation results as JSON string"
  - name: performance_summary
    type: String
    description: "Performance summary as JSON string"

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        python -c "
        import sys, os, pickle, json, pandas as pd, numpy as np
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

        trained_model_path = sys.argv[1]
        data_path = sys.argv[2]
        feature_importance_path = sys.argv[3]
        algorithm = sys.argv[4]
        metrics_path = sys.argv[5]
        evaluation_results_path = sys.argv[6]
        performance_summary_path = sys.argv[7]

        print(f'Starting {algorithm.upper()} Evaluation')

        class DataWrapper:
            def __init__(self, data_dict=None):
                if data_dict:
                    self.__dict__.update(data_dict)

        # Load trained model and data
        if not os.path.exists(trained_model_path):
            raise FileNotFoundError(f'trained_model does not exist: {trained_model_path}')
            
        if not os.path.exists(data_path):
            raise FileNotFoundError(f'data_path does not exist: {data_path}')
            
        try:
            with open(trained_model_path, 'rb') as f:
                model_pipeline = pickle.load(f)
            print(f'Trained {algorithm} model loaded successfully')
        except Exception as e:
            raise Exception(f'ERROR loading trained model: {e}')
            
        try:
            with open(data_path, 'rb') as f:
                data_wrapper = pickle.load(f)
            print('Data loaded successfully')
        except Exception as e:
            raise Exception(f'ERROR loading data: {e}')

        # Load feature importance
        try:
            with open(feature_importance_path, 'r') as f:
                feature_importance_csv = f.read()
            feature_importance_df = pd.read_csv(pd.compat.StringIO(feature_importance_csv))
            print('Feature importance loaded successfully')
        except Exception as e:
            print(f'Warning loading feature importance: {e}')
            feature_importance_df = pd.DataFrame()

        # Extract test data
        try:
            X_test = data_wrapper.X_test
            y_test = data_wrapper.y_test
            dataset_info = data_wrapper.dataset_info
        except AttributeError:
            if hasattr(data_wrapper, '__dict__'):
                X_test = data_wrapper.__dict__.get('X_test')
                y_test = data_wrapper.__dict__.get('y_test')
                dataset_info = data_wrapper.__dict__.get('dataset_info', {})
            else:
                X_test = data_wrapper.get('X_test')
                y_test = data_wrapper.get('y_test')
                dataset_info = data_wrapper.get('dataset_info', {})

        if not dataset_info:
            dataset_info = {
                'feature_columns': list(X_test.columns) if X_test is not None else [],
                'target_column': 'salary'
            }

        print(f'Evaluating {algorithm} on {len(X_test)} test samples')

        # Make predictions
        y_pred = model_pipeline.predict(X_test)

        # Calculate comprehensive metrics
        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mse = mean_squared_error(y_test, y_pred)

        # Additional metrics
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        explained_variance = 1 - (np.var(y_test - y_pred) / np.var(y_test))

        # Percentage within different error ranges
        error_percentage_10 = np.mean(np.abs((y_test - y_pred) / y_test) <= 0.10) * 100
        error_percentage_20 = np.mean(np.abs((y_test - y_pred) / y_test) <= 0.20) * 100
        error_percentage_30 = np.mean(np.abs((y_test - y_pred) / y_test) <= 0.30) * 100

        # Calculate additional statistics
        actual_mean = float(np.mean(y_test))
        actual_std = float(np.std(y_test))
        predicted_mean = float(np.mean(y_pred))
        predicted_std = float(np.std(y_pred))
        actual_min = float(np.min(y_test))
        actual_max = float(np.max(y_test))
        predicted_min = float(np.min(y_pred))
        predicted_max = float(np.max(y_pred))

        # Create comprehensive evaluation results
        evaluation_results = {
            'algorithm': algorithm,
            'test_metrics': {
                'r2_score': float(r2),
                'mae': float(mae),
                'rmse': float(rmse),
                'mse': float(mse),
                'mape': float(mape),
                'explained_variance': float(explained_variance),
                'error_within_10pct': float(error_percentage_10),
                'error_within_20pct': float(error_percentage_20),
                'error_within_30pct': float(error_percentage_30)
            },
            'target_statistics': {
                'actual_mean': actual_mean,
                'actual_std': actual_std,
                'actual_min': actual_min,
                'actual_max': actual_max,
                'predicted_mean': predicted_mean,
                'predicted_std': predicted_std,
                'predicted_min': predicted_min,
                'predicted_max': predicted_max
            },
            'feature_analysis': {
                'top_features': feature_importance_df.head(10).to_dict('records') if not feature_importance_df.empty else [],
                'total_features': len(feature_importance_df) if not feature_importance_df.empty else 0,
                'most_important_feature': feature_importance_df.iloc[0].to_dict() if not feature_importance_df.empty else {}
            },
            'model_info': {
                'model_type': f'{algorithm}_regression',
                'total_samples': len(y_test),
                'features_used': dataset_info.get('feature_columns', []),
                'evaluation_timestamp': pd.Timestamp.now().isoformat()
            },
            'prediction_analysis': {
                'mean_absolute_error_percentage': float(mape),
                'mean_bias': float(np.mean(y_pred - y_test)),
                'std_bias': float(np.std(y_pred - y_test)),
                'correlation_coefficient': float(np.corrcoef(y_test, y_pred)[0, 1])
            }
        }

        # Create performance summary
        performance_summary = {
            'algorithm': algorithm,
            'performance_score': float(r2),
            'primary_metrics': {
                'r2_score': float(r2),
                'mae': float(mae),
                'rmse': float(rmse),
                'mape': float(mape)
            },
            'accuracy_bands': {
                'within_10_percent': float(error_percentage_10),
                'within_20_percent': float(error_percentage_20),
                'within_30_percent': float(error_percentage_30)
            },
            'data_summary': {
                'test_samples': len(y_test),
                'target_mean': actual_mean,
                'target_std': actual_std
            },
            'top_features': feature_importance_df.head(5).to_dict('records') if not feature_importance_df.empty else []
        }

        # Save outputs
        try:
            os.makedirs(os.path.dirname(metrics_path) or '.', exist_ok=True)
            os.makedirs(os.path.dirname(evaluation_results_path) or '.', exist_ok=True)
            os.makedirs(os.path.dirname(performance_summary_path) or '.', exist_ok=True)
            
            # Save detailed metrics (for Argo Metrics)
            with open(metrics_path, 'w') as f:
                json.dump(evaluation_results, f, indent=2)
            
            # Save complete evaluation results as JSON string
            with open(evaluation_results_path, 'w') as f:
                json.dump(evaluation_results, f, indent=2)
            
            # Save performance summary as JSON string
            with open(performance_summary_path, 'w') as f:
                json.dump(performance_summary, f, indent=2)
            
            print(f'{algorithm.upper()} Evaluation Complete')
            print('=== PERFORMANCE SUMMARY ===')
            print(f'RÂ² Score: {r2:.4f}')
            print(f'MAE: {mae:,.0f}')
            print(f'RMSE: {rmse:,.0f}')
            print(f'MAPE: {mape:.2f}%')
            print(f'Accuracy within 10%: {error_percentage_10:.1f}%')
            print(f'Accuracy within 20%: {error_percentage_20:.1f}%')
            print(f'Samples evaluated: {len(y_test)}')
            
            if not feature_importance_df.empty:
                print('Top 3 Features:')
                for i, row in feature_importance_df.head(3).iterrows():
                    print(f'  {row[\"feature\"]}: {row[\"importance\"]:.4f}')
            
        except Exception as e:
            raise Exception(f'ERROR saving results: {e}')

        print(f'{algorithm.upper()} evaluation completed successfully!')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7"
    args:
      - {inputPath: trained_model}
      - {inputPath: data_path}
      - {inputPath: feature_importance}
      - {inputValue: algorithm}
      - {outputPath: metrics}
      - {outputPath: evaluation_results}
      - {outputPath: performance_summary}
