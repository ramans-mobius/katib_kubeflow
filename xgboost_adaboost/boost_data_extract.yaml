name: 2 2 Extract Dataset
description: Extracts specific dataset based on dataset name and algorithm
inputs:
  - name: cdn_url
    type: String
    description: URL to fetch the data.txt file from
  - name: dataset_name
    type: String
    description: Target dataset name to extract
  - name: algorithm
    type: String
    description: Algorithm to be used for processing
  - name: debug_info
    type: String
    description: Debug information from previous step
outputs:
  - name: extracted_data
    type: String
    description: Extracted dataset in JSON format
  - name: extraction_report
    type: String
    description: Report on extraction process
  - name: sample_records
    type: String
    description: Sample records from extracted data
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        python3 -m pip install --quiet requests pandas
        python3 -c "
        import sys, os, requests, json, pandas as pd, urllib.parse
        
        print('Number of arguments:', len(sys.argv))
        print('Arguments:', sys.argv)
        
        cdn_url = sys.argv[1]
        dataset_name = sys.argv[2]
        algorithm = sys.argv[3]
        debug_info_str = sys.argv[4]
        extracted_data_path = sys.argv[5]
        extraction_report_path = sys.argv[6]
        sample_records_path = sys.argv[7]
        
        print('Starting dataset extraction...')
        print(f'cdn_url: {cdn_url}')
        print(f'dataset_name: {dataset_name}')
        print(f'algorithm: {algorithm}')
        
        debug_info = json.loads(debug_info_str)
        decoded_url = urllib.parse.unquote(cdn_url)
        response = requests.get(decoded_url)
        response.raise_for_status()
        content = response.text
        lines = content.split('\n')
        
        dataset_sections = {
            'NetworkX-Graph-Intialize': ['node', 'degree', 'pagerank', 'reachable', 'betweenness'],
            'LineChart-Generation-Dbaas': ['x', 'y', 'chart_type'],
            'LLM-MODEL-BUILDING-TRAINING': ['text', 'tokens', 'length', 'epoch'],
            'Mig-Setup-Single-Playbook': ['GPU', 'mig_profile', 'batch', 'mem', 'runtime'],
            'GNN-STGNN': ['node_id', 'timestamp', 'cpu_usage', 'latency']
        }
        
        target_columns = dataset_sections.get(dataset_name, [])
        extracted_records = []
        
        for line in lines:
            line = line.strip()
            if line and any(col in line for col in target_columns):
                try:
                    record = json.loads(line)
                    extracted_records.append(record)
                except:
                    continue
        
        extraction_report = {
            'dataset_name': dataset_name,
            'algorithm': algorithm,
            'records_found': len(extracted_records),
            'target_columns': target_columns,
            'sample_size': min(5, len(extracted_records))
        }
        
        sample_records = extracted_records[:5] if extracted_records else []
        
        os.makedirs(os.path.dirname(extracted_data_path) or '.', exist_ok=True)
        with open(extracted_data_path, 'w') as f:
            json.dump(extracted_records, f)
        
        os.makedirs(os.path.dirname(extraction_report_path) or '.', exist_ok=True)
        with open(extraction_report_path, 'w') as f:
            json.dump(extraction_report, f)
        
        os.makedirs(os.path.dirname(sample_records_path) or '.', exist_ok=True)
        with open(sample_records_path, 'w') as f:
            json.dump(sample_records, f)
        
        print('Dataset extraction completed successfully')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7"
    args:
      - {inputValue: cdn_url}
      - {inputValue: dataset_name}
      - {inputValue: algorithm}
      - {inputPath: debug_info}
      - {outputPath: extracted_data}
      - {outputPath: extraction_report}
      - {outputPath: sample_records}
