name: SVT DQN RLAF Loop CNN
description: Triggers the DQN RLAF pipeline in a loop to optimize CNN model hyperparameters, controlled by a pierce_or_not flag.
inputs:
  - {name: trained_model, type: Model}
  - {name: init_metrics, type: Metrics}
  - {name: data_path, type: Dataset}
  - {name: config, type: String}
  - {name: domain, type: String}
  - {name: schema_id, type: String}
  - {name: model_id, type: String}
  - {name: dqn_pipeline_id, type: String}
  - {name: pipeline_domain, type: String}
  - {name: dqn_experiment_id, type: String}
  - {name: access_token, type: string}
  - {name: tasks, type: Dataset}
outputs:
  - {name: rlaf_output, type: Dataset}
  - {name: retrained_model, type: Model}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v19
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests || \
        python3 -m pip install --quiet requests --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        import pickle
        import time
        import numpy as np
        from typing import Dict, List, Any
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        from torch.utils.data import DataLoader
        import torch.nn as nn
        import torch.optim as optim
        from nesy_factory.CNNs.factory import CNNFactory

        def get_retry_session():
            retry_strategy = Retry(
                total=5,
                status_forcelist=[500, 502, 503, 504],
                backoff_factor=1
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session

        def trigger_pipeline(config, pipeline_domain, dqn_params=None):
            print(f"DEBUG: Triggering DQN pipeline with config: {config}")
            try:
                http = get_retry_session()
                url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
                pipeline_params = {"param_json": json.dumps(dqn_params)} if dqn_params else {}
                payload = json.dumps({
                    "pipelineType": "ML", 
                    "containerResources": {}, 
                    "experimentId": config['experiment_id'],
                    "enableCaching": True, 
                    "parameters": pipeline_params, 
                    "version": 1
                })
                headers = {
                    'accept': 'application/json', 
                    'Authorization': f"Bearer {config['access_token']}",
                    'Content-Type': 'application/json'
                }
                print(f"DEBUG: Sending request to: {url}")
                print(f"DEBUG: Payload: {payload}")
                response = http.post(url, headers=headers, data=payload, timeout=30)
                response.raise_for_status()
                result = response.json()
                print(f"DEBUG: DQN pipeline triggered successfully. Run ID: {result['runId']}")
                return result['runId']
            except Exception as e:
                print(f"ERROR: Failed to trigger DQN pipeline: {e}")
                raise

        def get_pipeline_status(config, pipeline_domain):
            print(f"DEBUG: Checking pipeline status for run ID: {config['run_id']}")
            try:
                http = get_retry_session()
                url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
                headers = {
                    'accept': 'application/json', 
                    'Authorization': f"Bearer {config['access_token']}"
                }
                response = http.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                pipeline_status = response.json()
                latest_state = pipeline_status['run_details']['state_history'][-1]
                print(f"DEBUG: DQN pipeline status: {latest_state['state']}")
                return latest_state['state']
            except Exception as e:
                print(f"ERROR: Failed to get pipeline status: {e}")
                raise

        def get_instance(access_token, domain, schema_id, model_id):
            print(f"DEBUG: Getting instance for model_id: {model_id}")
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            headers = {
                "Authorization": f"Bearer {access_token}", 
                "Content-Type": "application/json"
            }
            payload = {
                "dbType": "TIDB", 
                "ownedOnly": True, 
                "filter": {"model_id": model_id}
            }
            print(f"DEBUG: Sending request to: {url}")
            print(f"DEBUG: Payload: {payload}")
            response = http.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            print(f"DEBUG: Instance response: {data}")
            if not data['content']:
                raise ValueError(f"No instance found for model_id: {model_id}")
            instance = data['content'][0]
            print(f"DEBUG: Retrieved instance: {instance.get('model_id')}")
            return instance

        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
            print(f"DEBUG: Updating instance field: {field} for model_id: {model_id}")
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {
                "Authorization": f"Bearer {access_token}", 
                "Content-Type": "application/json"
            }
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {
                    "conditions": [{
                        "field": "model_id", 
                        "operator": "EQUAL", 
                        "value": model_id
                    }]
                },
                "partialUpdateRequests": [{
                    "patch": [{
                        "operation": "REPLACE", 
                        "path": f"{field}", 
                        "value": value
                    }]
                }]
            }
            print(f"DEBUG: Sending update request to: {url}")
            print(f"DEBUG: Update payload: {payload}")
            response = http.patch(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()
            print(f"DEBUG: Instance field {field} updated successfully")

        class CNNContinualTrainer:
            def __init__(self, config: Dict[str, Any]):
                self.config = config
                self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                print(f"DEBUG: CNNContinualTrainer initialized with device: {self.device}")
                
            def train_continual_cnn(self, tasks: List[Dict], model, strategies: List[str] = ['naive']) -> Dict[str, Any]:
                print(f"DEBUG: Starting continual training with {len(tasks)} tasks and strategies: {strategies}")
                results = {}
                for strategy_name in strategies:
                    print(f"DEBUG: Training with strategy: {strategy_name}")
                    strategy_results = self._train_single_strategy(tasks, strategy_name, model)
                    results[strategy_name] = strategy_results
                return results
            
            def _train_single_strategy(self, tasks: List[Dict], strategy_name: str, model) -> Dict[str, Any]:
                print(f"DEBUG: Starting single strategy training: {strategy_name}")
                task_metrics = []
                all_task_performance = []
                previous_task_data = []
                
                model.to(self.device)
                print(f"DEBUG: Model moved to device: {self.device}")
                
                for task_idx, task_data in enumerate(tasks):
                    print(f"DEBUG: Learning Task {task_idx + 1}")
                    
                    if strategy_name == 'naive':
                        training_loader = task_data['train_loader']
                    elif strategy_name == 'replay' and previous_task_data:
                        training_loader = self._create_replay_loader(task_data, previous_task_data)
                    else:
                        training_loader = task_data['train_loader']
                    
                    current_metrics = self._train_cnn_on_task(model, training_loader, task_data['test_loader'])
                    task_metrics.append(current_metrics)
                    
                    task_performance = []
                    for eval_task_idx in range(task_idx + 1):
                        eval_metrics = self._evaluate_cnn_on_task(model, tasks[eval_task_idx]['test_loader'])
                        task_performance.append({
                            'task_id': eval_task_idx,
                            'accuracy': eval_metrics['accuracy'],
                            'loss': eval_metrics['loss']
                        })
                    
                    all_task_performance.append(task_performance)
                    
                    if strategy_name == 'replay':
                        previous_task_data.append(task_data)
                        if len(previous_task_data) > 2:
                            previous_task_data = previous_task_data[-2:]
                
                cl_metrics = self._calculate_continual_metrics(all_task_performance)
                
                final_eval_metrics = []
                for i in range(len(tasks)):
                    task_eval = self._evaluate_cnn_on_task(model, tasks[i]['test_loader'])
                    final_eval_metrics.append(task_eval)
                
                avg_metrics = {}
                if final_eval_metrics:
                    for key in final_eval_metrics[0]:
                        avg_metrics[key] = np.mean([m[key] for m in final_eval_metrics if key in m])

                print(f"DEBUG: Strategy {strategy_name} completed with avg metrics: {avg_metrics}")
                return {
                    'strategy': strategy_name,
                    'task_metrics': task_metrics,
                    'all_task_performance': all_task_performance,
                    'continual_metrics': cl_metrics,
                    'final_model': model,
                    'average_eval_metrics': avg_metrics
                }
            
            def _train_cnn_on_task(self, model, train_loader, test_loader) -> Dict[str, float]:
                print(f"DEBUG: Training CNN on task")
                model.train()
                model.to(self.device)
                
                training_config = self.config.get('training', {})
                optimizer_config = training_config.get('optimizer', {})
                
                learning_rate = optimizer_config.get('learning_rate', 0.001)
                weight_decay = optimizer_config.get('weight_decay', 0.01)
                epochs = training_config.get('epochs', 5)
                
                print(f"DEBUG: Training config - lr: {learning_rate}, wd: {weight_decay}, epochs: {epochs}")
                
                optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
                criterion = nn.CrossEntropyLoss()
                
                for epoch in range(epochs):
                    total_loss = 0
                    correct = 0
                    total = 0
                    
                    for data, targets in train_loader:
                        data, targets = data.to(self.device), targets.to(self.device)
                        optimizer.zero_grad()
                        outputs = model(data)
                        loss = criterion(outputs, targets)
                        loss.backward()
                        optimizer.step()
                        
                        total_loss += loss.item()
                        _, predicted = outputs.max(1)
                        total += targets.size(0)
                        correct += predicted.eq(targets).sum().item()
                
                eval_metrics = self._evaluate_cnn_on_task(model, test_loader)
                print(f"DEBUG: Training completed with metrics: {eval_metrics}")
                return eval_metrics
            
            def _evaluate_cnn_on_task(self, model, test_loader) -> Dict[str, float]:
                print(f"DEBUG: Evaluating CNN on task")
                model.eval()
                model.to(self.device)
                
                criterion = nn.CrossEntropyLoss()
                total_loss = 0
                correct = 0
                total = 0
                
                with torch.no_grad():
                    for data, targets in test_loader:
                        data, targets = data.to(self.device), targets.to(self.device)
                        outputs = model(data)
                        loss = criterion(outputs, targets)
                        
                        total_loss += loss.item()
                        _, predicted = outputs.max(1)
                        total += targets.size(0)
                        correct += predicted.eq(targets).sum().item()
                
                accuracy = 100. * correct / total if total > 0 else 0.0
                avg_loss = total_loss / len(test_loader) if len(test_loader) > 0 else 0.0
                
                metrics = {
                    'accuracy': accuracy,
                    'loss': avg_loss,
                    'correct': correct,
                    'total': total
                }
                print(f"DEBUG: Evaluation metrics: {metrics}")
                return metrics
            
            def _create_replay_loader(self, current_task: Dict, previous_tasks: List[Dict]) -> DataLoader:
                from torch.utils.data import TensorDataset
                print(f"DEBUG: Creating replay loader with {len(previous_tasks)} previous tasks")
                
                replay_ratio = 0.2
                current_loader = current_task['train_loader']
                current_size = len(current_loader.dataset)
                replay_size = int(current_size * replay_ratio / (1 - replay_ratio))
                
                replay_data = []
                replay_labels = []
                
                for prev_task in previous_tasks:
                    prev_loader = prev_task['train_loader']
                    prev_dataset = prev_loader.dataset
                    if len(prev_dataset) > 0:
                        sample_size = min(replay_size // len(previous_tasks), len(prev_dataset))
                        indices = torch.randperm(len(prev_dataset))[:sample_size]
                        for idx in indices:
                            data, label = prev_dataset[idx]
                            replay_data.append(data)
                            replay_labels.append(label)
                
                current_data = []
                current_labels = []
                for data, label in current_loader.dataset:
                    current_data.append(data)
                    current_labels.append(label)
                
                if replay_data:
                    combined_data = torch.cat([torch.stack(current_data), torch.stack(replay_data)])
                    combined_labels = torch.cat([torch.tensor(current_labels), torch.tensor(replay_labels)])
                else:
                    combined_data = torch.stack(current_data)
                    combined_labels = torch.tensor(current_labels)
                
                combined_dataset = TensorDataset(combined_data, combined_labels)
                return DataLoader(combined_dataset, batch_size=current_loader.batch_size, shuffle=True)
            
            def _calculate_continual_metrics(self, all_task_performance: List[List[Dict]]) -> Dict[str, float]:
                print(f"DEBUG: Calculating continual metrics from {len(all_task_performance)} task performances")
                final_performance = all_task_performance[-1]
                average_accuracy = np.mean([task['accuracy'] for task in final_performance])
                average_loss = np.mean([task['loss'] for task in final_performance])
                
                metrics = {
                    'average_accuracy': average_accuracy,
                    'average_loss': average_loss,
                    'num_tasks': len(all_task_performance)
                }
                print(f"DEBUG: Continual metrics: {metrics}")
                return metrics

        def cnn_retraining(action, model_path, data_path, config_str, tasks_path, output_model_path, previous_metrics, dqn_params):
            print("DEBUG: Starting CNN retraining function")
            print(f"DEBUG: Model path: {model_path}")
            print(f"DEBUG: Output model path: {output_model_path}")
            print(f"DEBUG: Action received: {action}")
            
            config = json.loads(config_str)
            print(f"DEBUG: Config loaded: {list(config.keys())}")
            
            print("DEBUG: Loading tasks...")
            with open(tasks_path, "rb") as f:
                tasks = pickle.load(f)
            print(f"DEBUG: Loaded {len(tasks)} tasks")
            
            model_config = config.get('model', {})
            training_config = config.get('training', {})
            print(f"DEBUG: Model config: {model_config}")
            print(f"DEBUG: Training config: {training_config}")
            
            print("DEBUG: Applying action parameters to config")
            for param_key, param_value in action.items():
                print(f"DEBUG: Processing parameter: {param_key} = {param_value}")
                if 'learning_rate' in param_key:
                    training_config['optimizer']['learning_rate'] = float(param_value)
                    print(f"DEBUG: Set learning_rate to {param_value}")
                elif 'batch_size' in param_key:
                    training_config['batch_size'] = int(param_value)
                    print(f"DEBUG: Set batch_size to {param_value}")
                elif 'epochs' in param_key:
                    training_config['epochs'] = int(param_value)
                    print(f"DEBUG: Set epochs to {param_value}")
                elif 'architecture' in param_key:
                    model_config['architecture'] = param_value
                    print(f"DEBUG: Set architecture to {param_value}")
                elif 'variant' in param_key:
                    model_config['variant'] = param_value
                    print(f"DEBUG: Set variant to {param_value}")
            
            print(f"DEBUG: Creating model with architecture: {model_config['architecture']}")
            model = CNNFactory.create_model(model_config['architecture'], model_config)
            print(f"DEBUG: Model created: {type(model)}")
            
            print("DEBUG: Loading pre-trained weights...")
            print(f"DEBUG: Model file exists: {os.path.exists(model_path)}")
            
            checkpoint = torch.load(model_path, map_location=torch.device('cpu'))
            print(f"DEBUG: Checkpoint type: {type(checkpoint)}")
            print(f"DEBUG: Checkpoint keys: {list(checkpoint.keys()) if isinstance(checkpoint, dict) else 'Not a dict'}")
            
            if isinstance(checkpoint, dict):
                if 'model_state_dict' in checkpoint:
                    print("DEBUG: Found model_state_dict in checkpoint")
                    model.load_state_dict(checkpoint['model_state_dict'])
                    print("DEBUG: Loaded model from model_state_dict")
                elif 'state_dict' in checkpoint:
                    print("DEBUG: Found state_dict in checkpoint")
                    model.load_state_dict(checkpoint['state_dict'])
                    print("DEBUG: Loaded model from state_dict")
                else:
                    print("DEBUG: No standard keys found, attempting to load checkpoint as direct state dict")
                    model.load_state_dict(checkpoint)
                    print("DEBUG: Loaded model from direct state dict")
            else:
                print("DEBUG: Checkpoint is not a dict, attempting to load as direct state dict")
                model.load_state_dict(checkpoint)
                print("DEBUG: Loaded model from direct state dict")
            
            print("DEBUG: Starting continual training...")
            trainer = CNNContinualTrainer(config)
            results = trainer.train_continual_cnn(tasks=tasks, strategies=['naive'], model=model)
            
            average_eval_metrics = results['naive']['average_eval_metrics']
            print(f"DEBUG: Training completed. Average metrics: {average_eval_metrics}")
            
            improvement_score = 0
            print("DEBUG: Calculating improvement score...")
            for param in dqn_params:
                key = param['key']
                sign = 1 if param['sign'] == '+' else -1
                if key in average_eval_metrics and key in previous_metrics:
                    improvement = (average_eval_metrics[key] - previous_metrics[key]) * sign
                    improvement_score += improvement * param.get('mul', 1.0)
                    print(f"DEBUG: Key {key}: current={average_eval_metrics[key]}, previous={previous_metrics[key]}, improvement={improvement}")

            print(f"DEBUG: Final improvement score: {improvement_score:.4f}")
            
            os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
            print(f"DEBUG: Output directory prepared: {os.path.dirname(output_model_path)}")
            
            if improvement_score > 0:
                print("DEBUG: Improvement detected - saving retrained model")
                final_model = results['naive']['final_model']
                torch.save(final_model.state_dict(), output_model_path)
                print(f"DEBUG: Retrained model saved to: {output_model_path}")
            else:
                print("DEBUG: No improvement - saving original model state")
                torch.save(model.state_dict(), output_model_path)
                print(f"DEBUG: Original model saved to: {output_model_path}")

            return {"metrics": average_eval_metrics, "model_path": output_model_path}

        def trigger_and_wait_for_dqn_pipeline(config, pipeline_domain, dqn_params):
            print("DEBUG: Starting DQN pipeline trigger and wait")
            try:
                run_id = trigger_pipeline(config, pipeline_domain, dqn_params)
                config["run_id"] = run_id
                
                max_wait_time = 1800
                start_time = time.time()
                check_count = 0
                
                while time.time() - start_time < max_wait_time:
                    check_count += 1
                    print(f"DEBUG: Checking pipeline status (attempt {check_count})")
                    status = get_pipeline_status(config, pipeline_domain)
                    
                    if status == 'SUCCEEDED':
                        print("DEBUG: DQN pipeline completed successfully")
                        return True
                    elif status in ['FAILED', 'ERROR', 'CANCELLED']:
                        print(f"ERROR: DQN pipeline failed with status: {status}")
                        raise RuntimeError(f"DQN pipeline failed with status: {status}")
                    
                    print(f"DEBUG: Pipeline still running, waiting 30 seconds...")
                    time.sleep(30)
                
                print("ERROR: DQN pipeline timeout")
                raise RuntimeError("DQN pipeline timeout after 30 minutes")
                
            except Exception as e:
                print(f"ERROR: Error in DQN pipeline execution: {e}")
                raise

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            parser.add_argument('--pipeline_domain', type=str, required=True)
            parser.add_argument('--retrained_model', type=str, required=True)
            args = parser.parse_args()

            print("DEBUG: Starting CNN RLAF Loop main function")
            print(f"DEBUG: All input arguments parsed successfully")
            
            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            print("DEBUG: Access token loaded")
            
            with open(args.init_metrics, 'r') as f:
                current_metrics_data = json.load(f)
            print(f"DEBUG: Initial metrics loaded: {current_metrics_data}")
            
            current_metrics = {}
            if isinstance(current_metrics_data, dict):
                for key, value in current_metrics_data.items():
                    if isinstance(value, (int, float)):
                        current_metrics[key] = float(value)
            else:
                current_metrics = {'accuracy': 50.0, 'loss': 1.0}
            print(f"DEBUG: Processed metrics: {current_metrics}")
            
            for i in range(2):
                print(f"DEBUG: Starting RLAF iteration {i+1}")
                
                cleaned_metrics = {}
                dqn_params = []
                print("DEBUG: Preparing metrics for DQN")
                for key, value in current_metrics.items():
                    cleaned_metrics[key] = float(value)
                    if any(term in key.lower() for term in ["accuracy", "f1", "precision", "recall"]):
                        sign = "+"
                    else:
                        sign = "-"
                    dqn_params.append({"key": key, "sign": sign, "mul": 1.0})
                
                print(f"DEBUG: DQN parameters prepared: {json.dumps(dqn_params, indent=2)}")
                
                try:
                    print("DEBUG: Attempting to get instance from database")
                    instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                    print(f"DEBUG: Instance retrieved: {instance.get('model_id')}")
                    
                    if instance.get('pierce2rlaf'):
                        latest_pierce2rlaf = instance['pierce2rlaf'][-1]
                        previous_state = latest_pierce2rlaf['current_state']
                        print("DEBUG: Found existing pierce2rlaf history")
                    else:
                        previous_state = {key: 0.0 for key in cleaned_metrics.keys()}
                        print("DEBUG: No pierce2rlaf history found, creating default previous_state")
                    
                    new_pierce2rlaf_entry = {
                        "action_id": -1, 
                        "previous_state": previous_state,
                        "current_state": cleaned_metrics, 
                        "episode": i, 
                        "timestamp": int(time.time())
                    }
                    
                    pierce2rlaf_history = instance.get("pierce2rlaf", [])
                    pierce2rlaf_history.append(new_pierce2rlaf_entry)
                    print(f"DEBUG: Updating pierce2rlaf with {len(pierce2rlaf_history)} entries")
                    
                    update_instance_field(access_token, args.domain, args.schema_id, args.model_id, 
                                        "pierce2rlaf", pierce2rlaf_history)
                    print("DEBUG: Database update completed")
                    
                except Exception as e:
                    print(f"ERROR: Database update failed: {str(e)}")
                    raise
                
                try:
                    print("DEBUG: Attempting to trigger DQN pipeline")
                    dqn_config = {
                        "pipeline_id": args.dqn_pipeline_id, 
                        "experiment_id": args.dqn_experiment_id, 
                        "access_token": access_token
                    }
                    dqn_success = trigger_and_wait_for_dqn_pipeline(dqn_config, args.pipeline_domain, dqn_params)
                    
                    if dqn_success:
                        print("DEBUG: DQN pipeline succeeded, fetching recommendations")
                        updated_instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                        
                        if updated_instance.get('rlaf2pierce'):
                            latest_rlaf2pierce = updated_instance['rlaf2pierce'][-1]
                            print(f"DEBUG: Latest rlaf2pierce: {latest_rlaf2pierce}")
                            
                            if latest_rlaf2pierce.get("pierce_or_not", True):
                                rlaf_actions = updated_instance.get('rlaf_actions', {}).get('actions', [])
                                action_id = latest_rlaf2pierce['action_id']
                                print(f"DEBUG: Looking for action_id: {action_id} in {len(rlaf_actions)} actions")
                                
                                action_details = next((a for a in rlaf_actions if a["id"] == action_id), None)
                                
                                if action_details:
                                    action_to_use = action_details['params']
                                    print(f"DEBUG: Using DQN action: {action_to_use}")
                                else:
                                    print("ERROR: DQN action not found in rlaf_actions")
                                    raise ValueError(f"Action ID {action_id} not found in rlaf_actions")
                            else:
                                print("DEBUG: pierce_or_not is false. Stopping RLAF loop.")
                                break
                        else:
                            print("ERROR: No rlaf2pierce data found after DQN pipeline")
                            raise ValueError("No rlaf2pierce recommendations received from DQN")
                    else:
                        print("ERROR: DQN pipeline execution failed")
                        raise RuntimeError("DQN pipeline failed to complete successfully")
                        
                except Exception as e:
                    print(f"ERROR: DQN pipeline error: {str(e)}")
                    raise
                
                print(f"DEBUG: Proceeding with retraining using action: {action_to_use}")
                retraining_results = cnn_retraining(
                    action_to_use, 
                    args.trained_model, 
                    args.data_path, 
                    args.config, 
                    args.tasks,
                    args.retrained_model, 
                    previous_state, 
                    dqn_params
                )
                
                current_metrics = retraining_results["metrics"]
                print(f"DEBUG: Retraining completed. New metrics: {current_metrics}")
            
            print("DEBUG: Saving final results...")
            os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
            final_output = {
                "final_metrics": current_metrics,
                "model_type": "CNN_image_classifier",
                "iterations_completed": i + 1,
                "timestamp": time.time()
            }
            
            with open(args.rlaf_output, 'w') as f:
                json.dump(final_output, f, indent=2)
            
            print(f"DEBUG: Final results saved to: {args.rlaf_output}")
            print("DEBUG: CNN RLAF loop completed")

        if __name__ == '__main__':
            main()
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}
      - --tasks
      - {inputPath: tasks}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --retrained_model
      - {outputPath: retrained_model}
