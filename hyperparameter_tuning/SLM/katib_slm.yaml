name: Katib Hyperparameter Tuning
description: Runs a Katib hyperparameter tuning experiment for LLaMA fine-tuning and saves the best hyperparameters.

inputs:
  - name: experiment_name
    type: String
    default: llama-katib-exp
  - name: max_trial_count
    type: Integer
    default: 2
  - name: parallel_trial_count
    type: Integer
    default: 1
  - name: objective_metric_name
    type: String
    default: train_loss
  - name: objective_type
    type: String
    default: minimize
  - name: algorithm_name
    type: String
    default: random

outputs:
  - name: best_hyperparams
    type: String
    description: Path to store the best hyperparameters JSON file

implementation:
  container:
    image: sanram00/slm_katib_image:latest
    command:
      - python3
      - -u
      - -c
      - |
        import json, os, argparse
        from kubeflow.katib import KatibClient
        import kubeflow.katib as katib

        parser = argparse.ArgumentParser()
        parser.add_argument("--experiment_name", type=str, required=True)
        parser.add_argument("--max_trial_count", type=int, required=True)
        parser.add_argument("--parallel_trial_count", type=int, required=True)
        parser.add_argument("--objective_metric_name", type=str, required=True)
        parser.add_argument("--objective_type", type=str, required=True)
        parser.add_argument("--algorithm_name", type=str, required=True)
        parser.add_argument("--best_hyperparams", type=str, required=True)
        args = parser.parse_args()

        cl = KatibClient(namespace="kubeflow")

        # Example: run a dummy tuning (replace with real search space)
        cl.tune(
            name=args.experiment_name,
            objective_metric_name=args.objective_metric_name,
            objective_type=args.objective_type,
            algorithm_name=args.algorithm_name,
            max_trial_count=args.max_trial_count,
            parallel_trial_count=args.parallel_trial_count,
            resources_per_trial=katib.TrainerResources(
                num_workers=1,
                num_procs_per_worker=1,
                resources_per_worker={"cpu": 1, "memory": "2Gi"},
            ),
        )

        cl.wait_for_experiment_condition(name=args.experiment_name)

        best = cl.get_optimal_hyperparameters(args.experiment_name)

        os.makedirs(os.path.dirname(args.best_hyperparams), exist_ok=True)
        with open(args.best_hyperparams, "w") as f:
            json.dump(best, f, indent=2)
        print(f"Best hyperparameters saved at: {args.best_hyperparams}")

    args:
      - --experiment_name
      - {inputValue: experiment_name}
      - --max_trial_count
      - {inputValue: max_trial_count}
      - --parallel_trial_count
      - {inputValue: parallel_trial_count}
      - --objective_metric_name
      - {inputValue: objective_metric_name}
      - --objective_type
      - {inputValue: objective_type}
      - --algorithm_name
      - {inputValue: algorithm_name}
      - --best_hyperparams
      - {outputPath: best_hyperparams}


