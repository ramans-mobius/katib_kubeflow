name: Load and Split IMDB Dataset
description: Downloads IMDB dataset, splits into train/val/test, and saves metadata as JSON.

outputs:
  - {name: imdb_data, type: Dataset}

implementation:
  container:
    image: sanram00/slm_katib_image:latest
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet datasets scikit-learn || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet datasets scikit-learn --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import json
        from datasets import load_dataset
        from sklearn.model_selection import train_test_split

        # Output path for Elyra/KFP
        output_path = "{{$.outputs.artifacts['imdb_data'].path}}"
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # Download IMDB dataset from Hugging Face
        dataset = load_dataset("imdb")

        texts = dataset['train']['text'] + dataset['test']['text']
        labels = dataset['train']['label'] + dataset['test']['label']

        # Split: 70% train, 15% val, 15% test
        X_train, X_temp, y_train, y_temp = train_test_split(texts, labels, test_size=0.3, random_state=42, stratify=labels)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

        data = {
            "train": {"text": X_train, "labels": y_train},
            "val": {"text": X_val, "labels": y_val},
            "test": {"text": X_test, "labels": y_test}
        }

        with open(output_path, "w") as f:
            json.dump(data, f)

        print(f"[SUCCESS] IMDB dataset split saved to {output_path}")

