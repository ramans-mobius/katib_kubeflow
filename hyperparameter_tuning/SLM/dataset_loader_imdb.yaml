name: Load and Split IMDB Dataset
description: Downloads the IMDB dataset, splits into train/val/test, and outputs as JSON for downstream Katib experiments.

outputs:
  - {name: imdb_data, type: Dataset}

implementation:
  container:
    image: sanram00/slm_katib_image:latest
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet datasets scikit-learn || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet datasets scikit-learn --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import json
        from datasets import load_dataset
        from sklearn.model_selection import train_test_split

        output_path = "{{$.outputs.artifacts['imdb_data'].path}}"
        output_dir = os.path.dirname(output_path) or "/tmp/outputs/imdb_data"
        os.makedirs(output_dir, exist_ok=True)

        dataset = load_dataset("imdb")

        train_texts = dataset["train"]["text"]
        train_labels = dataset["train"]["label"]
        test_texts = dataset["test"]["text"]
        test_labels = dataset["test"]["label"]

        texts = list(train_texts) + list(test_texts)
        labels = list(train_labels) + list(test_labels)

        X_train, X_temp, y_train, y_temp = train_test_split(
            texts, labels, test_size=0.3, random_state=42, stratify=labels
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
        )

        data = {
            "train": {"text": X_train, "labels": y_train},
            "val": {"text": X_val, "labels": y_val},
            "test": {"text": X_test, "labels": y_test}
        }

        with open(output_path, "w") as f:
            json.dump(data, f)

        print(f"[SUCCESS] IMDB dataset split and saved at: {output_path}")



