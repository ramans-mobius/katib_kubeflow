apiVersion: v1
kind: ComponentSpec
metadata:
  name: SLM Dataset Loader
  description: Loads IMDB dataset using SLM Docker image, splits into train/validation/test, and outputs metadata as JSON.

inputs:
  - name: validation_split
    type: Float
    default: 0.1
    description: Fraction of dataset for validation
  - name: test_split
    type: Float
    default: 0.1
    description: Fraction of dataset for test

outputs:
  - name: output_metadata
    type: String
    description: Path to JSON file with dataset info

implementation:
  container:
    image: sanram00/slm_katib_image:latest
    command:
      - sh
      - -c
      - |
        python - << 'EOF'
        import json
        from datasets import load_dataset
        import argparse
        import os

        parser = argparse.ArgumentParser()
        parser.add_argument('--validation_split', type=float, default={{inputs.validation_split}})
        parser.add_argument('--test_split', type=float, default={{inputs.test_split}})
        parser.add_argument('--output_metadata', type=str, default='{{outputs.output_metadata}}')
        args = parser.parse_args()

        dataset = load_dataset("imdb")

        total = len(dataset['train'])
        val_size = int(total * args.validation_split)
        test_size = int(total * args.test_split)
        train_size = total - val_size - test_size

        train_data = dataset['train'].select(range(train_size))
        val_data = dataset['train'].select(range(train_size, train_size + val_size))
        test_data = dataset['test']

        metadata = {
            "train_samples": len(train_data),
            "validation_samples": len(val_data),
            "test_samples": len(test_data),
            "dataset_name": "imdb"
        }

        os.makedirs(os.path.dirname(args.output_metadata), exist_ok=True)
        with open(args.output_metadata, "w") as f:
            json.dump(metadata, f)

        print(f"Saved dataset metadata to {args.output_metadata}")
        EOF
