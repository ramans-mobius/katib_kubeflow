name: saay CNN Model Evaluator
description: Comprehensive evaluation of trained CNN model
inputs:
  - name: trained_model_path
    type: String
  - name: processed_data_path
    type: String
  - name: model_config
    type: String
outputs:
  - name: evaluation_results
    type: String
  - name: test_accuracy
    type: String
  - name: confusion_matrix
    type: String
implementation:
  container:
    image: nikhilv215/nesy-factory:v18
    command:
    - python3
    - -u
    - -c
    - |
      import os
      import sys
      import json
      import torch
      import torch.nn as nn
      import numpy as np
      from pathlib import Path
      import pickle
      from sklearn.metrics import confusion_matrix, classification_report
      
      print("CNN MODEL EVALUATOR")

      trained_model_path = sys.argv[1]
      processed_data_path = sys.argv[2]
      model_config_str = sys.argv[3]
      evaluation_results_file = sys.argv[4]
      test_accuracy_file = sys.argv[5]
      confusion_matrix_file = sys.argv[6]
      
      model_config = json.loads(model_config_str)
      
      print(f"Trained model path: {trained_model_path}")
      print(f"Data path: {processed_data_path}")
      
      try:
          from nesy_factory.CNNs.factory import CNNFactory
          from nesy_factory.utils import get_device
          
          device = get_device()
          
          print("Loading trained model...")
          checkpoint = torch.load(trained_model_path, map_location=device)
          
          model_config_eval = model_config.get('model', {})
          model = CNNFactory.create_model(model_config_eval.get('architecture', 'basecnn'), model_config_eval)
          model.load_state_dict(checkpoint['model_state_dict'])
          model = model.to(device)
          
          print("Loading test data...")
          with open(processed_data_path, 'rb') as f:
              data_wrapper = pickle.load(f)
          
          val_loader = data_wrapper.test_loader
          
          print(f"Test batches: {len(val_loader)}")
          
          print("Running evaluation...")
          model.eval()
          criterion = nn.CrossEntropyLoss()
          
          all_predictions = []
          all_labels = []
          total_loss = 0.0
          
          with torch.no_grad():
              for images, labels in val_loader:
                  if images.numel() == 0:
                      continue
                  images, labels = images.to(device), labels.to(device)
                  outputs = model(images)
                  
                  loss = criterion(outputs, labels)
                  total_loss += loss.item()
                  
                  _, predicted = outputs.max(1)
                  
                  all_predictions.extend(predicted.cpu().numpy())
                  all_labels.extend(labels.cpu().numpy())
          
          if len(all_labels) > 0:
              accuracy = 100.0 * np.sum(np.array(all_predictions) == np.array(all_labels)) / len(all_labels)
              val_loss = total_loss / len(val_loader)
          else:
              accuracy = 0.0
              val_loss = 0.0
          
          cm = confusion_matrix(all_labels, all_predictions)
          class_report = classification_report(all_labels, all_predictions, output_dict=True)
          
          evaluation_results = {
              'model_name': model_config_eval.get('architecture', 'unknown'),
              'accuracy': accuracy,
              'val_loss': val_loss,
              'total_samples': len(all_labels),
              'correct_predictions': np.sum(np.array(all_predictions) == np.array(all_labels)),
              'confusion_matrix': cm.tolist(),
              'classification_report': class_report,
              'training_history': checkpoint.get('history', {})
          }
          
          output_dir = Path('/tmp/evaluator_output')
          output_dir.mkdir(exist_ok=True)
          cm_data_path = output_dir / 'confusion_matrix.json'
          
          with open(cm_data_path, 'w') as f:
              json.dump(cm.tolist(), f)
          
          with open(evaluation_results_file, 'w') as f:
              json.dump(evaluation_results, f, indent=2)
          with open(test_accuracy_file, 'w') as f:
              f.write(f"{accuracy:.2f}")
          with open(confusion_matrix_file, 'w') as f:
              f.write(str(cm_data_path))
          
          print(f"Evaluation completed")
          print(f"Test Accuracy: {accuracy:.2f}%")
          print(f"Test Loss: {val_loss:.4f}")
          print(f"Correct: {evaluation_results['correct_predictions']}/{evaluation_results['total_samples']}")
          print(f"Confusion matrix saved to: {cm_data_path}")
          
      except Exception as e:
          print(f"MODEL EVALUATION FAILED: {e}")
          import traceback
          traceback.print_exc()
          sys.exit(1)
    args:
    - --trained_model_path
    - {inputValue: trained_model_path}
    - --processed_data_path
    - {inputValue: processed_data_path}
    - --model_config
    - {inputValue: model_config}
    - --evaluation_results
    - {outputPath: evaluation_results}
    - --test_accuracy
    - {outputPath: test_accuracy}
    - --confusion_matrix
    - {outputPath: confusion_matrix}
