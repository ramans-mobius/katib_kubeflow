name: CNN Model Trainer
description: Trains CNN model on preprocessed data

inputs:
  - name: model_path
    type: String
    description: Path to built model from Brick 1
    
  - name: processed_data_path
    type: String
    description: Path to preprocessed data from Data Preprocessing brick
    
  - name: training_config
    type: String
    description: JSON string with training configuration

outputs:
  - name: trained_model_path
    type: String
    description: Path to trained model file
    
  - name: training_history
    type: String
    description: JSON string with training history and metrics
    
  - name: final_accuracy
    type: String
    description: Final validation accuracy

implementation:
  container:
    image: nikhilv215/nesy-factory:v18
    command:
    - python3
    - -u
    - -c
    - |
      import os
      import sys
      import json
      import torch
      import torch.nn as nn
      import torch.optim as optim
      from pathlib import Path
      import pickle
      
      print("=== CNN MODEL TRAINER ===")
      
      model_path = sys.argv[1]
      processed_data_path = sys.argv[2]
      training_config_str = sys.argv[3]
      trained_model_path_file = sys.argv[4]
      training_history_file = sys.argv[5]
      final_accuracy_file = sys.argv[6]
      
      # Parse training config
      training_config = json.loads(training_config_str)
      epochs = training_config.get('epochs', 2)
      learning_rate = training_config.get('learning_rate', 0.001)
      patience = training_config.get('patience', 5)
      
      print(f"Model path: {model_path}")
      print(f"Data path: {processed_data_path}")
      print(f"Epochs: {epochs}")
      print(f"Learning rate: {learning_rate}")
      
      # Add CNN factory to path
      sys.path.insert(0, '/app/nesy_factory_cnn')
      
      try:
          from nesy_factory_cnn import BaseCNN
          
          class ConfigurableCNN(BaseCNN):
              def forward(self, x):
                  for conv_block in self.conv_blocks:
                      x = conv_block(x)
                      x = self.pool(x)
                  x = x.view(x.size(0), -1)
                  x = self.classifier(x)
                  return x
          
          # Load model
          print("Loading model...")
          checkpoint = torch.load(model_path, map_location='cpu')
          model = ConfigurableCNN(checkpoint['model_config'])
          model.load_state_dict(checkpoint['model_state_dict'])
          device = model.device
          
          # Load data
          print("Loading preprocessed data...")
          train_loader = torch.load(os.path.join(processed_data_path, 'train_loader.pth'))
          val_loader = torch.load(os.path.join(processed_data_path, 'val_loader.pth'))
          
          with open(os.path.join(processed_data_path, 'data_info.pkl'), 'rb') as f:
              data_info = pickle.load(f)
          
          print(f"Training samples: {len(train_loader.dataset)}")
          print(f"Validation samples: {len(val_loader.dataset)}")
          print(f"Number of classes: {data_info['num_classes']}")
          
          # Training setup
          optimizer = optim.Adam(model.parameters(), lr=learning_rate)
          criterion = nn.CrossEntropyLoss()
          
          # Training loop
          print("Starting training...")
          training_history = []
          best_val_acc = 0.0
          patience_counter = 0
          
          for epoch in range(epochs):
              # Training phase
              model.train()
              train_loss = 0.0
              train_correct = 0
              train_total = 0
              
              for data, target in train_loader:
                  data, target = data.to(device), target.to(device)
                  
                  optimizer.zero_grad()
                  output = model(data)
                  loss = criterion(output, target)
                  loss.backward()
                  optimizer.step()
                  
                  train_loss += loss.item()
                  _, predicted = output.max(1)
                  train_total += target.size(0)
                  train_correct += predicted.eq(target).sum().item()
              
              train_acc = 100. * train_correct / train_total
              avg_train_loss = train_loss / len(train_loader)
              
              # Validation phase
              model.eval()
              val_loss = 0.0
              val_correct = 0
              val_total = 0
              
              with torch.no_grad():
                  for data, target in val_loader:
                      data, target = data.to(device), target.to(device)
                      output = model(data)
                      loss = criterion(output, target)
                      
                      val_loss += loss.item()
                      _, predicted = output.max(1)
                      val_total += target.size(0)
                      val_correct += predicted.eq(target).sum().item()
              
              val_acc = 100. * val_correct / val_total
              avg_val_loss = val_loss / len(val_loader)
              
              # Save epoch results
              epoch_info = {
                  'epoch': epoch + 1,
                  'train_loss': avg_train_loss,
                  'train_accuracy': train_acc,
                  'val_loss': avg_val_loss,
                  'val_accuracy': val_acc
              }
              training_history.append(epoch_info)
              
              print(f'Epoch {epoch+1}/{epochs}: '
                    f'Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}% | '
                    f'Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%')
              
              # Early stopping check
              if val_acc > best_val_acc:
                  best_val_acc = val_acc
                  patience_counter = 0
                  # Save best model
                  best_model_path = os.path.join('/tmp', 'best_model.pth')
                  torch.save({
                      'model_state_dict': model.state_dict(),
                      'model_config': checkpoint['model_config'],
                      'training_history': training_history,
                      'best_val_accuracy': best_val_acc
                  }, best_model_path)
              else:
                  patience_counter += 1
                  
              if patience_counter >= patience:
                  print(f"Early stopping at epoch {epoch+1}")
                  break
          
          # Save final trained model
          output_dir = Path('/tmp/trainer_output')
          output_dir.mkdir(exist_ok=True)
          trained_model_file = output_dir / 'trained_model.pth'
          
          torch.save({
              'model_state_dict': model.state_dict(),
              'model_config': checkpoint['model_config'],
              'training_history': training_history,
              'final_val_accuracy': best_val_acc,
              'total_epochs': len(training_history)
          }, str(trained_model_file))
          
          # Write outputs
          with open(trained_model_path_file, 'w') as f:
              f.write(str(trained_model_file))
          with open(training_history_file, 'w') as f:
              json.dump(training_history, f)
          with open(final_accuracy_file, 'w') as f:
              f.write(f"{best_val_acc:.2f}")
          
          print(f"Training completed!")
          print(f"Best validation accuracy: {best_val_acc:.2f}%")
          print(f"Total epochs: {len(training_history)}")
          print(f"Trained model saved to: {trained_model_file}")
          print("MODEL TRAINING COMPLETED SUCCESSFULLY!")
          
      except Exception as e:
          print(f"MODEL TRAINING FAILED: {e}")
          import traceback
          traceback.print_exc()
          sys.exit(1)
    
    args:
    - --model_path
    - {inputValue: model_path}
    - --processed_data_path
    - {inputValue: processed_data_path}
    - --training_config
    - {inputValue: training_config}
    - --trained_model_path
    - {outputPath: trained_model_path}
    - --training_history
    - {outputPath: training_history}
    - --final_accuracy
    - {outputPath: final_accuracy}
