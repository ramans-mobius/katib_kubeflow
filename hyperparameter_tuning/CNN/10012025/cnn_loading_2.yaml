name: 2 Load UI Components from CDN ZIP
description: Downloads ZIP file from CDN containing UI component images and prepares train/test datasets.
inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download ZIP file'}
  - {name: train_split, type: Float, description: 'Train split ratio (default 0.7)', default: '0.7'}
  - {name: shuffle_seed, type: Integer, description: 'Random seed for shuffling', default: '42'}
outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: dataset_info, type: DatasetInfo}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests pandas scikit-learn Pillow torch torchvision || \
        python3 -m pip install --quiet requests pandas scikit-learn Pillow torch torchvision --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        import json
        import base64
        import io
        import zipfile
        from pathlib import Path
        from PIL import Image
        import pandas as pd
        from collections import Counter
        from torch.utils.data import random_split
        import torch
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import logging

        parser = argparse.ArgumentParser()
        parser.add_argument('--cdn_url', type=str, required=True, help='CDN URL to download ZIP file')
        parser.add_argument('--train_split', type=float, default=0.7, help='Train split ratio')
        parser.add_argument('--shuffle_seed', type=int, default=42, help='Random seed for shuffling')
        parser.add_argument('--train_data', type=str, required=True, help='Path to output train dataset')
        parser.add_argument('--test_data', type=str, required=True, help='Path to output test dataset')
        parser.add_argument('--dataset_info', type=str, required=True, help='Path to output dataset info')
        args = parser.parse_args()

        # Setup retry logger
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger("cdn_retry")

        # Setup session with retry logic
        session = requests.Session()
        retries = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        adapter = HTTPAdapter(max_retries=retries)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        def download_and_extract_zip(url):
            """Download ZIP file and extract image data with labels"""
            try:
                logger.info(f"üì¶ Downloading ZIP from: {url}")
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                response = session.get(url, headers=headers, stream=True, timeout=30)
                response.raise_for_status()
                
                logger.info(f"‚úÖ Download successful! Size: {len(response.content)} bytes")
                
                # Extract from ZIP
                zip_content = io.BytesIO(response.content)
                dataset = []
                
                with zipfile.ZipFile(zip_content, 'r') as zip_file:
                    file_list = zip_file.namelist()
                    logger.info(f"üìÅ Found {len(file_list)} files in ZIP")
                    
                    # Process each file in the ZIP
                    for file_path in file_list:
                        # Skip directories
                        if file_path.endswith('/'):
                            continue
                            
                        # Extract label from directory structure
                        parts = file_path.split('/')
                        if len(parts) >= 3 and parts[0] == 'Classification_dataset':
                            label = parts[1]  # inputfield or toggle
                            
                            # Read and encode image
                            with zip_file.open(file_path) as image_file:
                                image_data = image_file.read()
                                base64_image = base64.b64encode(image_data).decode('utf-8')
                                
                                dataset.append({
                                    'image_data': base64_image,
                                    'label': label,
                                    'filename': file_path
                                })
                
                logger.info(f"‚úÖ Extracted {len(dataset)} images")
                return dataset
                
            except Exception as e:
                logger.error(f"‚ùå Error processing ZIP: {e}")
                raise

        # Download and extract dataset
        raw_data = download_and_extract_zip(args.cdn_url)

        # Extract dataset information
        labels = [item['label'] for item in raw_data]
        unique_labels = sorted(list(set(labels)))
        label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}
        idx_to_label = {idx: label for label, idx in label_to_idx.items()}
        label_distribution = Counter(labels)

        # Create dataset info
        dataset_info = {
            'total_samples': len(raw_data),
            'classes': unique_labels,
            'class_distribution': dict(label_distribution),
            'label_to_idx': label_to_idx,
            'idx_to_label': idx_to_label,
            'output_dim': len(unique_labels),
            'train_split_ratio': args.train_split,
            'shuffle_seed': args.shuffle_seed,
            'data_source': 'cdn_zip',
            'file_types': 'png'
        }

        print(f"üìä Dataset Info:")
        print(f"   Total samples: {len(raw_data)}")
        print(f"   Classes: {unique_labels}")
        print(f"   Label distribution: {dict(label_distribution)}")
        print(f"   Auto-detected output_dim: {len(unique_labels)}")

        # Create train/test splits
        train_size = int(args.train_split * len(raw_data))
        test_size = len(raw_data) - train_size

        # Split the data
        train_data, test_data = random_split(
            raw_data, [train_size, test_size],
            generator=torch.Generator().manual_seed(args.shuffle_seed)
        )

        # Convert to lists for serialization
        train_data_list = [raw_data[i] for i in train_data.indices]
        test_data_list = [raw_data[i] for i in test_data.indices]

        # Save train data
        os.makedirs(os.path.dirname(args.train_data) or ".", exist_ok=True)
        with open(args.train_data, "wb") as f:
            pickle.dump(train_data_list, f)

        # Save test data
        os.makedirs(os.path.dirname(args.test_data) or ".", exist_ok=True)
        with open(args.test_data, "wb") as f:
            pickle.dump(test_data_list, f)

        # Save dataset info
        os.makedirs(os.path.dirname(args.dataset_info) or ".", exist_ok=True)
        with open(args.dataset_info, "wb") as f:
            pickle.dump(dataset_info, f)

        print(f"‚úÖ Dataset loading complete!")
        print(f"   Train samples: {len(train_data_list)}")
        print(f"   Test samples: {len(test_data_list)}")
        print(f"   Classes: {unique_labels}")
            
    args:
      - --cdn_url
      - {inputValue: cdn_url}
      - --train_split
      - {inputValue: train_split}
      - --shuffle_seed
      - {inputValue: shuffle_seed}
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --dataset_info
      - {outputPath: dataset_info}
