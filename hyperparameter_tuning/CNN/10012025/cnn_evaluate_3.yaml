name: 123 CNN Model Evaluator
description: Comprehensive evaluation of trained CNN model
inputs:
  - name: trained_model_path
    type: String
    description: Path to trained model file
  - name: processed_data_path
    type: String
    description: Path to preprocessed data file
  - name: model_config
    type: String
    description: Complete model configuration with evaluation parameters
outputs:
  - name: evaluation_results
    type: String
    description: JSON string with comprehensive evaluation results
  - name: test_accuracy
    type: String
    description: Final test accuracy percentage
  - name: confusion_matrix
    type: String
    description: Path to confusion matrix visualization

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v19
    command:
    - python3
    - -u
    - -c
    - |
      import os
      import sys
      import json
      import torch
      import torch.nn as nn
      import numpy as np
      import matplotlib.pyplot as plt
      from pathlib import Path
      import pickle
      from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support
      import seaborn as sns
      
      print("=== CNN MODEL EVALUATOR ===")
      
      # Read input PATHS from files
      with open(sys.argv[1], 'r') as f:
          trained_model_file_path = f.read().strip()
      with open(sys.argv[2], 'r') as f:
          data_file_path = f.read().strip()
          
      model_config_str = sys.argv[3]
      evaluation_results_file = sys.argv[4]
      test_accuracy_file = sys.argv[5]
      confusion_matrix_file = sys.argv[6]
      
      print("Trained model file path: " + trained_model_file_path)
      print("Data file path: " + data_file_path)
      
      # Parse complete model config and extract evaluation parameters
      model_config = json.loads(model_config_str)
      evaluation_config = model_config.get('evaluation', {})
      generate_plots = evaluation_config.get('generate_plots', True)
      detailed_metrics = evaluation_config.get('detailed_metrics', True)
      
      # Add CNN factory to path
      sys.path.insert(0, '/app/nesy_factory_cnn')
      
      try:
          from nesy_factory_cnn import BaseCNN
          
          class ConfigurableCNN(BaseCNN):
              def forward(self, x):
                  for conv_block in self.conv_blocks:
                      x = conv_block(x)
                      x = self.pool(x)
                  x = x.view(x.size(0), -1)
                  x = self.classifier(x)
                  return x
          
          # Load trained model from PATH
          print("Loading trained model...")
          checkpoint = torch.load(trained_model_file_path, map_location='cpu')
          model = ConfigurableCNN(checkpoint['model_config'])
          model.load_state_dict(checkpoint['model_state_dict'])
          device = model.device
          print("Model loaded successfully. Device: " + str(device))
          
          # Load test data from PATH
          print("Loading test data...")
          with open(data_file_path, 'rb') as f:
              processed_data = pickle.load(f)
          
          test_loader = processed_data.get('test_loader')
          data_info = processed_data.get('data_info', {})
          
          if test_loader is None:
              raise ValueError("Test loader not found in processed data")
          
          print("Test samples: " + str(len(test_loader.dataset)))
          print("Number of classes: " + str(data_info.get('num_classes', 'Unknown')))
          print("Input shape: " + str(data_info.get('input_shape', 'Unknown')))
          
          # Comprehensive evaluation
          print("Running comprehensive evaluation...")
          model.eval()
          criterion = nn.CrossEntropyLoss()
          
          test_loss = 0.0
          all_predictions = []
          all_targets = []
          all_probabilities = []
          all_correct = []
          
          with torch.no_grad():
              for batch_idx, (data, target) in enumerate(test_loader):
                  data, target = data.to(device), target.to(device)
                  output = model(data)
                  loss = criterion(output, target)
                  
                  test_loss += loss.item()
                  
                  probabilities = torch.softmax(output, dim=1)
                  _, predictions = output.max(1)
                  
                  all_predictions.extend(predictions.cpu().numpy())
                  all_targets.extend(target.cpu().numpy())
                  all_probabilities.extend(probabilities.cpu().numpy())
                  all_correct.extend(predictions.eq(target).cpu().numpy())
                  
                  if batch_idx % 10 == 0:
                      print("Processed batch " + str(batch_idx) + "/" + str(len(test_loader)))
          
          # Convert to numpy arrays
          all_predictions = np.array(all_predictions)
          all_targets = np.array(all_targets)
          all_probabilities = np.array(all_probabilities)
          all_correct = np.array(all_correct)
          
          # Calculate basic metrics
          accuracy = np.mean(all_predictions == all_targets) * 100
          avg_test_loss = test_loss / len(test_loader)
          
          # Per-class metrics
          num_classes = data_info.get('num_classes', len(np.unique(all_targets)))
          class_accuracy = {}
          class_precision = {}
          class_recall = {}
          class_f1 = {}
          
          # Calculate per-class metrics using sklearn
          precision, recall, f1, support = precision_recall_fscore_support(
              all_targets, all_predictions, average=None, zero_division=0
          )
          
          for class_id in range(num_classes):
              class_mask = all_targets == class_id
              if np.sum(class_mask) > 0:
                  class_accuracy[class_id] = np.mean(all_predictions[class_mask] == all_targets[class_mask]) * 100
                  class_precision[class_id] = precision[class_id] * 100
                  class_recall[class_id] = recall[class_id] * 100
                  class_f1[class_id] = f1[class_id] * 100
              else:
                  class_accuracy[class_id] = 0.0
                  class_precision[class_id] = 0.0
                  class_recall[class_id] = 0.0
                  class_f1[class_id] = 0.0
          
          # Confidence analysis
          max_probabilities = np.max(all_probabilities, axis=1)
          avg_confidence = np.mean(max_probabilities) * 100
          confidence_std = np.std(max_probabilities) * 100
          confidence_min = np.min(max_probabilities) * 100
          confidence_max = np.max(max_probabilities) * 100
          
          # Misclassification analysis
          misclassified_mask = all_predictions != all_targets
          misclassified_count = np.sum(misclassified_mask)
          misclassification_rate = misclassified_count / len(all_predictions) * 100
          
          if misclassified_count > 0:
              misclassified_confidence = np.mean(max_probabilities[misclassified_mask]) * 100
          else:
              misclassified_confidence = 0
          
          # Correct classification confidence
          correct_confidence = np.mean(max_probabilities[~misclassified_mask]) * 100
          
          # Generate confusion matrix
          cm = confusion_matrix(all_targets, all_predictions)
          
          # Calculate overall metrics using sklearn
          macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(
              all_targets, all_predictions, average='macro', zero_division=0
          )
          weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(
              all_targets, all_predictions, average='weighted', zero_division=0
          )
          
          # Create comprehensive evaluation results
          evaluation_results = {
              'overall_metrics': {
                  'test_accuracy': float(accuracy),
                  'test_loss': float(avg_test_loss),
                  'misclassification_rate': float(misclassification_rate),
                  'macro_precision': float(macro_precision * 100),
                  'macro_recall': float(macro_recall * 100),
                  'macro_f1': float(macro_f1 * 100),
                  'weighted_precision': float(weighted_precision * 100),
                  'weighted_recall': float(weighted_recall * 100),
                  'weighted_f1': float(weighted_f1 * 100)
              },
              'confidence_analysis': {
                  'average_confidence': float(avg_confidence),
                  'confidence_std': float(confidence_std),
                  'confidence_min': float(confidence_min),
                  'confidence_max': float(confidence_max),
                  'misclassified_confidence': float(misclassified_confidence),
                  'correct_confidence': float(correct_confidence)
              },
              'class_metrics': {
                  'class_' + str(class_id): {
                      'accuracy': float(class_accuracy[class_id]),
                      'precision': float(class_precision[class_id]),
                      'recall': float(class_recall[class_id]),
                      'f1_score': float(class_f1[class_id]),
                      'support': int(support[class_id])
                  } for class_id in range(num_classes)
              },
              'predictions_summary': {
                  'total_samples': len(all_predictions),
                  'correct_predictions': int(np.sum(all_predictions == all_targets)),
                  'misclassified': int(misclassified_count),
                  'per_class_distribution': {
                      'class_' + str(class_id): int(np.sum(all_targets == class_id)) for class_id in range(num_classes)
                  }
              },
              'model_info': {
                  'num_parameters': model.get_num_parameters(),
                  'input_shape': data_info.get('input_shape', 'Unknown'),
                  'output_dim': checkpoint.get('model_config', {}).get('output_dim', num_classes),
                  'device': str(device)
              },
              'training_summary': {
                  'final_val_accuracy': checkpoint.get('final_val_accuracy', 0),
                  'best_val_accuracy': checkpoint.get('best_val_accuracy', 0),
                  'total_training_epochs': len(checkpoint.get('training_history', [])),
                  'training_history_available': 'training_history' in checkpoint
              }
          }
          
          # Generate confusion matrix plot
          cm_plot_path = "/tmp/confusion_matrix.png"
          if generate_plots:
              try:
                  plt.figure(figsize=(12, 10))
                  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                             xticklabels=['Class ' + str(i) for i in range(num_classes)], 
                             yticklabels=['Class ' + str(i) for i in range(num_classes)],
                             cbar_kws={'label': 'Number of Samples'})
                  plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
                  plt.xlabel('Predicted Label', fontsize=12)
                  plt.ylabel('True Label', fontsize=12)
                  plt.xticks(rotation=45)
                  plt.yticks(rotation=0)
                  
                  plt.tight_layout()
                  plt.savefig(str(cm_plot_path), bbox_inches='tight', dpi=300, facecolor='white')
                  plt.close()
                  print("Confusion matrix plot saved to: " + cm_plot_path)
              except Exception as plot_error:
                  print("Failed to generate confusion matrix plot: " + str(plot_error))
                  cm_plot_path = "/tmp/confusion_matrix_failed.png"
                  # Create a simple text file as fallback
                  with open(cm_plot_path, 'w') as f:
                      f.write("Confusion matrix plot generation failed")
          else:
              cm_plot_path = "/tmp/no_plot_generated.png"
              with open(cm_plot_path, 'w') as f:
                  f.write("Plots disabled in configuration")
          
          # Write OUTPUT PATHS to files
          with open(evaluation_results_file, 'w') as f:
              json.dump(evaluation_results, f, indent=2)
          with open(test_accuracy_file, 'w') as f:
              f.write("{:.2f}".format(accuracy))
          with open(confusion_matrix_file, 'w') as f:
              f.write(str(cm_plot_path))
          
          # Print comprehensive summary
          print("=" * 50)
          print("EVALUATION SUMMARY")
          print("=" * 50)
          print("Test Accuracy: {:.2f}%".format(accuracy))
          print("Test Loss: {:.4f}".format(avg_test_loss))
          print("Misclassification Rate: {:.2f}%".format(misclassification_rate))
          print("Macro F1-Score: {:.2f}%".format(macro_f1 * 100))
          print("Average Confidence: {:.2f}%".format(avg_confidence))
          print("Correct Prediction Confidence: {:.2f}%".format(correct_confidence))
          print("Misclassified Confidence: {:.2f}%".format(misclassified_confidence))
          print("Total Samples: {}".format(len(all_predictions)))
          print("Correct Predictions: {}".format(np.sum(all_predictions == all_targets)))
          print("Misclassified: {}".format(misclassified_count))
          print("Confusion matrix saved to: " + cm_plot_path)
          print("=" * 50)
          print("MODEL EVALUATION COMPLETED SUCCESSFULLY!")
          
      except Exception as e:
          print("MODEL EVALUATION FAILED: " + str(e))
          import traceback
          traceback.print_exc()
          # Write error outputs
          try:
              with open(evaluation_results_file, 'w') as f:
                  json.dump({"error": str(e), "success": False}, f)
              with open(test_accuracy_file, 'w') as f:
                  f.write("0.00")
              with open(confusion_matrix_file, 'w') as f:
                  f.write("/tmp/evaluation_failed.png")
          except:
              pass
          sys.exit(1)
    
    args:
    - --trained_model_path
    - {inputValue: trained_model_path}
    - --processed_data_path
    - {inputValue: processed_data_path}
    - --model_config
    - {inputValue: model_config}
    - --evaluation_results
    - {outputPath: evaluation_results}
    - --test_accuracy
    - {outputPath: test_accuracy}
    - --confusion_matrix
    - {outputPath: confusion_matrix}
