name: CNN Model Evaluator
description: Comprehensive evaluation of trained CNN model

inputs:
  - name: trained_model_path
    type: String
    description: Path to trained model
    
  - name: processed_data_path
    type: String
    description: Path to preprocessed data
    
  - name: model_config
    type: String
    description: Complete model configuration with evaluation parameters

outputs:
  - name: evaluation_results
    type: String
    description: JSON string with comprehensive evaluation results
    
  - name: test_accuracy
    type: String
    description: Final test accuracy percentage
    
  - name: confusion_matrix
    type: String
    description: Path to confusion matrix visualization

implementation:
  container:
    image: nikhilv215/nesy-factory:v18
    command:
    - python3
    - -u
    - -c
    - |
      import os
      import sys
      import json
      import torch
      import torch.nn as nn
      import numpy as np
      import matplotlib.pyplot as plt
      from pathlib import Path
      import pickle
      from sklearn.metrics import confusion_matrix, classification_report
      import seaborn as sns
      
      print("=== CNN MODEL EVALUATOR ===")
      
      trained_model_path = sys.argv[1]
      processed_data_path = sys.argv[2]
      model_config_str = sys.argv[3]
      evaluation_results_file = sys.argv[4]
      test_accuracy_file = sys.argv[5]
      confusion_matrix_file = sys.argv[6]
      
      # Parse complete model config and extract evaluation parameters
      model_config = json.loads(model_config_str)
      evaluation_config = model_config.get('evaluation', {})
      generate_plots = evaluation_config.get('generate_plots', True)
      detailed_metrics = evaluation_config.get('detailed_metrics', True)
      
      print(f"Trained model path: {trained_model_path}")
      print(f"Data path: {processed_data_path}")
      
      # Add CNN factory to path
      sys.path.insert(0, '/app/nesy_factory_cnn')
      
      try:
          from nesy_factory_cnn import BaseCNN
          
          class ConfigurableCNN(BaseCNN):
              def forward(self, x):
                  for conv_block in self.conv_blocks:
                      x = conv_block(x)
                      x = self.pool(x)
                  x = x.view(x.size(0), -1)
                  x = self.classifier(x)
                  return x
          
          # Load trained model
          print("Loading trained model...")
          checkpoint = torch.load(trained_model_path, map_location='cpu')
          model = ConfigurableCNN(checkpoint['model_config'])
          model.load_state_dict(checkpoint['model_state_dict'])
          device = model.device
          
          # Load test data
          print("Loading test data...")
          test_loader = torch.load(os.path.join(processed_data_path, 'test_loader.pth'))
          
          with open(os.path.join(processed_data_path, 'data_info.pkl'), 'rb') as f:
              data_info = pickle.load(f)
          
          print(f"Test samples: {len(test_loader.dataset)}")
          print(f"Number of classes: {data_info['num_classes']}")
          
          # Comprehensive evaluation
          print("Running comprehensive evaluation...")
          model.eval()
          criterion = nn.CrossEntropyLoss()
          
          test_loss = 0.0
          all_predictions = []
          all_targets = []
          all_probabilities = []
          
          with torch.no_grad():
              for data, target in test_loader:
                  data, target = data.to(device), target.to(device)
                  output = model(data)
                  loss = criterion(output, target)
                  
                  test_loss += loss.item()
                  
                  probabilities = torch.softmax(output, dim=1)
                  _, predictions = output.max(1)
                  
                  all_predictions.extend(predictions.cpu().numpy())
                  all_targets.extend(target.cpu().numpy())
                  all_probabilities.extend(probabilities.cpu().numpy())
          
          # Convert to numpy arrays
          all_predictions = np.array(all_predictions)
          all_targets = np.array(all_targets)
          all_probabilities = np.array(all_probabilities)
          
          # Calculate metrics
          accuracy = np.mean(all_predictions == all_targets) * 100
          avg_test_loss = test_loss / len(test_loader)
          
          # Per-class metrics
          num_classes = data_info['num_classes']
          class_accuracy = {}
          for class_id in range(num_classes):
              class_mask = all_targets == class_id
              if np.sum(class_mask) > 0:
                  class_accuracy[class_id] = np.mean(all_predictions[class_mask] == all_targets[class_mask]) * 100
          
          # Confidence analysis
          max_probabilities = np.max(all_probabilities, axis=1)
          avg_confidence = np.mean(max_probabilities) * 100
          confidence_std = np.std(max_probabilities) * 100
          
          # Misclassification analysis
          misclassified_mask = all_predictions != all_targets
          misclassified_count = np.sum(misclassified_mask)
          misclassification_rate = misclassified_count / len(all_predictions) * 100
          
          if misclassified_count > 0:
              misclassified_confidence = np.mean(max_probabilities[misclassified_mask]) * 100
          else:
              misclassified_confidence = 0
          
          # Generate confusion matrix
          cm = confusion_matrix(all_targets, all_predictions)
          
          # Create evaluation results
          evaluation_results = {
              'overall_metrics': {
                  'test_accuracy': float(accuracy),
                  'test_loss': float(avg_test_loss),
                  'misclassification_rate': float(misclassification_rate)
              },
              'confidence_analysis': {
                  'average_confidence': float(avg_confidence),
                  'confidence_std': float(confidence_std),
                  'misclassified_confidence': float(misclassified_confidence)
              },
              'class_metrics': {
                  f'class_{class_id}': float(acc) for class_id, acc in class_accuracy.items()
              },
              'predictions_summary': {
                  'total_samples': len(all_predictions),
                  'correct_predictions': int(np.sum(all_predictions == all_targets)),
                  'misclassified': int(misclassified_count)
              },
              'model_info': {
                  'num_parameters': model.get_num_parameters(),
                  'input_shape': data_info['input_shape'],
                  'output_dim': checkpoint['model_config'].get('output_dim', num_classes)
              },
              'training_summary': {
                  'final_val_accuracy': checkpoint.get('final_val_accuracy', 0),
                  'total_training_epochs': len(checkpoint.get('training_history', []))
              }
          }
          
          # Generate confusion matrix plot
          if generate_plots:
              plt.figure(figsize=(10, 8))
              sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                         xticklabels=range(num_classes), 
                         yticklabels=range(num_classes))
              plt.title('Confusion Matrix')
              plt.xlabel('Predicted')
              plt.ylabel('Actual')
              
              output_dir = Path('/tmp/evaluator_output')
              output_dir.mkdir(exist_ok=True)
              cm_plot_path = output_dir / 'confusion_matrix.png'
              plt.savefig(str(cm_plot_path), bbox_inches='tight', dpi=300)
              plt.close()
          else:
              cm_plot_path = "/tmp/no_plot_generated.png"
          
          # Write outputs
          with open(evaluation_results_file, 'w') as f:
              json.dump(evaluation_results, f, indent=2)
          with open(test_accuracy_file, 'w') as f:
              f.write(f"{accuracy:.2f}")
          with open(confusion_matrix_file, 'w') as f:
              f.write(str(cm_plot_path))
          
          # Print summary
          print(f"Evaluation completed!")
          print(f"Test Accuracy: {accuracy:.2f}%")
          print(f"Test Loss: {avg_test_loss:.4f}")
          print(f"Misclassification Rate: {misclassification_rate:.2f}%")
          print(f"Average Confidence: {avg_confidence:.2f}%")
          print(f"Confusion matrix saved to: {cm_plot_path}")
          print("MODEL EVALUATION COMPLETED SUCCESSFULLY!")
          
      except Exception as e:
          print(f"MODEL EVALUATION FAILED: {e}")
          import traceback
          traceback.print_exc()
          sys.exit(1)
    
    args:
    - --trained_model_path
    - {inputValue: trained_model_path}
    - --processed_data_path
    - {inputValue: processed_data_path}
    - --model_config
    - {inputValue: model_config}
    - --evaluation_results
    - {outputPath: evaluation_results}
    - --test_accuracy
    - {outputPath: test_accuracy}
    - --confusion_matrix
    - {outputPath: confusion_matrix}
