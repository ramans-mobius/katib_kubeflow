name: Preprocess CNN dataset
description: Takes CNN image datasets, applies preprocessing, and outputs a pickled DataWrapper object with processed data loaders.
inputs:
  - name: train_data
    type: Dataset
    
  - name: test_data
    type: Dataset
    
  - name: val_data
    type: Dataset
    
outputs:
  - name: processed_data_pickle
    type: Dataset
    
  - name: weight_out
    type: String
    description: "CNN config weights as JSON string"
    
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        pip install numpy pandas scikit-learn pillow

        # Create CNN preprocessing script
        cat > cnn_preprocess.py << EOF
        import numpy as np
        import os
        import pickle
        import pandas as pd
        from sklearn.preprocessing import LabelEncoder
        import json

        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)

        class StandardScaler():
            def __init__(self, mean, std):
                self.mean = mean
                self.std = std
            def transform(self, data):
                return (data - self.mean) / self.std
            def inverse_transform(self, data):
                return (data * self.std) + self.mean

        class DataLoaderM(object):
            def __init__(self, xs, ys, batch_size, pad_with_last_sample=True, shuffle=False):
                self.batch_size = batch_size
                self.current_ind = 0
                self.shuffle_flag = shuffle
                if pad_with_last_sample:
                    num_padding = (batch_size - (len(xs) % batch_size)) % batch_size
                    if num_padding > 0:
                        x_padding = np.repeat(xs[-1:], num_padding, axis=0)
                        y_padding = np.repeat(ys[-1:], num_padding, axis=0)
                        xs = np.concatenate([xs, x_padding], axis=0)
                        ys = np.concatenate([ys, y_padding], axis=0)
                self.size = len(xs)
                self.num_batch = int(self.size // self.batch_size)
                self.xs = xs
                self.ys = ys

            def shuffle(self):
                permutation = np.random.permutation(self.size)
                self.xs, self.ys = self.xs[permutation], self.ys[permutation]

            def get_iterator(self):
                if self.shuffle_flag:
                    self.shuffle()
                self.current_ind = 0
                def _wrapper():
                    while self.current_ind < self.num_batch:
                        start_ind = self.batch_size * self.current_ind
                        end_ind = min(self.size, self.batch_size * (self.current_ind + 1))
                        x_i = self.xs[start_ind: end_ind, ...]
                        y_i = self.ys[start_ind: end_ind, ...]
                        yield (x_i, y_i)
                        self.current_ind += 1
                return _wrapper()

        def apply_simple_augmentation(images, probability=0.5):
            """Apply simple data augmentation"""
            augmented = []
            for img in images:
                # Random horizontal flip
                if np.random.rand() < probability:
                    img = np.fliplr(img)
                
                # Random rotation (small angles)
                if np.random.rand() < probability:
                    angle = np.random.uniform(-15, 15)  # degrees
                    # Simple rotation approximation for small angles
                    if abs(angle) > 5:  # Only apply for noticeable rotation
                        center = np.array(img.shape[:2]) / 2
                        # This is a simplified rotation - in practice you'd use proper image rotation
                        img = img  # Keep original for now
                
                # Random brightness adjustment
                if np.random.rand() < probability:
                    brightness_factor = np.random.uniform(0.8, 1.2)
                    img = np.clip(img * brightness_factor, 0, 1)
                
                augmented.append(img)
            
            return np.array(augmented)

        def preprocess_cnn_images(images, labels, image_size=32, normalize=True, augment=False):
            """Preprocess images for CNN training"""
            processed_images = []
            
            for i, img in enumerate(images):
                # Ensure proper dtype
                img = img.astype(np.float32)
                
                # Resize if needed (simple nearest neighbor)
                if img.shape[:2] != (image_size, image_size):
                    # Simple resizing by repetition/truncation
                    if img.shape[0] != image_size or img.shape[1] != image_size:
                        # Resize by taking center crop or padding
                        h, w = img.shape[:2]
                        if h > image_size:
                            start_h = (h - image_size) // 2
                            img = img[start_h:start_h + image_size, :]
                        elif h < image_size:
                            pad_h = (image_size - h) // 2
                            img = np.pad(img, ((pad_h, image_size - h - pad_h), (0, 0), (0, 0)), mode='constant')
                        
                        if w > image_size:
                            start_w = (w - image_size) // 2
                            img = img[:, start_w:start_w + image_size]
                        elif w < image_size:
                            pad_w = (image_size - w) // 2
                            if img.ndim == 3:
                                img = np.pad(img, ((0, 0), (pad_w, image_size - w - pad_w), (0, 0)), mode='constant')
                            else:
                                img = np.pad(img, ((0, 0), (pad_w, image_size - w - pad_w)), mode='constant')
                
                # Ensure channel dimension
                if img.ndim == 2:  # Grayscale
                    img = np.expand_dims(img, axis=-1)
                
                # Convert HWC to CHW format for CNN
                if img.ndim == 3:
                    img = np.transpose(img, (2, 0, 1))
                
                # Normalize to [0, 1]
                if normalize and img.max() > 1:
                    img = img / 255.0
                
                processed_images.append(img)
                
                if (i + 1) % 1000 == 0:
                    print(f"Processed {i + 1}/{len(images)} images")
            
            processed_images = np.array(processed_images, dtype=np.float32)
            
            # Apply augmentation if requested
            if augment:
                print("Applying data augmentation...")
                processed_images = apply_simple_augmentation(processed_images)
            
            return processed_images, labels

        def load_cnn_dataset(train_path, test_path, val_path, config):
            """Load and preprocess CNN dataset"""
            batch_size = config['batch_size']
            image_size = config.get('image_size', 32)
            
            print("Loading train data...")
            with open(train_path, 'rb') as f:
                train_data = pickle.load(f)
            
            print("Loading test data...")
            with open(test_path, 'rb') as f:
                test_data = pickle.load(f)
            
            print("Loading validation data...")
            with open(val_path, 'rb') as f:
                val_data = pickle.load(f)
            
            # Extract images and labels
            if isinstance(train_data, tuple):
                train_images, train_labels = train_data
            else:
                # Assume it's a list of (image, label) pairs
                train_images, train_labels = zip(*train_data)
                train_images, train_labels = np.array(train_images), np.array(train_labels)
            
            if isinstance(test_data, tuple):
                test_images, test_labels = test_data
            else:
                test_images, test_labels = zip(*test_data)
                test_images, test_labels = np.array(test_images), np.array(test_labels)
            
            if isinstance(val_data, tuple):
                val_images, val_labels = val_data
            else:
                val_images, val_labels = zip(*val_data)
                val_images, val_labels = np.array(val_images), np.array(val_labels)
            
            print(f"Dataset sizes - Train: {len(train_images)}, Test: {len(test_images)}, Val: {len(val_images)}")
            print(f"Image shapes - Train: {train_images[0].shape}, Test: {test_images[0].shape}, Val: {val_images[0].shape}")
            
            # Encode labels
            label_encoder = LabelEncoder()
            all_labels = np.concatenate([train_labels, test_labels, val_labels])
            label_encoder.fit(all_labels)
            
            train_labels = label_encoder.transform(train_labels)
            test_labels = label_encoder.transform(test_labels)
            val_labels = label_encoder.transform(val_labels)
            
            print(f"Number of classes: {len(label_encoder.classes_)}")
            print(f"Class distribution - Train: {np.bincount(train_labels)}")
            
            # Preprocess images
            print("Preprocessing training images...")
            train_images, train_labels = preprocess_cnn_images(
                train_images, train_labels, image_size=image_size, augment=True
            )
            
            print("Preprocessing test images...")
            test_images, test_labels = preprocess_cnn_images(
                test_images, test_labels, image_size=image_size, augment=False
            )
            
            print("Preprocessing validation images...")
            val_images, val_labels = preprocess_cnn_images(
                val_images, val_labels, image_size=image_size, augment=False
            )
            
            data = {}
            data['x_train'], data['y_train'] = train_images, train_labels
            data['x_test'], data['y_test'] = test_images, test_labels
            data['x_val'], data['y_val'] = val_images, val_labels
            
            # Calculate statistics for normalization
            train_mean = data['x_train'].mean(axis=(0, 2, 3))
            train_std = data['x_train'].std(axis=(0, 2, 3))
            
            # Create scaler (per-channel)
            scaler = StandardScaler(mean=train_mean, std=train_std)
            
            # Apply normalization
            print("Applying normalization...")
            for category in ['train', 'test', 'val']:
                x_key = 'x_' + category
                for c in range(data[x_key].shape[1]):  # For each channel
                    data[x_key][:, c, :, :] = scaler.transform(data[x_key][:, c, :, :])
            
            # Create data loaders
            print("Creating data loaders...")
            data['train_loader'] = DataLoaderM(data['x_train'], data['y_train'], batch_size, shuffle=True)
            data['val_loader'] = DataLoaderM(data['x_val'], data['y_val'], batch_size, shuffle=False)
            data['test_loader'] = DataLoaderM(data['x_test'], data['y_test'], batch_size, shuffle=False)
            data['scaler'] = scaler
            data['label_encoder'] = label_encoder
            
            # Add compatibility masks
            data['train_mask'] = np.ones(len(data['x_train']), dtype=bool)
            data['test_mask'] = np.ones(len(data['x_test']), dtype=bool)
            data['val_mask'] = np.ones(len(data['x_val']), dtype=bool)
            
            # Add dataset info
            data['num_classes'] = len(label_encoder.classes_)
            data['image_shape'] = data['x_train'][0].shape
            data['class_names'] = label_encoder.classes_
            
            print(f"Final image shape: {data['image_shape']}")
            print(f"Number of classes: {data['num_classes']}")
            
            return data

        if __name__ == '__main__':
            # CNN-specific config
            config = {
                'batch_size': 64,
                'image_size': 32,  # Target image size
                'normalize': True,
                'augment': True
            }
            
            # Get input paths from environment
            train_path = os.environ['TRAIN_DATA_PATH']
            test_path = os.environ['TEST_DATA_PATH']
            val_path = os.environ['VAL_DATA_PATH']
            
            print("=== CNN PREPROCESSING STARTED ===")
            data_dict = load_cnn_dataset(train_path, test_path, val_path, config)
            data = DataWrapper(data_dict)
            
            output_path = os.environ['PROCESSED_DATA_PICKLE_PATH']
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, 'wb') as f:
                pickle.dump(data, f)
            
            print(f"Saved processed data to: {output_path}")
            print("=== CNN PREPROCESSING COMPLETED ===")
        EOF

        # Create CNN weights script
        cat > create_cnn_weights.py << EOF
        import json
        import os

        if __name__ == '__main__':
            # CNN-specific dummy weights/config
            cnn_weights = {
                "learning_rate": 0.001,
                "weight_decay": 1e-4,
                "dropout": 0.5,
                "conv_channels": [32, 64, 128],
                "fc_hidden": 512,
                "optimizer": "adam",
                "loss_function": "crossentropy",
                "epochs": 50,
                "early_stopping": True,
                "patience": 10
            }
            
            output_path = os.environ['WEIGHT_OUT_PATH']
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w') as f:
                json.dump(cnn_weights, f, indent=2)
            
            print(f"Saved CNN weights config to: {output_path}")
        EOF

        # Run the preprocessing
        TRAIN_DATA_PATH="$0" TEST_DATA_PATH="$1" VAL_DATA_PATH="$2" \
        PROCESSED_DATA_PICKLE_PATH="$3" python cnn_preprocess.py
        
        # Create weights file
        WEIGHT_OUT_PATH="$4" python create_cnn_weights.py

    args:
      - {inputPath: train_data}
      - {inputPath: test_data}
      - {inputPath: val_data}
      - {outputPath: processed_data_pickle}
      - {outputPath: weight_out}
