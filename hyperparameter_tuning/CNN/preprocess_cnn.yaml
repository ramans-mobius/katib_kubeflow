name: CNN Preprocessing Brick
description: Prepares train/validation/test datasets for CNN training with optional resizing, normalization, and augmentation.

inputs:
  - name: train_dataset
    type: Dataset
    description: "Pickle file with training dataset"
    
  - name: val_dataset
    type: Dataset
    description: "Pickle file with validation dataset"
    
  - name: test_dataset
    type: Dataset
    description: "Pickle file with test dataset"
    
  - name: image_size
    type: int
    description: "Target image size (assumes square)"
    default: 32
    
  - name: batch_size
    type: int
    description: "Batch size for DataLoaders"
    default: 64
    
  - name: augmentation
    type: String
    description: "Whether to apply simple augmentation (true/false)"
    default: "true"
    
outputs:
  - name: train_loader_pickle
    type: Dataset
    description: "Pickle file with train DataLoader"
    
  - name: val_loader_pickle
    type: Dataset
    description: "Pickle file with validation DataLoader"
    
  - name: test_loader_pickle
    type: Dataset
    description: "Pickle file with test DataLoader"

implementation:
  container:
    image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import pickle
        import torch
        from torch.utils.data import DataLoader, TensorDataset
        import torch.nn.functional as F
        import numpy as np
        import os
        from PIL import Image

        def str_to_bool(v):
            if isinstance(v, bool):
                return v
            if v.lower() in ('yes', 'true', 't', 'y', '1'):
                return True
            elif v.lower() in ('no', 'false', 'f', 'n', '0'):
                return False
            else:
                raise argparse.ArgumentTypeError('Boolean value expected.')

        parser = argparse.ArgumentParser()
        parser.add_argument('--train_dataset', type=str, required=True)
        parser.add_argument('--val_dataset', type=str, required=True)
        parser.add_argument('--test_dataset', type=str, required=True)
        parser.add_argument('--image_size', type=int, default=32)
        parser.add_argument('--batch_size', type=int, default=64)
        parser.add_argument('--augmentation', type=str_to_bool, default=True)
        parser.add_argument('--train_loader_pickle', type=str, required=True)
        parser.add_argument('--val_loader_pickle', type=str, required=True)
        parser.add_argument('--test_loader_pickle', type=str, required=True)
        args = parser.parse_args()

        print("=== CNN PREPROCESSING STARTED ===")
        print(f"Image size: {args.image_size}")
        print(f"Batch size: {args.batch_size}")
        print(f"Augmentation: {args.augmentation}")

        def load_pickle(path):
            print(f"Loading dataset from: {path}")
            with open(path, 'rb') as f:
                data = pickle.load(f)
            print(f"Loaded dataset with {len(data)} samples")
            return data

        def save_pickle(obj, path):
            os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
            with open(path, 'wb') as f:
                pickle.dump(obj, f)
            print(f"Saved to: {path}")

        def preprocess_images(data, image_size, augmentation=False):
            print(f"Preprocessing images - Size: {image_size}, Augmentation: {augmentation}")
            
            # Handle different data formats
            if isinstance(data, (list, tuple)) and len(data) == 2:
                X, y = data
            elif isinstance(data, dict):
                X = data.get('X', data.get('data', data.get('images')))
                y = data.get('y', data.get('labels', data.get('targets')))
            else:
                # Assume it's a list of (image, label) tuples
                X, y = zip(*data)
            
            print(f"Found {len(X)} images and {len(y)} labels")
            
            # Convert to numpy arrays if needed
            if not isinstance(X, np.ndarray):
                X = np.array(X)
            if not isinstance(y, np.ndarray):
                y = np.array(y)
            
            print(f"Original image shape: {X[0].shape if len(X) > 0 else 'Unknown'}")
            
            # Process images
            processed_images = []
            for i, img in enumerate(X):
                # Convert to tensor and ensure proper format
                if isinstance(img, np.ndarray):
                    if img.ndim == 2:  # Grayscale
                        img = np.expand_dims(img, axis=0)  # Add channel dimension
                    elif img.ndim == 3 and img.shape[2] in [1, 3]:  # HWC format
                        img = np.transpose(img, (2, 0, 1))  # Convert to CHW
                    
                    img_tensor = torch.from_numpy(img).float()
                else:
                    img_tensor = torch.tensor(img, dtype=torch.float32)
                
                # Ensure tensor is in CHW format
                if img_tensor.ndim == 2:
                    img_tensor = img_tensor.unsqueeze(0)
                
                # Resize if needed
                if img_tensor.shape[-2:] != (image_size, image_size):
                    img_tensor = F.interpolate(
                        img_tensor.unsqueeze(0), 
                        size=(image_size, image_size), 
                        mode='bilinear', 
                        align_corners=False
                    ).squeeze(0)
                
                # Simple augmentation for training
                if augmentation and np.random.rand() > 0.5:
                    img_tensor = torch.flip(img_tensor, [2])  # Horizontal flip
                
                # Normalize to [0, 1] if needed
                if img_tensor.max() > 1:
                    img_tensor = img_tensor / 255.0
                
                processed_images.append(img_tensor)
                
                if (i + 1) % 1000 == 0:
                    print(f"Processed {i + 1}/{len(X)} images")
            
            # Stack all images
            X_tensor = torch.stack(processed_images)
            y_tensor = torch.tensor(y, dtype=torch.long)
            
            print(f"Final tensor shapes - X: {X_tensor.shape}, y: {y_tensor.shape}")
            
            return TensorDataset(X_tensor, y_tensor)

        try:
            # Load datasets
            print("Loading datasets...")
            train_data = load_pickle(args.train_dataset)
            val_data = load_pickle(args.val_dataset)
            test_data = load_pickle(args.test_dataset)

            # Preprocess datasets
            print("Preprocessing training dataset...")
            train_ds = preprocess_images(train_data, args.image_size, args.augmentation)
            
            print("Preprocessing validation dataset...")
            val_ds = preprocess_images(val_data, args.image_size, False)
            
            print("Preprocessing test dataset...")
            test_ds = preprocess_images(test_data, args.image_size, False)

            # Create DataLoaders with num_workers=0 to avoid pickling issues
            print("Creating DataLoaders...")
            train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=0)
            val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=0)
            test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, num_workers=0)

            print(f"DataLoader sizes - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}")

            # Save DataLoaders as pickle
            print("Saving DataLoaders...")
            save_pickle(train_loader, args.train_loader_pickle)
            save_pickle(val_loader, args.val_loader_pickle)
            save_pickle(test_loader, args.test_loader_pickle)

            print("=== CNN PREPROCESSING COMPLETED SUCCESSFULLY ===")

        except Exception as e:
            print(f"=== ERROR IN CNN PREPROCESSING ===")
            print(f"Error: {e}")
            import traceback
            traceback.print_exc()
            raise

    args:
      - --train_dataset
      - {inputPath: train_dataset}
      - --val_dataset
      - {inputPath: val_dataset}
      - --test_dataset
      - {inputPath: test_dataset}
      - --image_size
      - {inputValue: image_size}
      - --batch_size
      - {inputValue: batch_size}
      - --augmentation
      - {inputValue: augmentation}
      - --train_loader_pickle
      - {outputPath: train_loader_pickle}
      - --val_loader_pickle
      - {outputPath: val_loader_pickle}
      - --test_loader_pickle
      - {outputPath: test_loader_pickle}
