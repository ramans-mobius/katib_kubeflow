name: New DQN RLAF Loop CNN
description: Triggers the DQN RLAF pipeline in a loop to optimize CNN model hyperparameters, controlled by a pierce_or_not flag.
inputs:
  - {name: trained_model, type: Model}
  - {name: init_metrics, type: Metrics}
  - {name: data_path, type: Dataset}
  - {name: config, type: String}
  - {name: domain, type: String}
  - {name: schema_id, type: String}
  - {name: model_id, type: String}
  - {name: dqn_pipeline_id, type: String}
  - {name: pipeline_domain, type: String}
  - {name: dqn_experiment_id, type: String}
  - {name: access_token, type: string}
  - {name: tasks, type: Dataset}
outputs:
  - {name: rlaf_output, type: Dataset}
  - {name: retrained_model, type: Model}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v19
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests || \
        python3 -m pip install --quiet requests --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        import pickle
        import time
        import numpy as np
        from typing import Dict, List, Any
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        from torch.utils.data import DataLoader
        import torch.nn as nn
        import torch.optim as optim
        from nesy_factory.CNNs.factory import CNNFactory

        # API Helper Functions
        def get_retry_session():
            retry_strategy = Retry(
                total=5,
                status_forcelist=[500, 502, 503, 504],
                backoff_factor=1
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session

        def trigger_pipeline(config, pipeline_domain, dqn_params=None):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
            pipeline_params = {"param_json": json.dumps(dqn_params)} if dqn_params else {}
            payload = json.dumps({
                "pipelineType": "ML", 
                "containerResources": {}, 
                "experimentId": config['experiment_id'],
                "enableCaching": True, 
                "parameters": pipeline_params, 
                "version": 1
            })
            headers = {
                'accept': 'application/json', 
                'Authorization': f"Bearer {config['access_token']}",
                'Content-Type': 'application/json'
            }
            response = http.post(url, headers=headers, data=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            return result['runId']

        def get_pipeline_status(config, pipeline_domain):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
            headers = {
                'accept': 'application/json', 
                'Authorization': f"Bearer {config['access_token']}"
            }
            response = http.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            pipeline_status = response.json()
            latest_state = pipeline_status['run_details']['state_history'][-1]
            return latest_state['state']

        def get_instance(access_token, domain, schema_id, model_id):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            headers = {
                "Authorization": f"Bearer {access_token}", 
                "Content-Type": "application/json"
            }
            payload = {
                "dbType": "TIDB", 
                "ownedOnly": True, 
                "filter": {"model_id": model_id}
            }
            response = http.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            if not data['content']:
                raise ValueError(f"No instance found for model_id: {model_id}")
            return data['content'][0]

        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {
                "Authorization": f"Bearer {access_token}", 
                "Content-Type": "application/json"
            }
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {
                    "conditions": [{
                        "field": "model_id", 
                        "operator": "EQUAL", 
                        "value": model_id
                    }]
                },
                "partialUpdateRequests": [{
                    "patch": [{
                        "operation": "REPLACE", 
                        "path": f"{field}", 
                        "value": value
                    }]
                }]
            }
            response = http.patch(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()

        # CNN Continual Learning Trainer
        class CNNContinualTrainer:
            def __init__(self, config: Dict[str, Any]):
                self.config = config
                self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                
            def train_continual_cnn(self, tasks: List[Dict], model, strategies: List[str] = ['naive']) -> Dict[str, Any]:
                results = {}
                for strategy_name in strategies:
                    strategy_results = self._train_single_strategy(tasks, strategy_name, model)
                    results[strategy_name] = strategy_results
                return results
            
            def _train_single_strategy(self, tasks: List[Dict], strategy_name: str, model) -> Dict[str, Any]:
                task_metrics = []
                all_task_performance = []
                previous_task_data = []
                
                model.to(self.device)
                
                for task_idx, task_data in enumerate(tasks):
                    # Apply strategy-specific training
                    if strategy_name == 'naive':
                        training_loader = task_data['train_loader']
                    elif strategy_name == 'replay' and previous_task_data:
                        training_loader = self._create_replay_loader(task_data, previous_task_data)
                    else:
                        training_loader = task_data['train_loader']
                    
                    # Train CNN on current task
                    current_metrics = self._train_cnn_on_task(model, training_loader, task_data['test_loader'])
                    task_metrics.append(current_metrics)
                    
                    # Evaluate on all tasks seen so far
                    task_performance = []
                    for eval_task_idx in range(task_idx + 1):
                        eval_metrics = self._evaluate_cnn_on_task(model, tasks[eval_task_idx]['test_loader'])
                        task_performance.append({
                            'task_id': eval_task_idx,
                            'accuracy': eval_metrics['accuracy'],
                            'loss': eval_metrics['loss']
                        })
                    
                    all_task_performance.append(task_performance)
                    
                    if strategy_name == 'replay':
                        previous_task_data.append(task_data)
                        if len(previous_task_data) > 2:
                            previous_task_data = previous_task_data[-2:]
                
                # Calculate continual learning metrics
                cl_metrics = self._calculate_continual_metrics(all_task_performance)
                
                # Final evaluation on all tasks
                final_eval_metrics = []
                for i in range(len(tasks)):
                    task_eval = self._evaluate_cnn_on_task(model, tasks[i]['test_loader'])
                    final_eval_metrics.append(task_eval)
                
                avg_metrics = {}
                if final_eval_metrics:
                    for key in final_eval_metrics[0]:
                        avg_metrics[key] = np.mean([m[key] for m in final_eval_metrics if key in m])

                return {
                    'strategy': strategy_name,
                    'task_metrics': task_metrics,
                    'all_task_performance': all_task_performance,
                    'continual_metrics': cl_metrics,
                    'final_model': model,
                    'average_eval_metrics': avg_metrics
                }
            
            def _train_cnn_on_task(self, model, train_loader, test_loader) -> Dict[str, float]:
                model.train()
                model.to(self.device)
                
                # Get training parameters from config
                training_config = self.config.get('training', {})
                optimizer_config = training_config.get('optimizer', {})
                
                learning_rate = optimizer_config.get('learning_rate', 0.001)
                weight_decay = optimizer_config.get('weight_decay', 0.01)
                epochs = training_config.get('epochs', 10)
                
                optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
                criterion = nn.CrossEntropyLoss()
                
                for epoch in range(epochs):
                    total_loss = 0
                    correct = 0
                    total = 0
                    
                    for data, targets in train_loader:
                        data, targets = data.to(self.device), targets.to(self.device)
                        optimizer.zero_grad()
                        outputs = model(data)
                        loss = criterion(outputs, targets)
                        loss.backward()
                        optimizer.step()
                        
                        total_loss += loss.item()
                        _, predicted = outputs.max(1)
                        total += targets.size(0)
                        correct += predicted.eq(targets).sum().item()
                
                return self._evaluate_cnn_on_task(model, test_loader)
            
            def _evaluate_cnn_on_task(self, model, test_loader) -> Dict[str, float]:
                model.eval()
                model.to(self.device)
                
                criterion = nn.CrossEntropyLoss()
                total_loss = 0
                correct = 0
                total = 0
                
                with torch.no_grad():
                    for data, targets in test_loader:
                        data, targets = data.to(self.device), targets.to(self.device)
                        outputs = model(data)
                        loss = criterion(outputs, targets)
                        
                        total_loss += loss.item()
                        _, predicted = outputs.max(1)
                        total += targets.size(0)
                        correct += predicted.eq(targets).sum().item()
                
                accuracy = 100. * correct / total
                avg_loss = total_loss / len(test_loader)
                
                return {
                    'accuracy': accuracy,
                    'loss': avg_loss,
                    'correct': correct,
                    'total': total
                }
            
            def _create_replay_loader(self, current_task: Dict, previous_tasks: List[Dict]) -> DataLoader:
                from torch.utils.data import TensorDataset
                
                replay_ratio = 0.2
                current_loader = current_task['train_loader']
                current_size = len(current_loader.dataset)
                replay_size = int(current_size * replay_ratio / (1 - replay_ratio))
                
                replay_data = []
                replay_labels = []
                
                for prev_task in previous_tasks:
                    prev_loader = prev_task['train_loader']
                    prev_dataset = prev_loader.dataset
                    if len(prev_dataset) > 0:
                        sample_size = min(replay_size // len(previous_tasks), len(prev_dataset))
                        indices = torch.randperm(len(prev_dataset))[:sample_size]
                        for idx in indices:
                            data, label = prev_dataset[idx]
                            replay_data.append(data)
                            replay_labels.append(label)
                
                current_data = []
                current_labels = []
                for data, label in current_loader.dataset:
                    current_data.append(data)
                    current_labels.append(label)
                
                if replay_data:
                    combined_data = torch.cat([torch.stack(current_data), torch.stack(replay_data)])
                    combined_labels = torch.cat([torch.tensor(current_labels), torch.tensor(replay_labels)])
                else:
                    combined_data = torch.stack(current_data)
                    combined_labels = torch.tensor(current_labels)
                
                combined_dataset = TensorDataset(combined_data, combined_labels)
                return DataLoader(combined_dataset, batch_size=current_loader.batch_size, shuffle=True)
            
            def _calculate_continual_metrics(self, all_task_performance: List[List[Dict]]) -> Dict[str, float]:
                final_performance = all_task_performance[-1]
                average_accuracy = np.mean([task['accuracy'] for task in final_performance])
                average_loss = np.mean([task['loss'] for task in final_performance])
                
                forgetting_scores = []
                for task_idx in range(len(all_task_performance) - 1):
                    max_acc = all_task_performance[task_idx][task_idx]['accuracy']
                    final_acc = all_task_performance[-1][task_idx]['accuracy']
                    forgetting = max_acc - final_acc
                    forgetting_scores.append(max(0, forgetting))
                
                avg_forgetting = np.mean(forgetting_scores) if forgetting_scores else 0.0
                
                return {
                    'average_accuracy': average_accuracy,
                    'average_loss': average_loss,
                    'forgetting': avg_forgetting,
                    'num_tasks': len(all_task_performance)
                }

        # CNN Retraining Logic
        def cnn_retraining(action, model_path, data_path, config_str, tasks_path, output_model_path, previous_metrics, dqn_params):
            # Load configuration
            config = json.loads(config_str)
            
            # Load tasks
            with open(tasks_path, "rb") as f:
                tasks = pickle.load(f)
            
            # Update config with DQN action parameters
            model_config = config.get('model', {})
            training_config = config.get('training', {})
            
            # Apply action parameters to model and training config
            for param_key, param_value in action.items():
                if 'learning_rate' in param_key:
                    training_config['optimizer']['learning_rate'] = float(param_value)
                elif 'batch_size' in param_key:
                    training_config['batch_size'] = int(param_value)
                elif 'epochs' in param_key:
                    training_config['epochs'] = int(param_value)
                elif 'architecture' in param_key:
                    model_config['architecture'] = param_value
                elif 'variant' in param_key:
                    model_config['variant'] = param_value
            
            # Create CNN model using CNNFactory
            model = CNNFactory.create_model(model_config['architecture'], model_config)
            
            # Load pre-trained weights
            model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
            
            # Train model with continual learning
            trainer = CNNContinualTrainer(config)
            results = trainer.train_continual_cnn(tasks=tasks, strategies=['naive'], model=model)
            
            average_eval_metrics = results['naive']['average_eval_metrics']
            
            # Calculate improvement score
            improvement_score = 0
            for param in dqn_params:
                key = param['key']
                sign = 1 if param['sign'] == '+' else -1
                if key in average_eval_metrics and key in previous_metrics:
                    improvement = (average_eval_metrics[key] - previous_metrics[key]) * sign
                    improvement_score += improvement * param.get('mul', 1.0)

            # Save model if improved
            os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
            
            if improvement_score > 0:
                final_model = results['naive']['final_model']
                torch.save(final_model.state_dict(), output_model_path)
            else:
                # Keep original model if no improvement
                torch.save(model.state_dict(), output_model_path)

            return {"metrics": average_eval_metrics, "model_path": output_model_path}

        def trigger_and_wait_for_dqn_pipeline(config, pipeline_domain, dqn_params):
            run_id = trigger_pipeline(config, pipeline_domain, dqn_params)
            config["run_id"] = run_id
            
            while True:
                status = get_pipeline_status(config, pipeline_domain)
                if status == 'SUCCEEDED':
                    break
                elif status in ['FAILED', 'ERROR', 'CANCELLED']:
                    raise RuntimeError(f"DQN pipeline failed with status: {status}")
                time.sleep(30)

        # Main execution
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            parser.add_argument('--pipeline_domain', type=str, required=True)
            parser.add_argument('--retrained_model', type=str, required=True)
            args = parser.parse_args()

            # Load access token
            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            
            # Load initial metrics
            with open(args.init_metrics, 'r') as f:
                current_metrics = json.load(f)
            
            action_id_for_next_pierce = -1
            
            for i in range(2):  # RLAF loop iterations
                print(f"=== CNN RLAF Iteration {i+1} ===")
                
                # Prepare metrics for DQN
                cleaned_metrics = {}
                dqn_params = []
                for key, value in current_metrics.items():
                    cleaned_metrics[key] = float(value)
                    if any(term in key.lower() for term in ["accuracy", "f1", "precision", "recall"]):
                        sign = "+"
                    else:
                        sign = "-"
                    dqn_params.append({"key": key, "sign": sign, "mul": 1.0})
                
                # Update database with current state
                instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                
                if instance.get('pierce2rlaf'):
                    latest_pierce2rlaf = instance['pierce2rlaf'][-1]
                    previous_state = latest_pierce2rlaf['current_state']
                    episode = latest_pierce2rlaf['episode']
                else:
                    previous_state = {key: 0.0 for key in cleaned_metrics.keys()}
                    episode = 0
                
                new_pierce2rlaf_entry = {
                    "action_id": action_id_for_next_pierce, 
                    "previous_state": previous_state,
                    "current_state": cleaned_metrics, 
                    "episode": episode, 
                    "timestamp": int(time.time())
                }
                
                pierce2rlaf_history = instance.get("pierce2rlaf", [])
                pierce2rlaf_history.append(new_pierce2rlaf_entry)
                update_instance_field(access_token, args.domain, args.schema_id, args.model_id, 
                                    "pierce2rlaf", pierce2rlaf_history)

                # Trigger DQN pipeline
                dqn_config = {
                    "pipeline_id": args.dqn_pipeline_id, 
                    "experiment_id": args.dqn_experiment_id, 
                    "access_token": access_token
                }
                trigger_and_wait_for_dqn_pipeline(dqn_config, args.pipeline_domain, dqn_params)

                # Get DQN recommendation
                updated_instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                latest_rlaf2pierce = updated_instance['rlaf2pierce'][-1]
                
                if not latest_rlaf2pierce.get("pierce_or_not", True):
                    break
                
                action_id_for_next_pierce = latest_rlaf2pierce['action_id']
                rlaf_actions = updated_instance.get('rlaf_actions', {}).get('actions', [])
                action_details = next((a for a in rlaf_actions if a["id"] == action_id_for_next_pierce))
                
                # Retrain model with DQN action
                retraining_results = cnn_retraining(
                    action_details['params'], 
                    args.trained_model, 
                    args.data_path, 
                    args.config, 
                    args.tasks,
                    args.retrained_model, 
                    previous_state, 
                    dqn_params
                )
                
                current_metrics = retraining_results["metrics"]
            
            # Save final results
            os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
            final_output = {
                "final_metrics": current_metrics,
                "model_type": "CNN_image_classifier",
                "iterations_completed": i + 1
            }
            
            with open(args.rlaf_output, 'w') as f:
                json.dump(final_output, f, indent=2)

        if __name__ == '__main__':
            main()
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}
      - --tasks
      - {inputPath: tasks}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --retrained_model
      - {outputPath: retrained_model}
