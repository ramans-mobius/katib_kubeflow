name: New CNN RLAF Loop
description: Triggers the DQN RLAF pipeline in a loop to optimize CNN model hyperparameters for image classification, controlled by a pierce_or_not flag.
inputs:
  - name: trained_model
    type: Model
    description: "Pre-trained CNN model weights"
  - name: init_metrics
    type: Metrics
    description: "Initial performance metrics"
  - name: data_path
    type: Dataset
    description: "Path to processed CNN dataset"
  - name: config
    type: String
    description: "CNN model and training configuration"
  - name: domain
    type: String
    description: "API domain for database operations"
  - name: schema_id
    type: String
    description: "Schema ID for model storage"
  - name: model_id
    type: String
    description: "Model identifier"
  - name: dqn_pipeline_id
    type: String
    description: "DQN pipeline ID for hyperparameter optimization"
  - name: pipeline_domain
    type: String
    description: "Pipeline service domain"
  - name: dqn_experiment_id
    type: String
    description: "DQN experiment identifier"
  - name: access_token
    type: string
    description: "Authentication token"
  - name: tasks
    type: Dataset
    description: "Continual learning tasks from CNN Continual Tasks Generator"
outputs:
  - name: rlaf_output
    type: Dataset
    description: "Final RLAF optimization results"
  - name: retrained_model
    type: Model
    description: "Optimized CNN model after RLAF loop"

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v19
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests || \
        python3 -m pip install --quiet requests --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        import pickle
        import time
        import numpy as np
        from typing import Dict, List, Any, Tuple
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        from torch.utils.data import DataLoader
        from nesy_factory.CNNs.factory import CNNFactory

        # API/DB Helper Functions (same as VAE version)
        def get_retry_session():
            retry_strategy = Retry(
                total=5,
                status_forcelist=[500, 502, 503, 504],
                backoff_factor=1
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session

        def trigger_pipeline(config, pipeline_domain, dqn_params=None):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
            pipeline_params = {"param_json": json.dumps(dqn_params)} if dqn_params else {}
            payload = json.dumps({
                "pipelineType": "ML", "containerResources": {}, "experimentId": config['experiment_id'],
                "enableCaching": True, "parameters": pipeline_params, "version": 1
            })
            print(f"Pipeline trigger payload: {payload}")
            headers = {
                'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}",
                'Content-Type': 'application/json'
            }
            response = http.post(url, headers=headers, data=payload, timeout=30)
            response.raise_for_status()
            return response.json()['runId']

        def get_pipeline_status(config, pipeline_domain):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
            headers = {'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}"}
            response = http.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            pipeline_status = response.json()
            latest_state = pipeline_status['run_details']['state_history'][-1]
            return latest_state['state']

        def get_instance(access_token, domain, schema_id, model_id):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {"dbType": "TIDB", "ownedOnly": True, "filter": {"model_id": model_id}}
            response = http.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            return response.json()['content'][0]

        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {"conditions": [{"field": "model_id", "operator": "EQUAL", "value": model_id}]},
                "partialUpdateRequests": [{"patch": [{"operation": "REPLACE", "path": f"{field}", "value": value}]}]
            }
            response = http.patch(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()

        # CNN Continual Learning Trainer
        class CNNContinualTrainer:
            def __init__(self, config: Dict[str, Any]):
                self.config = config
                self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                
            def train_continual_cnn(self, tasks: List[Dict], model, strategies: List[str] = ['naive']) -> Dict[str, Any]:
                results = {}
                
                for strategy_name in strategies:
                    print(f'Training CNN with {strategy_name.upper()} strategy')
                    strategy_results = self._train_single_strategy(tasks, strategy_name, model)
                    results[strategy_name] = strategy_results
                    
                return results
            
            def _train_single_strategy(self, tasks: List[Dict], strategy_name: str, model) -> Dict[str, Any]:
                task_metrics = []
                all_task_performance = []
                previous_task_data = []
                
                print(f"Learning {len(tasks)} sequential CNN tasks for image classification")
                model.to(self.device)
                
                for task_idx, task_data in enumerate(tasks):
                    print(f"Learning Task {task_idx + 1}: {task_data['description']}")
                    
                    # Prepare training data based on strategy
                    if strategy_name == 'naive':
                        training_loader = task_data['train_loader']
                    elif strategy_name == 'replay':
                        training_loader = self._create_replay_loader(task_data, previous_task_data)
                    else:
                        training_loader = task_data['train_loader']
                    
                    # Train CNN on current task
                    task_metrics_current = self._train_cnn_on_task(model, training_loader, task_data['test_loader'])
                    task_metrics.append(task_metrics_current)
                    
                    # Evaluate on all previous tasks
                    task_performance = []
                    for eval_task_idx in range(task_idx + 1):
                        eval_metrics = self._evaluate_cnn_on_task(model, tasks[eval_task_idx]['test_loader'])
                        task_performance.append({
                            'task_id': eval_task_idx,
                            'accuracy': eval_metrics['accuracy'],
                            'loss': eval_metrics['loss'],
                            'description': tasks[eval_task_idx]['description']
                        })
                    
                    all_task_performance.append(task_performance)
                    
                    if strategy_name == 'replay':
                        previous_task_data.append(task_data)
                        if len(previous_task_data) > 3:
                            previous_task_data = previous_task_data[-3:]
                    
                    print(f"     Task {task_idx + 1} Test Accuracy: {task_metrics_current['accuracy']:.4f}")
                    
                    if task_idx > 0:
                        print(f"Performance on previous tasks:")
                        for prev_task in task_performance[:-1]:
                            print(f"    Task {prev_task['task_id'] + 1}: Accuracy {prev_task['accuracy']:.4f}")
                
                # Calculate continual learning metrics
                cl_metrics = self._calculate_continual_metrics(all_task_performance)
                
                final_eval_metrics = []
                for i in range(len(tasks)):
                    task_eval = self._evaluate_cnn_on_task(model, tasks[i]['test_loader'])
                    final_eval_metrics.append(task_eval)
                
                avg_metrics = {}
                if final_eval_metrics:
                    for key in final_eval_metrics[0]:
                        avg_metrics[key] = np.mean([m[key] for m in final_eval_metrics if key in m])

                results = {
                    'strategy': strategy_name,
                    'task_metrics': task_metrics,
                    'all_task_performance': all_task_performance,
                    'continual_metrics': cl_metrics,
                    'final_model': model,
                    'average_eval_metrics': avg_metrics
                }
                
                return results
            
            def _train_cnn_on_task(self, model, train_loader, test_loader) -> Dict[str, float]:
                model.train()
                model.to(self.device)
                
                # Initialize optimizer if not exists
                if not hasattr(model, 'optimizer'):
                    model.optimizer = torch.optim.AdamW(
                        model.parameters(), 
                        lr=self.config.get('training', {}).get('learning_rate', 0.001),
                        weight_decay=self.config.get('training', {}).get('weight_decay', 0.01)
                    )
                
                criterion = torch.nn.CrossEntropyLoss()
                
                epochs = self.config.get('training', {}).get('epochs', 10)
                
                for epoch in range(epochs):
                    total_loss = 0
                    correct = 0
                    total = 0
                    
                    for batch_idx, (data, targets) in enumerate(train_loader):
                        data, targets = data.to(self.device), targets.to(self.device)
                        
                        model.optimizer.zero_grad()
                        outputs = model(data)
                        loss = criterion(outputs, targets)
                        loss.backward()
                        model.optimizer.step()
                        
                        total_loss += loss.item()
                        _, predicted = outputs.max(1)
                        total += targets.size(0)
                        correct += predicted.eq(targets).sum().item()
                    
                    accuracy = 100. * correct / total
                    avg_loss = total_loss / len(train_loader)
                    
                    if epoch % 5 == 0:
                        print(f"    Epoch {epoch:03d} | Loss: {avg_loss:.4f} | Acc: {accuracy:.2f}%")
                
                # Evaluate on test set
                test_metrics = self._evaluate_cnn_on_task(model, test_loader)
                return test_metrics
            
            def _evaluate_cnn_on_task(self, model, test_loader) -> Dict[str, float]:
                model.eval()
                model.to(self.device)
                
                criterion = torch.nn.CrossEntropyLoss()
                total_loss = 0
                correct = 0
                total = 0
                
                with torch.no_grad():
                    for data, targets in test_loader:
                        data, targets = data.to(self.device), targets.to(self.device)
                        outputs = model(data)
                        loss = criterion(outputs, targets)
                        
                        total_loss += loss.item()
                        _, predicted = outputs.max(1)
                        total += targets.size(0)
                        correct += predicted.eq(targets).sum().item()
                
                accuracy = 100. * correct / total
                avg_loss = total_loss / len(test_loader)
                
                return {
                    'accuracy': accuracy,
                    'loss': avg_loss,
                    'correct': correct,
                    'total': total
                }
            
            def _create_replay_loader(self, current_task: Dict, previous_tasks: List[Dict]) -> DataLoader:
                replay_ratio = 0.3
                current_loader = current_task['train_loader']
                
                # Get current dataset size
                current_size = len(current_loader.dataset)
                replay_size = int(current_size * replay_ratio / (1 - replay_ratio))
                
                replay_data = []
                replay_labels = []
                
                # Sample from previous tasks
                for prev_task in previous_tasks:
                    prev_loader = prev_task['train_loader']
                    prev_dataset = prev_loader.dataset
                    if len(prev_dataset) > 0:
                        sample_size = min(replay_size // len(previous_tasks), len(prev_dataset))
                        if sample_size > 0:
                            indices = torch.randperm(len(prev_dataset))[:sample_size]
                            for idx in indices:
                                data, label = prev_dataset[idx]
                                replay_data.append(data)
                                replay_labels.append(label)
                
                # Combine with current task data
                current_data = []
                current_labels = []
                for data, label in current_loader.dataset:
                    current_data.append(data)
                    current_labels.append(label)
                
                if replay_data:
                    combined_data = current_data + replay_data
                    combined_labels = current_labels + replay_labels
                else:
                    combined_data = current_data
                    combined_labels = current_labels
                
                # Create new dataset and loader
                from torch.utils.data import TensorDataset
                combined_dataset = TensorDataset(
                    torch.stack(combined_data), 
                    torch.tensor(combined_labels)
                )
                
                return DataLoader(
                    combined_dataset, 
                    batch_size=current_loader.batch_size, 
                    shuffle=True
                )
            
            def _calculate_continual_metrics(self, all_task_performance: List[List[Dict]]) -> Dict[str, float]:
                final_performance = all_task_performance[-1]
                average_accuracy = np.mean([task['accuracy'] for task in final_performance])
                average_loss = np.mean([task['loss'] for task in final_performance])
                
                # Calculate forgetting
                forgetting_scores = []
                for task_idx in range(len(all_task_performance) - 1):
                    max_acc = all_task_performance[task_idx][task_idx]['accuracy']
                    final_acc = all_task_performance[-1][task_idx]['accuracy']
                    forgetting = max_acc - final_acc
                    forgetting_scores.append(max(0, forgetting))
                
                avg_forgetting = np.mean(forgetting_scores) if forgetting_scores else 0.0
                
                return {
                    'average_accuracy': average_accuracy,
                    'average_loss': average_loss,
                    'forgetting': avg_forgetting,
                    'num_tasks': len(all_task_performance)
                }

        # Core CNN Retraining Logic
        def cnn_retraining(action, model_path, data_path, config, tasks_path, output_model_path, previous_metrics, dqn_params):
            # Load data and tasks
            with open(data_path, "rb") as f: 
                data = pickle.load(f)
            with open(tasks_path, "rb") as f: 
                tasks_data = pickle.load(f)
            
            # Handle different task data formats
            if isinstance(tasks_data, dict) and 'tasks' in tasks_data:
                tasks = tasks_data['tasks']
            else:
                tasks = tasks_data
            
            # Update config with DQN action
            config.update(action)
            model_config = config.get('model', {})
            
            # Create CNN model using CNNFactory
            try:
                model = CNNFactory.create_model(model_config['architecture'], model_config)
                print(f"Created CNN model: {model_config['architecture']}")
            except Exception as e:
                print(f"Failed to create CNN model: {e}")
                return {"metrics": previous_metrics, "model_path": output_model_path}
            
            # Load pre-trained weights if available
            try:
                if os.path.exists(model_path) and os.path.getsize(model_path) > 0:
                    model_state = torch.load(model_path, map_location='cpu')
                    model.load_state_dict(model_state)
                    print("Loaded pre-trained model weights")
            except Exception as e:
                print(f"Error loading model weights: {e}")
            
            # Train CNN with continual learning
            trainer = CNNContinualTrainer(config)
            continual_strategies = ['naive']
            
            results = trainer.train_continual_cnn(tasks=tasks, strategies=continual_strategies, model=model)
            average_eval_metrics = results['naive']['average_eval_metrics']
            
            # Calculate improvement score based on DQN parameters
            improvement_score = 0
            for param in dqn_params:
                key = param['key']
                sign = 1 if param['sign'] == '+' else -1
                if key in average_eval_metrics and key in previous_metrics:
                    improvement = (average_eval_metrics[key] - previous_metrics[key]) * sign
                    improvement_score += improvement * param.get('mul', 1.0)

            print(f"CNN model improvement score: {improvement_score:.4f}")
            
            # Save model if improved
            os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
            
            if improvement_score > 0:
                print(f"CNN metrics improved (score: {improvement_score:.4f}). Saving model.")
                try:
                    final_model = results['naive']['final_model']
                    torch.save(final_model.state_dict(), output_model_path)
                    print(f"Saved retrained CNN model to {output_model_path}")
                except Exception as e:
                    print(f"Error saving model: {e}")
                    with open(output_model_path, 'w') as f:
                        f.write(f"Error saving CNN model: {e}")
            else:
                print(f"No improvement in CNN metrics (score: {improvement_score:.4f}). Creating placeholder.")
                with open(output_model_path, 'w') as f:
                    f.write("CNN model not saved due to lack of improvement.")

            return {"metrics": average_eval_metrics, "model_path": output_model_path}

        # Pipeline triggering function
        def trigger_and_wait_for_dqn_pipeline(config, pipeline_domain, dqn_params):
            run_id = trigger_pipeline(config, pipeline_domain, dqn_params)
            config["run_id"] = run_id
            while True:
                status = get_pipeline_status(config, pipeline_domain)
                print(f"Current DQN pipeline status: {status}")
                if status == 'SUCCEEDED':
                    print("DQN Pipeline execution completed.")
                    break
                elif status in ['FAILED', 'ERROR']:
                    raise RuntimeError(f"DQN Pipeline failed with status {status}")
                time.sleep(60)

        # Main execution
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            parser.add_argument('--pipeline_domain', type=str, required=True)
            parser.add_argument('--retrained_model', type=str, required=True)
            args = parser.parse_args()

            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            
            # Load initial metrics
            try:
                with open(args.init_metrics, 'r') as f:
                    current_metrics = json.load(f)
            except:
                with open(args.init_metrics, 'rb') as f:
                    metrics_data = pickle.load(f)
                    current_metrics = metrics_data if isinstance(metrics_data, dict) else {"accuracy": 0.5, "loss": 1.0}
 
            action_id_for_next_pierce = -1
 
            for i in range(2):  # RLAF loop iterations
                print(f"=== CNN RLAF Loop Iteration {i+1} ===")
                
                # Prepare metrics for DQN
                cleaned_metrics = {}
                dqn_params = []
                for key, value in current_metrics.items():
                    try:
                        cleaned_metrics[key] = float(value)
                        # Define optimization direction for CNN metrics
                        if any(term in key.lower() for term in ["loss", "error"]):
                            sign = "-"  # Lower is better
                        elif any(term in key.lower() for term in ["accuracy", "f1", "precision", "recall"]):
                            sign = "+"  # Higher is better
                        else:
                            sign = "-"  # Default to minimization
                        dqn_params.append({"key": key, "sign": sign, "mul": 1.0})
                    except (ValueError, TypeError):
                        print(f"Warning: Could not convert CNN metric '{key}' to float. Skipping.")
                
                print(f"Dynamically generated param_json for CNN DQN: {json.dumps(dqn_params)}")

                # Update instance with current state
                instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                
                if instance.get('pierce2rlaf'):
                    latest_pierce2rlaf = instance['pierce2rlaf'][-1]
                    previous_state = latest_pierce2rlaf['current_state']
                    episode = latest_pierce2rlaf['episode']
                else:
                    previous_state = {key: 0.0 for key in cleaned_metrics.keys()}
                    episode = 0
 
                new_pierce2rlaf_entry = {
                    "action_id": action_id_for_next_pierce, 
                    "previous_state": previous_state,
                    "current_state": cleaned_metrics, 
                    "episode": episode, 
                    "timestamp": int(time.time())
                }
                
                pierce2rlaf_history = instance.get("pierce2rlaf", [])
                pierce2rlaf_history.append(new_pierce2rlaf_entry)
                update_instance_field(access_token, args.domain, args.schema_id, args.model_id, "pierce2rlaf", pierce2rlaf_history)

                # Trigger DQN pipeline for CNN optimization
                dqn_config = {
                    "pipeline_id": args.dqn_pipeline_id, 
                    "experiment_id": args.dqn_experiment_id, 
                    "access_token": access_token
                }
                trigger_and_wait_for_dqn_pipeline(dqn_config, args.pipeline_domain, dqn_params)

                # Get DQN recommendation
                updated_instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                
                if not updated_instance.get('rlaf2pierce') or len(updated_instance['rlaf2pierce']) == 0:
                    print("No DQN recommendations found. Using default action.")
                    rlaf_actions = updated_instance.get('rlaf_actions', {}).get('actions', [])
                    if not rlaf_actions:
                        raise ValueError("No actions available in rlaf_actions")
                    action_details = rlaf_actions[0]
                    action_id_for_next_pierce = action_details['id']
                    latest_rlaf2pierce = {"action_id": action_id_for_next_pierce, "pierce_or_not": True}
                else:
                    latest_rlaf2pierce = updated_instance['rlaf2pierce'][-1]
                    
                    if not latest_rlaf2pierce.get("pierce_or_not", True):
                        print("pierce_or_not is false. Exiting CNN RLAF loop.")
                        break
                        
                    print(f"DQN recommendation for CNN: {latest_rlaf2pierce}")
                    rlaf_actions = updated_instance.get('rlaf_actions', {}).get('actions', [])
                    action_id_for_next_pierce = latest_rlaf2pierce['action_id']
                    action_details = next((a for a in rlaf_actions if a["id"] == action_id_for_next_pierce), None)
                    
                    if not action_details:
                        raise ValueError(f"Action with ID {action_id_for_next_pierce} not found in rlaf_actions")
 
                print(f"DQN pipeline recommended CNN action: {action_details}. Retraining CNN model.")
                retraining_results = cnn_retraining(
                    action_details['params'], args.trained_model, args.data_path, json.loads(args.config), args.tasks,
                    args.retrained_model, previous_state, dqn_params
                )
                current_metrics = retraining_results["metrics"]
                print(f"CNN retraining completed. New metrics: {current_metrics}")

            # Save final RLAF output
            os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
            with open(args.rlaf_output, 'w') as f:
                json.dump({
                    "final_metrics": current_metrics,
                    "model_type": "CNN_image_classifier",
                    "rlaf_iterations": i + 1
                }, f, indent=4)
            print(f"CNN RLAF loop finished. Final parameters written to {args.rlaf_output}")

        if __name__ == '__main__':
            main()
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}
      - --tasks
      - {inputPath: tasks}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --retrained_model
      - {outputPath: retrained_model}
