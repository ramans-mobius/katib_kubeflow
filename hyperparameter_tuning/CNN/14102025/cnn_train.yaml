name: CNN Train Model
description: Trains CNN model using provided data and configuration
inputs:
  - name: data_path
    type: Dataset
  - name: model
    type: Model
  - name: config
    type: String
outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v19
    command:
      - python3
      - -u
      - -c
      - |
        import torch, argparse, pickle, os, json, torch.nn as nn, torch.optim as optim
        
        print('=' * 80)
        print('STARTING CNN TRAIN MODEL - DEBUG VERSION')
        print('=' * 80)
        
        class DataWrapper:
            def __init__(self, data_dict=None):
                print('    DataWrapper.__init__ called')
                if data_dict:
                    print('    Updating __dict__ with data_dict')
                    self.__dict__.update(data_dict)
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--data_path', type=str, required=True)
        parser.add_argument('--model', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--training_history', type=str, required=True)
        args = parser.parse_args()

        print('')
        print('=' * 50)
        print('ARGUMENT ANALYSIS')
        print('=' * 50)
        print('data_path: ' + str(args.data_path))
        print('model: ' + str(args.model))
        print('config length: ' + str(len(args.config)) + ' characters')
        print('trained_model: ' + str(args.trained_model))
        print('training_history: ' + str(args.training_history))

        print('')
        print('=' * 50)
        print('STEP 1: CHECKING FILE EXISTENCE')
        print('=' * 50)
        print('data_path exists: ' + str(os.path.exists(args.data_path)))
        print('model path exists: ' + str(os.path.exists(args.model)))
        
        if os.path.exists(args.data_path):
            data_size = os.path.getsize(args.data_path)
            print('data file size: ' + str(data_size) + ' bytes')
        else:
            print('ERROR: data_path does not exist!')
            exit(1)
            
        if os.path.exists(args.model):
            model_size = os.path.getsize(args.model)
            print('model file size: ' + str(model_size) + ' bytes')
        else:
            print('ERROR: model path does not exist!')
            exit(1)

        print('')
        print('=' * 50)
        print('STEP 2: LOADING DATA PICKLE')
        print('=' * 50)
        try:
            print('Attempting to load pickle file...')
            with open(args.data_path, 'rb') as f:
                raw_data = f.read()
            print('Successfully read ' + str(len(raw_data)) + ' bytes from pickle file')
            
            print('Attempting to unpickle...')
            processed_data = pickle.loads(raw_data)
            print('SUCCESS: Pickle loaded successfully!')
            print('Loaded object type: ' + str(type(processed_data)))
            
        except Exception as e:
            print('ERROR loading pickle: ' + str(e))
            print('Full error traceback:')
            import traceback
            traceback.print_exc()
            exit(1)

        print('')
        print('=' * 50)
        print('STEP 3: ANALYZING LOADED DATA STRUCTURE')
        print('=' * 50)
        if hasattr(processed_data, '__dict__'):
            print('Data is an object with __dict__')
            attributes = list(processed_data.__dict__.keys())
            print('Number of attributes: ' + str(len(attributes)))
            print('Attribute names: ' + str(attributes))
            
            for attr_name in attributes:
                attr_value = getattr(processed_data, attr_name)
                print('')
                print('  ' + str(attr_name) + ':')
                print('    Type: ' + str(type(attr_value)))
                
                if attr_value is None:
                    print('    Value: None')
                elif hasattr(attr_value, '__len__'):
                    print('    Length: ' + str(len(attr_value)))
                    
                    if hasattr(attr_value, 'batch_size'):
                        print('    Batch size: ' + str(attr_value.batch_size))
                    if hasattr(attr_value, 'dataset'):
                        print('    Dataset type: ' + str(type(attr_value.dataset)))
                        if hasattr(attr_value.dataset, '__len__'):
                            print('    Dataset length: ' + str(len(attr_value.dataset)))
        else:
            print('Data is not an object')
            print('Type: ' + str(type(processed_data)))

        print('')
        print('=' * 50)
        print('STEP 4: ACCESSING DATA LOADERS')
        print('=' * 50)
        train_loader = None
        test_loader = None
        
        if hasattr(processed_data, 'train_loader'):
            train_loader = processed_data.train_loader
            print('train_loader: ' + str(type(train_loader)))
            if train_loader is not None:
                print('  Number of batches: ' + str(len(train_loader)))
                if hasattr(train_loader, 'batch_size'):
                    print('  Batch size: ' + str(train_loader.batch_size))
            else:
                print('  train_loader is None!')
        else:
            print('ERROR: train_loader attribute not found!')
            exit(1)
            
        if hasattr(processed_data, 'test_loader'):
            test_loader = processed_data.test_loader
            print('test_loader: ' + str(type(test_loader)))
            if test_loader is not None:
                print('  Number of batches: ' + str(len(test_loader)))
            else:
                print('  test_loader is None!')
        else:
            print('WARNING: test_loader attribute not found')

        print('')
        print('=' * 50)
        print('STEP 5: LOADING CONFIG')
        print('=' * 50)
        try:
            config = json.loads(args.config)
            print('SUCCESS: Config parsed as JSON')
            print('Config keys: ' + str(list(config.keys())))
            
            training_config = config.get('training', {})
            model_config = config.get('model', {})
            
            print('Training config keys: ' + str(list(training_config.keys())))
            print('Model config keys: ' + str(list(model_config.keys())))
            
            epochs = training_config.get('epochs', 5)
            learning_rate = training_config.get('optimizer', {}).get('learning_rate', 0.001)
            
            print('Epochs: ' + str(epochs))
            print('Learning rate: ' + str(learning_rate))
            
        except Exception as e:
            print('ERROR loading config: ' + str(e))
            import traceback
            traceback.print_exc()
            exit(1)

        print('')
        print('=' * 50)
        print('STEP 6: LOADING MODEL')
        print('=' * 50)
        try:
            from nesy_factory.CNNs.factory import CNNFactory
            print('SUCCESS: Imported CNNFactory')
            
            model_architecture = model_config.get('architecture', 'resnet')
            print('Creating model with architecture: ' + str(model_architecture))
            
            model = CNNFactory.create_model(model_architecture, model_config)
            print('SUCCESS: Model created')
            
            print('Loading state dict from: ' + str(args.model))
            model.load_state_dict(torch.load(args.model, map_location=torch.device('cpu')))
            print('SUCCESS: Model weights loaded')
            
        except Exception as e:
            print('ERROR loading model: ' + str(e))
            import traceback
            traceback.print_exc()
            exit(1)

        print('')
        print('=' * 50)
        print('STEP 7: SETTING UP TRAINING')
        print('=' * 50)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print('Using device: ' + str(device))
        
        model = model.to(device)
        print('Model moved to device')
        
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        print('Optimizer and criterion created')

        print('')
        print('=' * 50)
        print('STEP 8: DATA LOADER SANITY CHECK')
        print('=' * 50)
        if train_loader is None:
            print('ERROR: train_loader is None, cannot train!')
            exit(1)
            
        print('Testing one batch from train_loader...')
        try:
            data_iter = iter(train_loader)
            first_batch = next(data_iter)
            print('First batch type: ' + str(type(first_batch)))
            
            if isinstance(first_batch, (list, tuple)) and len(first_batch) >= 2:
                images, labels = first_batch[0], first_batch[1]
                print('  Images shape: ' + str(images.shape))
                print('  Labels shape: ' + str(labels.shape))
            else:
                print('  Unexpected batch structure')
                
        except Exception as e:
            print('ERROR testing data loader: ' + str(e))
            import traceback
            traceback.print_exc()
            exit(1)

        print('')
        print('=' * 50)
        print('STEP 9: STARTING TRAINING LOOP')
        print('=' * 50)
        training_history = []
        best_val_acc = 0.0

        for epoch in range(epochs):
            print('')
            print('--- Epoch ' + str(epoch+1) + '/' + str(epochs) + ' ---')
            
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            batch_count = 0
            
            print('Training phase...')
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = output.max(1)
                train_total += target.size(0)
                train_correct += predicted.eq(target).sum().item()
                batch_count += 1
                
                if batch_idx % 10 == 0:
                    print('  Batch ' + str(batch_idx) + ': loss=' + str(loss.item()))
            
            train_acc = 100.0 * train_correct / train_total
            avg_train_loss = train_loss / batch_count
            
            val_acc = 0.0
            if test_loader is not None:
                print('Validation phase...')
                model.eval()
                val_correct = 0
                val_total = 0
                
                with torch.no_grad():
                    for data, target in test_loader:
                        data, target = data.to(device), target.to(device)
                        output = model(data)
                        _, predicted = output.max(1)
                        val_total += target.size(0)
                        val_correct += predicted.eq(target).sum().item()
                
                val_acc = 100.0 * val_correct / val_total
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    best_model_state = model.state_dict().copy()
                    print('  New best validation accuracy: ' + str(val_acc))
            else:
                print('No validation loader available')
                best_model_state = model.state_dict().copy()
            
            epoch_info = {
                'epoch': epoch + 1,
                'train_loss': avg_train_loss,
                'train_accuracy': train_acc,
                'val_accuracy': val_acc
            }
            training_history.append(epoch_info)
            
            print('Epoch ' + str(epoch+1) + ' Summary:')
            print('  Train Loss: ' + str(avg_train_loss))
            print('  Train Accuracy: ' + str(train_acc))
            print('  Val Accuracy: ' + str(val_acc))

        print('')
        print('=' * 50)
        print('STEP 10: SAVING RESULTS')
        print('=' * 50)
        try:
            model.load_state_dict(best_model_state)
            
            os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
            os.makedirs(os.path.dirname(args.training_history), exist_ok=True)
            
            print('Saving trained model to: ' + str(args.trained_model))
            torch.save(model.state_dict(), args.trained_model)
            
            print('Saving training history to: ' + str(args.training_history))
            with open(args.training_history, 'w') as f:
                json.dump(training_history, f, indent=2)
                
            print('SUCCESS: All outputs saved')
            
        except Exception as e:
            print('ERROR saving results: ' + str(e))
            import traceback
            traceback.print_exc()
            exit(1)

        print('')
        print('=' * 80)
        print('TRAINING COMPLETED SUCCESSFULLY!')
        print('=' * 80)
        print('Best validation accuracy: ' + str(best_val_acc))
        print('Final model saved: ' + str(args.trained_model))
        print('Training history saved: ' + str(args.training_history))
    args:
      - --data_path
      - {inputPath: data_path}
      - --model
      - {inputPath: model}
      - --config
      - {inputValue: config}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
