name: 2 CNN Download Artifacts
description: Downloads CNN model artifacts from CDN URLs for inference and prints detailed information
inputs:
  - name: model_weights_url
    type: String
    description: URL to fetch the model weights file
  - name: model_config_url
    type: String
    description: URL to fetch the model configuration
  - name: class_labels_url
    type: String
    description: URL to fetch class labels mapping
  - name: inference_config_url
    type: String
    description: URL to fetch inference configuration
  - name: preprocess_data_url
    type: String
    description: URL to fetch preprocessed dataset
outputs:
  - name: model_weights
    type: Model
    description: Downloaded model weights
  - name: model_config
    type: String
    description: Downloaded model configuration
  - name: class_labels
    type: String
    description: Downloaded class labels
  - name: inference_config
    type: String
    description: Downloaded inference configuration
  - name: preprocess_data
    type: Dataset
    description: Downloaded preprocessed dataset

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        python3 -m pip install --quiet requests
        
        # Write URLs to files to avoid command line parsing issues
        echo "$1" | tr -d '\n\r' > /tmp/model_weights_url.txt
        echo "$2" | tr -d '\n\r' > /tmp/model_config_url.txt
        echo "$3" | tr -d '\n\r' > /tmp/class_labels_url.txt
        echo "$4" | tr -d '\n\r' > /tmp/inference_config_url.txt
        echo "$5" | tr -d '\n\r' > /tmp/preprocess_data_url.txt
        
        python3 -u -c "
import os, requests, json, pickle, urllib.parse, io

def read_url_from_file(file_path):
    with open(file_path, 'r') as f:
        return f.read().strip()

def download_file(url, output_path, description):
    # Decode URL-encoded characters
    decoded_url = urllib.parse.unquote(url)
    print(f'Fetching {description} from: {decoded_url}')
    r = requests.get(decoded_url)
    r.raise_for_status()
    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)
    with open(output_path, 'wb') as f:
        f.write(r.content)
    print(f'{description} saved to {output_path}')
    return output_path

# Read URLs from files
model_weights_url = read_url_from_file('/tmp/model_weights_url.txt')
model_config_url = read_url_from_file('/tmp/model_config_url.txt')
class_labels_url = read_url_from_file('/tmp/class_labels_url.txt')
inference_config_url = read_url_from_file('/tmp/inference_config_url.txt')
preprocess_data_url = read_url_from_file('/tmp/preprocess_data_url.txt')

# Output paths
model_weights_path = '/tmp/outputs/model_weights/data'
model_config_path = '/tmp/outputs/model_config/data'
class_labels_path = '/tmp/outputs/class_labels/data'
inference_config_path = '/tmp/outputs/inference_config/data'
preprocess_data_path = '/tmp/outputs/preprocess_data/data'

# Download all files
download_file(model_weights_url, model_weights_path, 'model weights')
download_file(model_config_url, model_config_path, 'model config')
download_file(class_labels_url, class_labels_path, 'class labels')
download_file(inference_config_url, inference_config_path, 'inference config')
download_file(preprocess_data_url, preprocess_data_path, 'preprocessed data')

print('DEBUG INFORMATION - PRINTING ALL CONFIGURATIONS')

print('MODEL CONFIG:')
with open(model_config_path, 'r') as f:
    model_config = json.load(f)
print(json.dumps(model_config, indent=2))

model_info = model_config.get('model_info', {})
if 'output_dim' not in model_info and 'num_classes' not in model_info:
    print('WARNING: output_dim or num_classes not found in model config')
else:
    output_dim = model_info.get('output_dim') or model_info.get('num_classes')
    print(f'Model output dimension: {output_dim}')

print('CLASS LABELS:')
with open(class_labels_path, 'r') as f:
    class_labels = json.load(f)
print(f'Number of classes: {len(class_labels)}')
print('Class mapping:')
for class_id, class_name in class_labels.items():
    print(f'  {class_id} -> {class_name}')

print('INFERENCE CONFIG:')
with open(inference_config_path, 'r') as f:
    inference_config = json.load(f)
print(json.dumps(inference_config, indent=2))

print('PREPROCESSED DATA ANALYSIS:')
try:
    with open(preprocess_data_path, 'rb') as f:
        raw_data = f.read()
    
    class LabeledDataset:
        def __init__(self, dataset=None, label_mapping=None):
            self.dataset = dataset or []
            self.label_mapping = label_mapping or {}
        def __len__(self):
            try:
                if hasattr(self.dataset, '__len__'):
                    return len(self.dataset)
                return 100
            except:
                return 100
        def __getitem__(self, idx):
            try:
                if hasattr(self.dataset, '__getitem__'):
                    item = self.dataset[idx]
                    if isinstance(item, tuple) and len(item) == 2:
                        data, label = item
                    elif isinstance(item, dict):
                        data = item.get('image_data')
                        label = item.get('label', 0)
                        return data, label
                    else:
                        return item, 0
            except:
                pass
            return 'dummy_data', 0

    class SimpleDataset:
        def __init__(self, data=None):
            self.data = data or []
        def __len__(self):
            try:
                if hasattr(self.data, '__len__'):
                    length = len(self.data)
                    if length > 0:
                        return length
            except:
                pass
            return 100
        def __getitem__(self, idx):
            try:
                if hasattr(self.data, '__getitem__'):
                    item = self.data[idx]
                    if isinstance(item, tuple) and len(item) == 2:
                        return item
                    elif isinstance(item, dict):
                        data = item.get('image_data')
                        label = item.get('label', 0)
                        return data, label
                    else:
                        return item, 0
            except:
                pass
            return 'dummy_data', 0

    class DataWrapper:
        def __init__(self, data_dict=None):
            if data_dict:
                self.__dict__.update(data_dict)

    class SafeUnpickler(pickle.Unpickler):
        def find_class(self, module, name):
            try:
                return super().find_class(module, name)
            except:
                if name == 'LabeledDataset':
                    return LabeledDataset
                elif name == 'DataWrapper':
                    return DataWrapper
                elif name == 'SimpleDataset':
                    return SimpleDataset
                else:
                    class FallbackClass:
                        def __init__(self, *args, **kwargs):
                            pass
                    return FallbackClass

    processed_data = SafeUnpickler(io.BytesIO(raw_data)).load()
    print('Preprocessed data loaded successfully')
    
    print(f'Data type: {type(processed_data)}')
    print(f'Data attributes: {[attr for attr in dir(processed_data) if not attr.startswith(\"_\")]}')
    
    found_output_dim = None
    
    if hasattr(processed_data, 'class_names'):
        print(f'Found class_names: {processed_data.class_names}')
        found_output_dim = len(processed_data.class_names)
        print(f'Number of classes in data: {found_output_dim}')
        
    if hasattr(processed_data, 'num_classes'):
        print(f'Found num_classes: {processed_data.num_classes}')
        found_output_dim = processed_data.num_classes
        
    if hasattr(processed_data, 'label_mapping'):
        print(f'Found label_mapping: {processed_data.label_mapping}')
        if found_output_dim is None:
            found_output_dim = len(processed_data.label_mapping)
            
    if hasattr(processed_data, 'train_loader'):
        print('Found train_loader')
        
    if hasattr(processed_data, 'test_loader'):
        print('Found test_loader')
        try:
            test_loader = processed_data.test_loader
            if hasattr(test_loader, 'dataset'):
                dataset = test_loader.dataset
                print(f'Test dataset length: {len(dataset)}')
                if hasattr(dataset, 'classes'):
                    print(f'Dataset classes: {dataset.classes}')
                    if found_output_dim is None:
                        found_output_dim = len(dataset.classes)
        except Exception as e:
            print(f'Could not analyze test_loader: {e}')
            
    if found_output_dim:
        print(f'OUTPUT_DIM FOUND FROM DATA: {found_output_dim}')
        
        if 'output_dim' not in model_info and 'num_classes' not in model_info:
            print('SUGGESTION: Add output_dim to model config:')
            print(f'  \"output_dim\": {found_output_dim}')
        else:
            config_output_dim = model_info.get('output_dim') or model_info.get('num_classes')
            if config_output_dim != found_output_dim:
                print(f'WARNING: Config output_dim ({config_output_dim}) != Data output_dim ({found_output_dim})')
            else:
                print('SUCCESS: Config output_dim matches data output_dim')
    else:
        print('WARNING: Could not determine output_dim from data')
        
except Exception as e:
    print(f'Error analyzing preprocessed data: {e}')
    import traceback
    traceback.print_exc()

print('CONSISTENCY CHECK:')
expected_classes = model_info.get('output_dim') or model_info.get('num_classes')
actual_classes = len(class_labels)

if expected_classes and actual_classes:
    if expected_classes == actual_classes:
        print(f'SUCCESS: Model output_dim ({expected_classes}) matches class labels count ({actual_classes})')
    else:
        print(f'ERROR: Model output_dim ({expected_classes}) != class labels count ({actual_classes})')
else:
    print('WARNING: Cannot perform consistency check - missing output_dim or class labels')

print('DEBUG COMPLETE')
print('All CNN artifacts downloaded successfully')
"
    args:
      - {inputValue: model_weights_url}
      - {inputValue: model_config_url}
      - {inputValue: class_labels_url}
      - {inputValue: inference_config_url}
      - {inputValue: preprocess_data_url}
