name: Pls Pls Pls CNN Download Artifacts
description: Downloads CNN model artifacts from CDN URLs for inference and prints detailed information
inputs:
  - name: model_weights_url
    type: String
    description: URL to fetch the model weights file
  - name: model_config_url
    type: String
    description: URL to fetch the model configuration
  - name: class_labels_url
    type: String
    description: URL to fetch class labels mapping
  - name: inference_config_url
    type: String
    description: URL to fetch inference configuration
  - name: preprocess_data_url
    type: String
    description: URL to fetch preprocessed dataset
outputs:
  - name: model_weights
    type: Model
    description: Downloaded model weights
  - name: model_config
    type: String
    description: Downloaded model configuration
  - name: class_labels
    type: String
    description: Downloaded class labels
  - name: inference_config
    type: String
    description: Downloaded inference configuration
  - name: preprocess_data
    type: Dataset
    description: Downloaded preprocessed dataset

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        python3 -m pip install --quiet requests
        python3 -c "
        import sys, os, requests, json, pickle, urllib.parse, io
        
        print('Number of arguments:', len(sys.argv))
        print('Arguments:', sys.argv)
        
        # Get args directly from command line
        model_weights_url = sys.argv[1]
        model_config_url = sys.argv[2]
        class_labels_url = sys.argv[3]
        inference_config_url = sys.argv[4]
        preprocess_data_url = sys.argv[5]
        model_weights_path = sys.argv[6]
        model_config_path = sys.argv[7]
        class_labels_path = sys.argv[8]
        inference_config_path = sys.argv[9]
        preprocess_data_path = sys.argv[10]
        
        # Print all input parameters with their names and values
        print('=== INPUT PARAMETERS RECEIVED ===')
        print(f'1. model_weights_url: {repr(model_weights_url)}')
        print(f'2. model_config_url: {repr(model_config_url)}')
        print(f'3. class_labels_url: {repr(class_labels_url)}')
        print(f'4. inference_config_url: {repr(inference_config_url)}')
        print(f'5. preprocess_data_url: {repr(preprocess_data_url)}')
        print(f'6. model_weights_path: {model_weights_path}')
        print(f'7. model_config_path: {model_config_path}')
        print(f'8. class_labels_path: {class_labels_path}')
        print(f'9. inference_config_path: {inference_config_path}')
        print(f'10. preprocess_data_path: {preprocess_data_path}')
        print('=== END INPUT PARAMETERS ===')
        
        def download_file(url, output_path, description):
            # Decode URL-encoded characters
            decoded_url = urllib.parse.unquote(url)
            print(f'Fetching {description} from: {decoded_url}')
            r = requests.get(decoded_url)
            r.raise_for_status()
            os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)
            with open(output_path, 'wb') as f:
                f.write(r.content)
            print(f'{description} saved to {output_path}')
            return output_path

        # Download all files
        download_file(model_weights_url, model_weights_path, 'model weights')
        download_file(model_config_url, model_config_path, 'model config')
        download_file(class_labels_url, class_labels_path, 'class labels')
        download_file(inference_config_url, inference_config_path, 'inference config')
        download_file(preprocess_data_url, preprocess_data_path, 'preprocessed data')

        print('DEBUG INFORMATION - PRINTING ALL CONFIGURATIONS')

        print('MODEL CONFIG:')
        with open(model_config_path, 'r') as f:
            model_config = json.load(f)
        print(json.dumps(model_config, indent=2))
        
        model_info = model_config.get('model_info', {})
        if 'output_dim' not in model_info and 'num_classes' not in model_info:
            print('WARNING: output_dim or num_classes not found in model config')
        else:
            output_dim = model_info.get('output_dim') or model_info.get('num_classes')
            print(f'Model output dimension: {output_dim}')

        print('CLASS LABELS:')
        with open(class_labels_path, 'r') as f:
            class_labels = json.load(f)
        print(f'Number of classes: {len(class_labels)}')
        print('Class mapping:')
        for class_id, class_name in class_labels.items():
            print(f'  {class_id} -> {class_name}')

        print('INFERENCE CONFIG:')
        with open(inference_config_path, 'r') as f:
            inference_config = json.load(f)
        print(json.dumps(inference_config, indent=2))

        print('PREPROCESSED DATA ANALYSIS:')
        try:
            with open(preprocess_data_path, 'rb') as f:
                raw_data = f.read()
            
            class LabeledDataset:
                def __init__(self, dataset=None, label_mapping=None):
                    self.dataset = dataset or []
                    self.label_mapping = label_mapping or {}
                def __len__(self):
                    try:
                        if hasattr(self.dataset, '__len__'):
                            return len(self.dataset)
                        return 100
                    except:
                        return 100
                def __getitem__(self, idx):
                    try:
                        if hasattr(self.dataset, '__getitem__'):
                            item = self.dataset[idx]
                            if isinstance(item, tuple) and len(item) == 2:
                                data, label = item
                            elif isinstance(item, dict):
                                data = item.get('image_data')
                                label = item.get('label', 0)
                                return data, label
                            else:
                                return item, 0
                    except:
                        pass
                    return 'dummy_data', 0

            class SimpleDataset:
                def __init__(self, data=None):
                    self.data = data or []
                def __len__(self):
                    try:
                        if hasattr(self.data, '__len__'):
                            length = len(self.data)
                            if length > 0:
                                return length
                    except:
                        pass
                    return 100
                def __getitem__(self, idx):
                    try:
                        if hasattr(self.data, '__getitem__'):
                            item = self.data[idx]
                            if isinstance(item, tuple) and len(item) == 2:
                                return item
                            elif isinstance(item, dict):
                                data = item.get('image_data')
                                label = item.get('label', 0)
                                return data, label
                            else:
                                return item, 0
                    except:
                        pass
                    return 'dummy_data', 0

            class DataWrapper:
                def __init__(self, data_dict=None):
                    if data_dict:
                        self.__dict__.update(data_dict)

            class SafeUnpickler(pickle.Unpickler):
                def find_class(self, module, name):
                    try:
                        return super().find_class(module, name)
                    except:
                        if name == 'LabeledDataset':
                            return LabeledDataset
                        elif name == 'DataWrapper':
                            return DataWrapper
                        elif name == 'SimpleDataset':
                            return SimpleDataset
                        else:
                            class FallbackClass:
                                def __init__(self, *args, **kwargs):
                                    pass
                            return FallbackClass

            processed_data = SafeUnpickler(io.BytesIO(raw_data)).load()
            print('Preprocessed data loaded successfully')
            
            print(f'Data type: {type(processed_data)}')
            print(f'Data attributes: {[attr for attr in dir(processed_data) if not attr.startswith(\"_\")]}')
            
            found_output_dim = None
            
            if hasattr(processed_data, 'class_names'):
                print(f'Found class_names: {processed_data.class_names}')
                found_output_dim = len(processed_data.class_names)
                print(f'Number of classes in data: {found_output_dim}')
                
            if hasattr(processed_data, 'num_classes'):
                print(f'Found num_classes: {processed_data.num_classes}')
                found_output_dim = processed_data.num_classes
                
            if hasattr(processed_data, 'label_mapping'):
                print(f'Found label_mapping: {processed_data.label_mapping}')
                if found_output_dim is None:
                    found_output_dim = len(processed_data.label_mapping)
                    
            if hasattr(processed_data, 'train_loader'):
                print('Found train_loader')
                
            if hasattr(processed_data, 'test_loader'):
                print('Found test_loader')
                try:
                    test_loader = processed_data.test_loader
                    if hasattr(test_loader, 'dataset'):
                        dataset = test_loader.dataset
                        print(f'Test dataset length: {len(dataset)}')
                        if hasattr(dataset, 'classes'):
                            print(f'Dataset classes: {dataset.classes}')
                            if found_output_dim is None:
                                found_output_dim = len(dataset.classes)
                except Exception as e:
                    print(f'Could not analyze test_loader: {e}')
                    
            if found_output_dim:
                print(f'OUTPUT_DIM FOUND FROM DATA: {found_output_dim}')
                
                if 'output_dim' not in model_info and 'num_classes' not in model_info:
                    print('SUGGESTION: Add output_dim to model config:')
                    print(f'  \"output_dim\": {found_output_dim}')
                else:
                    config_output_dim = model_info.get('output_dim') or model_info.get('num_classes')
                    if config_output_dim != found_output_dim:
                        print(f'WARNING: Config output_dim ({config_output_dim}) != Data output_dim ({found_output_dim})')
                    else:
                        print('SUCCESS: Config output_dim matches data output_dim')
            else:
                print('WARNING: Could not determine output_dim from data')
                
        except Exception as e:
            print(f'Error analyzing preprocessed data: {e}')
            import traceback
            traceback.print_exc()

        print('CONSISTENCY CHECK:')
        expected_classes = model_info.get('output_dim') or model_info.get('num_classes')
        actual_classes = len(class_labels)
        
        if expected_classes and actual_classes:
            if expected_classes == actual_classes:
                print(f'SUCCESS: Model output_dim ({expected_classes}) matches class labels count ({actual_classes})')
            else:
                print(f'ERROR: Model output_dim ({expected_classes}) != class labels count ({actual_classes})')
        else:
            print('WARNING: Cannot perform consistency check - missing output_dim or class labels')

        print('DEBUG COMPLETE')
        print('All CNN artifacts downloaded successfully')
        " "$1" "$2" "$3" "$4" "$5" "$6" "$7" "$8" "$9" "$10"
    args:
      - {inputValue: model_weights_url}
      - {inputValue: model_config_url}
      - {inputValue: class_labels_url}
      - {inputValue: inference_config_url}
      - {inputValue: preprocess_data_url}
      - {outputPath: model_weights}
      - {outputPath: model_config}
      - {outputPath: class_labels}
      - {outputPath: inference_config}
      - {outputPath: preprocess_data}
