name: CNN Evaluate Model
description: Evaluates trained CNN model and generates performance metrics
inputs:
  - name: trained_model
    type: Model
  - name: data_path
    type: Dataset
  - name: config
    type: String
outputs:
  - name: metrics
    type: Metrics
  - name: metrics_json
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v19
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch, argparse, pickle, json, os, numpy as np
        from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--data_path', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--metrics', type=str, required=True)
        parser.add_argument('--metrics_json', type=str, required=True)
        args = parser.parse_args()

        print(f"Starting CNN Evaluation")
        print(f"Trained model path: {args.trained_model}")
        print(f"Data path: {args.data_path}")

        # Load the preprocessed data
        try:
            with open(args.data_path, "rb") as f:
                processed_data = pickle.load(f)
            print("Data loaded successfully")
        except Exception as e:
            print(f"Error loading data: {e}")
            exit(1)

        # Load config
        try:
            config = json.loads(args.config)
            model_config = config.get('model', {})
            print("Config loaded successfully")
        except Exception as e:
            print(f"Error loading config: {e}")
            exit(1)

        # Load model
        try:
            from nesy_factory.CNNs.factory import CNNFactory
            model = CNNFactory.create_model(model_config.get('architecture', 'resnet'), model_config)
            model.load_state_dict(torch.load(args.trained_model, map_location=torch.device('cpu')))
            print("Model loaded successfully")
        except Exception as e:
            print(f"Error loading model: {e}")
            exit(1)

        # Evaluation
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        model.eval()

        test_loader = processed_data.test_loader
        if test_loader is None:
            print("test_loader not found in data")
            exit(1)

        print("Starting evaluation...")
        
        all_predictions = []
        all_targets = []
        total_loss = 0.0
        criterion = torch.nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                
                # Calculate loss
                loss = criterion(output, target)
                total_loss += loss.item()
                
                _, predictions = output.max(1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(target.cpu().numpy())

        # Calculate metrics
        if len(all_targets) > 0:
            accuracy = accuracy_score(all_targets, all_predictions) * 100
            avg_loss = total_loss / len(test_loader)
            
            # Classification report
            class_report = classification_report(all_targets, all_predictions, output_dict=True)
            
            # Confusion matrix
            cm = confusion_matrix(all_targets, all_predictions)
        else:
            accuracy = 0.0
            avg_loss = 0.0
            class_report = {}
            cm = []

        # Create comprehensive metrics
        metrics = {
            'accuracy': accuracy,
            'loss': avg_loss,
            'total_samples': len(all_targets),
            'correct_predictions': np.sum(np.array(all_predictions) == np.array(all_targets)),
            'classification_report': class_report,
            'confusion_matrix': cm.tolist() if hasattr(cm, 'tolist') else cm
        }

        # Save metrics
        os.makedirs(os.path.dirname(args.metrics), exist_ok=True)
        os.makedirs(os.path.dirname(args.metrics_json), exist_ok=True)

        with open(args.metrics, 'w') as f:
            json.dump(metrics, f, indent=2)
            
        with open(args.metrics_json, 'w') as f:
            json.dump(metrics, f, indent=2)

        print(f"Evaluation complete!")
        print(f"   Accuracy: {accuracy:.2f}%")
        print(f"   Loss: {avg_loss:.4f}")
        print(f"   Correct: {metrics['correct_predictions']}/{metrics['total_samples']}")
        print(f"Metrics saved to: {args.metrics}")
        print(f"Metrics JSON saved to: {args.metrics_json}")

    args:
      - --trained_model
      - {inputPath: trained_model}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
