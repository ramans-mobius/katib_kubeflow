name: dataset CNN Evaluate Model
description: Evaluates trained CNN model and generates performance metrics
inputs:
  - name: trained_model
    type: Model
  - name: data_path
    type: Dataset
  - name: config
    type: String
outputs:
  - name: metrics
    type: Metrics
  - name: metrics_json
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v19
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch, argparse, pickle, json, os, numpy as np
        from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--data_path', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--metrics', type=str, required=True)
        parser.add_argument('--metrics_json', type=str, required=True)
        args = parser.parse_args()

        print(f"Trained model path: {args.trained_model}")
        print(f"Data path: {args.data_path}")

        # Load data
        try:
            with open(args.data_path, "rb") as f:
                processed_data = pickle.load(f)
            print("Successfully loaded data")
        except Exception as e:
            print(f"Error loading data: {e}")
            exit(1)

        # Load config
        try:
            config = json.loads(args.config)
            print("Successfully loaded config")
        except Exception as e:
            print(f"Error loading config: {e}")
            exit(1)

        # Load model
        try:
            from nesy_factory.CNNs.factory import CNNFactory
            model_config = config.get('model', {})
            model = CNNFactory.create_model(model_config.get('architecture', 'resnet'), model_config)
            model.load_state_dict(torch.load(args.trained_model, map_location=torch.device('cpu')))
            print("Successfully loaded model")
        except Exception as e:
            print(f"Error loading model: {e}")
            exit(1)

        # Evaluation
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        model.eval()

        test_loader = processed_data.get('test_loader')
        if test_loader is None:
            print("ERROR: test_loader not found in data")
            exit(1)

        print("Starting evaluation...")
        
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                _, predictions = output.max(1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(target.cpu().numpy())

        # Calculate metrics
        accuracy = accuracy_score(all_targets, all_predictions) * 100
        
        # Classification report
        class_report = classification_report(all_targets, all_predictions, output_dict=True)
        
        # Confusion matrix
        cm = confusion_matrix(all_targets, all_predictions)
        
        # Create comprehensive metrics
        metrics = {
            'overall_accuracy': accuracy,
            'classification_report': class_report,
            'confusion_matrix': cm.tolist(),
            'predictions_summary': {
                'total_samples': len(all_predictions),
                'correct_predictions': np.sum(np.array(all_predictions) == np.array(all_targets)),
                'incorrect_predictions': np.sum(np.array(all_predictions) != np.array(all_targets))
            }
        }

        # Save metrics
        os.makedirs(os.path.dirname(args.metrics), exist_ok=True)
        os.makedirs(os.path.dirname(args.metrics_json), exist_ok=True)

        with open(args.metrics, 'w') as f:
            json.dump(metrics, f, indent=2)
            
        with open(args.metrics_json, 'w') as f:
            json.dump(metrics, f, indent=2)

        print(f"Evaluation completed! Test Accuracy: {accuracy:.2f}%")
        print(f"Metrics saved to {args.metrics}")
        print(f"Metrics JSON saved to {args.metrics_json}")

    args:
      - --trained_model
      - {inputPath: trained_model}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
