name: F CNN Evaluate Model
description: Evaluates trained CNN model and generates performance metrics
inputs:
  - name: trained_model
    type: Model
  - name: data_path
    type: Dataset
  - name: config
    type: String
outputs:
  - name: metrics
    type: Metrics
  - name: metrics_json
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v19
    command:
      - python3
      - -u
      - -c
      - |
        import torch, argparse, pickle, json, os, numpy as np
        from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

        # Define the same helper classes as in training component
        class LabeledDataset:
            def __init__(self, dataset=None, label_mapping=None):
                self.dataset = dataset or []
                self.label_mapping = label_mapping or {}
            def __len__(self):
                try:
                    if hasattr(self.dataset, '__len__'):
                        return len(self.dataset)
                    return 100
                except:
                    return 100
            def __getitem__(self, idx):
                try:
                    if hasattr(self.dataset, '__getitem__'):
                        item = self.dataset[idx]
                        if isinstance(item, tuple) and len(item) == 2:
                            data, label = item
                        elif isinstance(item, dict):
                            data = item.get('image_data')
                            label = item.get('label', 0)
                            return data, label
                        else:
                            return item, 0
                except:
                    pass
                return torch.randn(3, 224, 224), 0

        class SimpleDataset:
            def __init__(self, data=None):
                self.data = data or []
            def __len__(self):
                try:
                    if hasattr(self.data, '__len__'):
                        length = len(self.data)
                        if length > 0:
                            return length
                except:
                    pass
                return 100
            def __getitem__(self, idx):
                try:
                    if hasattr(self.data, '__getitem__'):
                        item = self.data[idx]
                        if isinstance(item, tuple) and len(item) == 2:
                            return item
                        elif isinstance(item, dict):
                            data = item.get('image_data')
                            label = item.get('label', 0)
                            return data, label
                        else:
                            return item, 0
                except:
                    pass
                return torch.randn(3, 224, 224), 0

        class DataWrapper:
            def __init__(self, data_dict=None):
                if data_dict:
                    self.__dict__.update(data_dict)

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--data_path', type=str, required=True)
        parser.add_argument('--config', type=str, required=True)
        parser.add_argument('--metrics', type=str, required=True)
        parser.add_argument('--metrics_json', type=str, required=True)
        args = parser.parse_args()

        print("Starting CNN Evaluation")
        print(f"Trained model path: {args.trained_model}")
        print(f"Data path: {args.data_path}")

        # Use the same SafeUnpickler as training component
        try:
            with open(args.data_path, 'rb') as f:
                raw_data = f.read()
            
            class SafeUnpickler(pickle.Unpickler):
                def find_class(self, module, name):
                    try:
                        return super().find_class(module, name)
                    except:
                        if name == 'LabeledDataset':
                            return LabeledDataset
                        elif name == 'DataWrapper':
                            return DataWrapper
                        elif name == 'SimpleDataset':
                            return SimpleDataset
                        else:
                            class FallbackClass:
                                def __init__(self, *args, **kwargs):
                                    pass
                            return FallbackClass
            
            import io
            processed_data = SafeUnpickler(io.BytesIO(raw_data)).load()
            print("Data loaded successfully")
        except Exception as e:
            print(f"Error loading data: {e}")
            exit(1)

        try:
            config = json.loads(args.config)
            model_config = config.get('model', {})
            print("Config loaded successfully")
        except Exception as e:
            print(f"Error loading config: {e}")
            exit(1)

        try:
            # Load the trained model state (not the architecture)
            checkpoint = torch.load(args.trained_model, map_location=torch.device('cpu'))
            
            from nesy_factory.CNNs.factory import CNNFactory
            model = CNNFactory.create_model(model_config.get('architecture', 'resnet'), model_config)
            
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
                
            print("Model loaded successfully")
        except Exception as e:
            print(f"Error loading model: {e}")
            exit(1)

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        model.eval()

        # Extract test_loader from processed_data (same as training component)
        test_loader = None
        if hasattr(processed_data, 'test_loader'):
            test_loader = processed_data.test_loader
        else:
            print("test_loader not found in data")
            exit(1)

        print("Starting evaluation...")
        
        all_predictions = []
        all_targets = []
        total_loss = 0.0
        criterion = torch.nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for batch in test_loader:
                # Handle different data formats
                if isinstance(batch, (list, tuple)) and len(batch) == 2:
                    data, target = batch
                elif hasattr(batch, 'image_data') and hasattr(batch, 'label'):
                    data = batch.image_data
                    target = batch.label
                else:
                    print(f"Unexpected batch format: {type(batch)}")
                    continue
                    
                data, target = data.to(device), target.to(device)
                
                # Ensure data is float32
                if data.dtype != torch.float32:
                    data = data.float()
                    
                output = model(data)
                loss = criterion(output, target)
                total_loss += loss.item()
                
                _, predictions = output.max(1)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(target.cpu().numpy())

        if len(all_targets) > 0:
            accuracy = accuracy_score(all_targets, all_predictions) * 100
            avg_loss = total_loss / len(test_loader) if len(test_loader) > 0 else 0.0
            
            class_report = classification_report(all_targets, all_predictions, output_dict=True)
            cm = confusion_matrix(all_targets, all_predictions)
            
            print(f"Evaluation complete!")
            print(f"Accuracy: {accuracy:.2f}%")
            print(f"Loss: {avg_loss:.4f}")
            print(f"Correct: {np.sum(np.array(all_predictions) == np.array(all_targets))}/{len(all_targets)}")
        else:
            accuracy = 0.0
            avg_loss = 0.0
            class_report = {}
            cm = []
            print("No samples were processed during evaluation")

        metrics = {
            'accuracy': accuracy,
            'loss': avg_loss,
            'total_samples': len(all_targets),
            'correct_predictions': np.sum(np.array(all_predictions) == np.array(all_targets)),
            'classification_report': class_report,
            'confusion_matrix': cm.tolist() if hasattr(cm, 'tolist') else cm
        }

        os.makedirs(os.path.dirname(args.metrics), exist_ok=True)
        os.makedirs(os.path.dirname(args.metrics_json), exist_ok=True)

        with open(args.metrics, 'w') as f:
            json.dump(metrics, f, indent=2)
            
        with open(args.metrics_json, 'w') as f:
            json.dump(metrics, f, indent=2)

        print(f"Metrics saved to: {args.metrics}")
        print(f"Metrics JSON saved to: {args.metrics_json}")
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
