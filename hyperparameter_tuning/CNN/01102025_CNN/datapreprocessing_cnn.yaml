name: CNN preprocessing
description: "Preprocesses data with transforms, creates train/val/test splits and data loaders"

inputs:
  - name: dataset_name
    type: String
    description: Name of dataset

  - name: data_path
    type: String
    description: Path to raw data

  - name: batch_size
    type: Integer
    default: 64
    description: Batch size for data loaders

  - name: validation_split
    type: Float
    default: 0.2
    description: Fraction of training data for validation

outputs:
  - name: processed_data_path
    type: string
    description: "Path to processed data"

  - name: updated_data_info
    type: File
    description: "Updated data information with preprocessing details"

implementation:
  container:
    image: sanram00/slm-image:v18
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        from pathlib import Path
        import torch
        from torch.utils.data import DataLoader, random_split
        import torchvision.transforms as transforms
        import torchvision

        print("=== BRICK 2: DATA PREPROCESSING ===")

        parser = argparse.ArgumentParser()
        parser.add_argument("--dataset_name", type=str, required=True)
        parser.add_argument("--data_path", type=str, required=True)
        parser.add_argument("--batch_size", type=int, required=True)
        parser.add_argument("--validation_split", type=float, required=True)
        parser.add_argument("--processed_data_path", type=str, required=True)
        parser.add_argument("--updated_data_info", type=str, required=True)
        
        args = parser.parse_args()

        print(f"DEBUG: Dataset: {args.dataset_name}")
        print(f"DEBUG: Data path: {args.data_path}")
        print(f"DEBUG: Batch size: {args.batch_size}")
        print(f"DEBUG: Validation split: {args.validation_split}")

        def get_transforms(dataset_name, is_training=False):
            if dataset_name in ['cifar10', 'cifar100']:
                mean = [0.4914, 0.4822, 0.4465]
                std = [0.2470, 0.2435, 0.2616]
                resize = (32, 32)
            elif dataset_name in ['mnist', 'fashionmnist']:
                mean = [0.5]
                std = [0.5]
                resize = (28, 28)
            else:
                mean = [0.5, 0.5, 0.5]
                std = [0.5, 0.5, 0.5]
                resize = (32, 32)
            
            transform_list = []
            
            if is_training and dataset_name in ['cifar10', 'cifar100']:
                transform_list.extend([
                    transforms.RandomHorizontalFlip(),
                    transforms.RandomCrop(resize, padding=4),
                ])
            
            transform_list.extend([
                transforms.Resize(resize),
                transforms.ToTensor(),
                transforms.Normalize(mean, std)
            ])
            
            return transforms.Compose(transform_list)

        # Load original data info
        data_info_path = os.path.join(args.data_path, 'data_info.pkl')
        with open(data_info_path, 'rb') as f:
            data_info = pickle.load(f)

        dataset_map = {
            'mnist': torchvision.datasets.MNIST,
            'cifar10': torchvision.datasets.CIFAR10,
            'cifar100': torchvision.datasets.CIFAR100,
            'fashionmnist': torchvision.datasets.FashionMNIST
        }
        
        if args.dataset_name not in dataset_map:
            raise ValueError(f"Unsupported dataset: {args.dataset_name}")

        # Load datasets with transforms
        train_dataset = dataset_map[args.dataset_name](
            root=args.data_path,
            train=True,
            download=False,
            transform=get_transforms(args.dataset_name, is_training=True)
        )
        
        test_dataset = dataset_map[args.dataset_name](
            root=args.data_path,
            train=False,
            download=False,
            transform=get_transforms(args.dataset_name, is_training=False)
        )
        
        # Split train into train and validation
        val_size = int(len(train_dataset) * args.validation_split)
        train_size = len(train_dataset) - val_size
        
        train_subset, val_subset = random_split(train_dataset, [train_size, val_size])
        
        # Create data loaders
        train_loader = DataLoader(train_subset, batch_size=args.batch_size, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=args.batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
        
        # Save processed data
        processed_path = os.path.join(args.data_path, 'preprocessed')
        Path(processed_path).mkdir(parents=True, exist_ok=True)
        
        torch.save(train_loader, os.path.join(processed_path, 'train_loader.pth'))
        torch.save(val_loader, os.path.join(processed_path, 'val_loader.pth'))
        torch.save(test_loader, os.path.join(processed_path, 'test_loader.pth'))
        
        # Update data info
        data_info['batch_size'] = args.batch_size
        data_info['input_shape'] = next(iter(train_loader))[0].shape[1:]
        data_info['preprocessed_path'] = processed_path
        
        with open(os.path.join(processed_path, 'data_info.pkl'), 'wb') as f:
            pickle.dump(data_info, f)
        
        # Save outputs
        with open(args.processed_data_path, 'w') as f:
            f.write(processed_path)
        
        with open(args.updated_data_info, 'wb') as f:
            pickle.dump(data_info, f)
        
        print(f"âœ“ Preprocessing completed!")
        print(f"  Train batches: {len(train_loader)}")
        print(f"  Val batches: {len(val_loader)}")
        print(f"  Test batches: {len(test_loader)}")
        print(f"  Input shape: {data_info['input_shape']}")
        print(f"  Processed data path: {processed_path}")

    args:
      - --dataset_name
      - {inputValue: dataset_name}
      - --data_path
      - {inputValue: data_path}
      - --batch_size
      - {inputValue: batch_size}
      - --validation_split
      - {inputValue: validation_split}
      - --processed_data_path
      - {outputPath: processed_data_path}
      - --updated_data_info
      - {outputPath: updated_data_info}
