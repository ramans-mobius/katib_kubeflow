name: Load JSON dataset for CNN
description: Fetches JSON from API and prepares train/test/validation datasets with image data for CNN training.
inputs:
  - name: api_url
    type: String
    description: 'API URL to fetch JSON dataset'
    
  - name: access_token
    type: string
    description: 'Bearer access token for API auth'
    
outputs:
  - name: train_data
    type: Dataset

  - name: test_data
    type: Dataset
    
  - name: val_data
    type: Dataset
    
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests pandas scikit-learn numpy pillow || \
        python3 -m pip install --quiet requests pandas scikit-learn numpy pillow --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        import pandas as pd
        import numpy as np
        import requests
        from sklearn.model_selection import train_test_split
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import logging
        import base64
        from PIL import Image
        import io

        parser = argparse.ArgumentParser()
        parser.add_argument('--api_url', type=str, required=True, help='API URL to fetch JSON dataset')
        parser.add_argument('--access_token', type=str, required=True, help='Bearer token for API')
        parser.add_argument('--train_data', type=str, required=True, help='Path to output train dataset')
        parser.add_argument('--test_data', type=str, required=True, help='Path to output test dataset')
        parser.add_argument('--val_data', type=str, required=True, help='Path to output validation dataset')
        args = parser.parse_args()

        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()
        
        # Setup retry logger
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger("api_retry")

        # Setup session with retry logic
        session = requests.Session()
        retries = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[500, 502, 503, 504],
            allowed_methods=["POST", "GET"]
        )
        adapter = HTTPAdapter(max_retries=retries)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        def process_image_data(data_item):
            """Process different image data formats from API"""
            # Handle base64 encoded images
            if 'image' in data_item and isinstance(data_item['image'], str):
                try:
                    # Remove data URL prefix if present
                    image_data = data_item['image']
                    if image_data.startswith('data:image'):
                        image_data = image_data.split(',')[1]
                    
                    # Decode base64
                    img_bytes = base64.b64decode(image_data)
                    img = Image.open(io.BytesIO(img_bytes))
                    
                    # Convert to RGB if needed
                    if img.mode != 'RGB':
                        img = img.convert('RGB')
                    
                    # Convert to numpy array (HWC format)
                    img_array = np.array(img, dtype=np.float32)
                    
                    return img_array
                except Exception as e:
                    logger.warning(f"Failed to decode image: {e}")
                    return None
            
            # Handle raw pixel data
            elif 'pixels' in data_item:
                pixels = data_item['pixels']
                if isinstance(pixels, list):
                    pixels = np.array(pixels, dtype=np.float32)
                
                # Try to infer image dimensions
                if 'width' in data_item and 'height' in data_item:
                    width, height = data_item['width'], data_item['height']
                    channels = len(pixels) // (width * height)
                    if channels == 1:
                        img_array = pixels.reshape(height, width, 1)
                    else:
                        img_array = pixels.reshape(height, width, channels)
                else:
                    # Assume square image
                    size = int(np.sqrt(len(pixels) / 3)) if len(pixels) % 3 == 0 else int(np.sqrt(len(pixels)))
                    channels = 3 if len(pixels) % 3 == 0 else 1
                    img_array = pixels.reshape(size, size, channels)
                
                return img_array
            
            # Handle direct array data
            elif 'data' in data_item:
                img_data = data_item['data']
                if isinstance(img_data, list):
                    img_array = np.array(img_data, dtype=np.float32)
                    
                    # Reshape if 1D
                    if img_array.ndim == 1:
                        if len(img_array) == 784:  # MNIST-like
                            img_array = img_array.reshape(28, 28, 1)
                        elif len(img_array) == 3072:  # CIFAR-10-like
                            img_array = img_array.reshape(32, 32, 3)
                        else:
                            # Try square
                            size = int(np.sqrt(len(img_array)))
                            if size * size == len(img_array):
                                img_array = img_array.reshape(size, size, 1)
                            else:
                                return None
                    
                    return img_array
            
            return None

        # Fetch dataset from API with retry + timeout
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {access_token}"
        }
        
        # Try different payload structures for image APIs
        payloads_to_try = [
            {
                "format": "image",
                "include_labels": True,
                "data_type": "training"
            },
            {
                "request_type": "dataset",
                "image_data": True
            },
            {}  # Empty payload as fallback
        ]

        raw_data = None
        for i, payload in enumerate(payloads_to_try):
            try:
                logger.info(f"Trying payload {i+1}: {payload}")
                resp = session.post(args.api_url, headers=headers, json=payload, timeout=60)
                resp.raise_for_status()
                raw_data = resp.json()
                logger.info(f"Successfully fetched data with payload {i+1}")
                break
            except requests.exceptions.RequestException as e:
                logger.warning(f"Payload {i+1} failed: {e}")
                if i == len(payloads_to_try) - 1:
                    logger.error("All payloads failed")
                    raise
        
        # Process the JSON response
        if isinstance(raw_data, dict):
            if 'images' in raw_data and 'labels' in raw_data:
                # Standard format: {images: [...], labels: [...]}
                images_raw = raw_data['images']
                labels_raw = raw_data['labels']
            elif 'data' in raw_data:
                # Data array format
                data_array = raw_data['data']
                images_raw = []
                labels_raw = []
                for item in data_array:
                    if isinstance(item, dict):
                        img = process_image_data(item)
                        if img is not None:
                            images_raw.append(img)
                            labels_raw.append(item.get('label', 0))
            else:
                # Assume it's a list of image objects
                images_raw = []
                labels_raw = []
                for key, item in raw_data.items():
                    if isinstance(item, dict):
                        img = process_image_data(item)
                        if img is not None:
                            images_raw.append(img)
                            labels_raw.append(item.get('label', 0))
        else:
            # Assume it's a list of image objects
            images_raw = []
            labels_raw = []
            for item in raw_data:
                if isinstance(item, dict):
                    img = process_image_data(item)
                    if img is not None:
                        images_raw.append(img)
                        labels_raw.append(item.get('label', 0))
        
        if not images_raw:
            raise ValueError("No valid image data found in API response")
        
        logger.info(f"Processed {len(images_raw)} images from API")
        
        # Convert to numpy arrays
        images = np.array(images_raw, dtype=np.float32)
        labels = np.array(labels_raw, dtype=np.int64)
        
        logger.info(f"Image array shape: {images.shape}")
        logger.info(f"Labels shape: {labels.shape}")
        logger.info(f"Unique labels: {np.unique(labels)}")
        
        # Normalize images to [0, 1] if needed
        if images.max() > 1:
            images = images / 255.0
        
        # Train-test-validation split
        # First split: 80% train+val, 20% test
        train_val_images, test_images, train_val_labels, test_labels = train_test_split(
            images, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        # Second split: 75% train, 25% val (of the 80%)
        train_images, val_images, train_labels, val_labels = train_test_split(
            train_val_images, train_val_labels, test_size=0.25, random_state=42, stratify=train_val_labels
        )
        
        logger.info(f"Final splits - Train: {len(train_images)}, Val: {len(val_images)}, Test: {len(test_images)}")
        
        # Save datasets as (images, labels) tuples
        os.makedirs(os.path.dirname(args.train_data) or ".", exist_ok=True)
        with open(args.train_data, "wb") as f:
            pickle.dump((train_images, train_labels), f)
        
        os.makedirs(os.path.dirname(args.test_data) or ".", exist_ok=True)
        with open(args.test_data, "wb") as f:
            pickle.dump((test_images, test_labels), f)
        
        os.makedirs(os.path.dirname(args.val_data) or ".", exist_ok=True)
        with open(args.val_data, "wb") as f:
            pickle.dump((val_images, val_labels), f)
        
        logger.info("Successfully saved all datasets")
            
    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --val_data
      - {outputPath: val_data}
