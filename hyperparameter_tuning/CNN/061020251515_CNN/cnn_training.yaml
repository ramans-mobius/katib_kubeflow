name: dynamic_cnn_trainer_evaluator
description: Dynamic CNN training and evaluation with universal data loading

inputs:
  - name: processed_data_path
    type: string
    description: Path to preprocessed data with loaders
  - name: model_config
    type: dict
    description: Model configuration including architecture and training parameters
  - name: output_path
    type: string
    description: Path for saving model outputs and checkpoints
    default: /tmp/output

outputs:
  - name: training_results
    type: dict
    description: Training results including accuracy and metrics
  - name: model_checkpoint_path
    type: string
    description: Path to saved model checkpoint
  - name: evaluation_metrics
    type: dict
    description: Detailed evaluation metrics

implementation:
  language: "python"
  source: |
    import os
    import json
    import pickle
    import time
    from pathlib import Path
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from sklearn.metrics import accuracy_score, f1_score

    class DynamicCNN(nn.Module):
        def __init__(self, config, input_shape, num_classes):
            super(DynamicCNN, self).__init__()
            
            self.config = config
            self.input_shape = input_shape
            self.num_classes = num_classes
            
            # Build conv layers
            conv_layers = []
            in_channels = input_shape[0]
            
            for layer_cfg in config['layers']:
                if layer_cfg['type'] == 'conv':
                    conv_layers.append(nn.Conv2d(
                        in_channels,
                        layer_cfg['out_channels'],
                        kernel_size=layer_cfg['kernel_size'],
                        stride=layer_cfg.get('stride', 1),
                        padding=layer_cfg.get('padding', 0)
                    ))
                    
                    if layer_cfg.get('batch_norm', False):
                        conv_layers.append(nn.BatchNorm2d(layer_cfg['out_channels']))
                    
                    conv_layers.append(nn.ReLU())
                    
                    if layer_cfg.get('dropout', 0) > 0:
                        conv_layers.append(nn.Dropout2d(layer_cfg['dropout']))
                    
                    in_channels = layer_cfg['out_channels']
                    
                elif layer_cfg['type'] == 'maxpool':
                    conv_layers.append(nn.MaxPool2d(
                        kernel_size=layer_cfg['kernel_size'],
                        stride=layer_cfg.get('stride', layer_cfg['kernel_size'])
                    ))
            
            self.conv = nn.Sequential(*conv_layers)
            self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))
            self.flatten = nn.Flatten()
            
            # Calculate flattened size
            with torch.no_grad():
                x = torch.zeros(1, *input_shape)
                x = self.conv(x)
                x = self.adaptive_pool(x)
                flattened_size = x.view(1, -1).size(1)
            
            # Build FC layers
            fc_layers = []
            in_features = flattened_size
            
            for fc_units in config.get('fc_layers', [128]):
                fc_layers.append(nn.Linear(in_features, fc_units))
                fc_layers.append(nn.ReLU())
                fc_layers.append(nn.Dropout(config.get('fc_dropout', 0.5)))
                in_features = fc_units
            
            fc_layers.append(nn.Linear(in_features, num_classes))
            self.fc = nn.Sequential(*fc_layers)
        
        def forward(self, x):
            x = self.conv(x)
            x = self.adaptive_pool(x)
            x = self.flatten(x)
            x = self.fc(x)
            return x

    def load_data_info(data_path):
        """Universal data info loader - supports JSON and PKL"""
        # Try JSON first, then PKL
        json_path = os.path.join(data_path, 'data_info.json')
        pkl_path = os.path.join(data_path, 'data_info.pkl')
        
        if os.path.exists(json_path):
            print("Loading data info from JSON...")
            with open(json_path, 'r') as f:
                return json.load(f)
        elif os.path.exists(pkl_path):
            print("Loading data info from PKL...")
            with open(pkl_path, 'rb') as f:
                return pickle.load(f)
        else:
            raise FileNotFoundError(f"No data info file found in {data_path}")

    def load_data_loaders(data_path):
        """Universal data loader - handles different structures"""
        loader_files = {}
        
        # Look for loader files in the directory
        for file in os.listdir(data_path):
            if file.endswith('.pth'):
                if 'train' in file:
                    loader_files['train'] = os.path.join(data_path, file)
                elif 'val' in file or 'validation' in file:
                    loader_files['val'] = os.path.join(data_path, file)
                elif 'test' in file:
                    loader_files['test'] = os.path.join(data_path, file)
        
        # Fallback to standard names if not found
        standard_names = {
            'train': os.path.join(data_path, 'train_loader.pth'),
            'val': os.path.join(data_path, 'val_loader.pth'), 
            'test': os.path.join(data_path, 'test_loader.pth')
        }
        
        for key, path in standard_names.items():
            if key not in loader_files and os.path.exists(path):
                loader_files[key] = path
        
        # Load the loaders
        loaders = {}
        for key, path in loader_files.items():
            print(f"Loading {key} loader from: {path}")
            loaders[key] = torch.load(path)
        
        # Validate we have required loaders
        if 'train' not in loaders:
            raise FileNotFoundError("No train loader found")
        if 'test' not in loaders:
            raise FileNotFoundError("No test loader found")
        
        # If no validation loader, use test loader as fallback
        if 'val' not in loaders:
            print("Warning: No validation loader found, using test loader for validation")
            loaders['val'] = loaders['test']
        
        return loaders['train'], loaders['val'], loaders['test']

    def train_model(model, train_loader, val_loader, config, device):
        """Train the model"""
        criterion = nn.CrossEntropyLoss()
        
        # Optimizer
        optim_config = config['optimizer']
        if optim_config['type'] == 'adam':
            optimizer = optim.Adam(model.parameters(), lr=optim_config['lr'])
        elif optim_config['type'] == 'sgd':
            optimizer = optim.SGD(model.parameters(), lr=optim_config['lr'], 
                                momentum=optim_config.get('momentum', 0.9))
        
        # Training loop
        best_val_acc = 0
        history = {'train_loss': [], 'val_acc': []}
        epochs = config.get('epochs', 10)
        
        print(f"Starting training for {epochs} epochs...")
        
        for epoch in range(epochs):
            # Training
            model.train()
            train_loss = 0
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            # Validation
            model.eval()
            val_preds = []
            val_targets = []
            with torch.no_grad():
                for data, target in val_loader:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    pred = output.argmax(dim=1)
                    val_preds.extend(pred.cpu().numpy())
                    val_targets.extend(target.cpu().numpy())
            
            val_acc = accuracy_score(val_targets, val_preds)
            history['train_loss'].append(train_loss / len(train_loader))
            history['val_acc'].append(val_acc)
            
            print(f'Epoch {epoch+1}/{epochs} - '
                  f'Train Loss: {train_loss/len(train_loader):.4f}, '
                  f'Val Acc: {val_acc:.4f}')
            
            # Save best model
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save(model.state_dict(), 'best_model.pth')
                print(f"  âœ“ New best model saved (val_acc: {val_acc:.4f})")
        
        return history, best_val_acc

    def evaluate_model(model, test_loader, device):
        """Evaluate the model"""
        model.eval()
        test_preds = []
        test_targets = []
        test_probs = []
        
        print("Running evaluation...")
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                prob = torch.softmax(output, dim=1)
                pred = output.argmax(dim=1)
                
                test_preds.extend(pred.cpu().numpy())
                test_targets.extend(target.cpu().numpy())
                test_probs.extend(prob.cpu().numpy())
        
        accuracy = accuracy_score(test_targets, test_preds)
        f1 = f1_score(test_targets, test_preds, average='weighted')
        
        return {
            'accuracy': accuracy,
            'f1_score': f1,
            'predictions': test_preds,
            'targets': test_targets,
            'probabilities': test_probs
        }

    def save_metrics(metrics: dict, metric_name: str = "accuracy"):
        """Save metrics in Katib-compatible format"""
        global_step = 1
        trial_id = "0"
        timestamp = time.strftime("%Y%m%d%H%M%S")
        
        print("=== SAVING METRICS ===")
        
        # Determine which metric to save
        if metric_name in metrics:
            metric_value = f"{metrics[metric_name]:.4f}"
        elif "accuracy" in metrics:
            metric_value = f"{metrics['accuracy']:.4f}"
        elif "test_accuracy" in metrics:
            metric_value = f"{metrics['test_accuracy']:.4f}"
        else:
            # Pick the first remaining metric
            remaining_metrics = {k: v for k, v in metrics.items() if k not in ["checkpoint_path"]}
            if remaining_metrics:
                metric_name, metric_value = next(iter(remaining_metrics.items()))
                metric_value = f"{metric_value:.4f}"
            else:
                print("WARNING: No valid metrics found, using default accuracy=0.0")
                metric_name = "accuracy"
                metric_value = "0.0"

        record = {
            metric_name: metric_value, 
            "checkpoint_path": "",
            "global_step": str(global_step),
            "timestamp": timestamp,
            "trial": trial_id,
        } 
        
        print(f"Record being saved: {record}")
        
        # Ensure output directory exists
        Path("/katib").mkdir(parents=True, exist_ok=True)
        
        with open("/katib/mnist.json", "a", encoding="utf-8") as f:
            json.dump(record, f)
            f.write("\n")
            
        print("=== METRICS SAVING COMPLETE ===")

    def execute(inputs):
        processed_data_path = inputs["processed_data_path"]
        model_config = inputs["model_config"]
        output_path = inputs["output_path"]
        
        print("=== DYNAMIC CNN TRAINING ===")
        print(f"Data path: {processed_data_path}")
        print(f"Output path: {output_path}")
        print(f"Model config layers: {len(model_config.get('layers', []))}")
        
        # Create output directory
        Path(output_path).mkdir(parents=True, exist_ok=True)
        
        try:
            # UNIVERSAL: Load data info (JSON or PKL)
            data_info = load_data_info(processed_data_path)
            print(f"Loaded data info: {data_info['dataset_name']}")
            print(f"  Input shape: {data_info['input_shape']}")
            print(f"  Num classes: {data_info['num_classes']}")
            
            # UNIVERSAL: Load data loaders (flexible file names)
            train_loader, val_loader, test_loader = load_data_loaders(processed_data_path)
            print(f"Loaded data loaders: train={len(train_loader)}, val={len(val_loader)}, test={len(test_loader)}")
            
            # Setup device
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            print(f"Using device: {device}")
            
            # Create dynamic model
            model = DynamicCNN(
                config=model_config,
                input_shape=data_info['input_shape'],
                num_classes=data_info['num_classes']
            ).to(device)
            
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"Model created with {total_params:,} total parameters ({trainable_params:,} trainable)")
            
            # Train model
            start_time = time.time()
            history, best_val_acc = train_model(model, train_loader, val_loader, model_config, device)
            training_time = time.time() - start_time
            
            # Load best model and evaluate
            print("Loading best model for evaluation...")
            model.load_state_dict(torch.load('best_model.pth'))
            results = evaluate_model(model, test_loader, device)
            
            # Save model and results
            model_checkpoint_path = os.path.join(output_path, 'model.pth')
            torch.save(model.state_dict(), model_checkpoint_path)
            
            results_dict = {
                'test_accuracy': results['accuracy'],
                'test_f1_score': results['f1_score'],
                'best_val_accuracy': best_val_acc,
                'training_time': training_time,
                'epochs_completed': model_config.get('epochs', 10),
                'history': history,
                'dataset_info': data_info,
                'model_config': model_config
            }
            
            # Save as JSON
            with open(os.path.join(output_path, 'results.json'), 'w') as f:
                json.dump(results_dict, f, indent=2)
            
            # Save as PKL
            with open(os.path.join(output_path, 'results.pkl'), 'wb') as f:
                pickle.dump(results_dict, f)
            
            # Save predictions
            with open(os.path.join(output_path, 'predictions.pkl'), 'wb') as f:
                pickle.dump({
                    'predictions': results['predictions'],
                    'targets': results['targets'],
                    'probabilities': results['probabilities']
                }, f)
            
            # Save metrics for Katib
            save_metrics({"accuracy": results['accuracy']})
            
            print(f"=== TRAINING COMPLETED ===")
            print(f"   Test Accuracy: {results['accuracy']:.4f}")
            print(f"   Test F1 Score: {results['f1_score']:.4f}")
            print(f"   Best Val Accuracy: {best_val_acc:.4f}")
            print(f"   Training Time: {training_time:.2f}s")
            print(f"   Model saved to: {model_checkpoint_path}")
            
            evaluation_metrics = {
                "accuracy": results['accuracy'],
                "f1_score": results['f1_score'],
                "precision": results.get('test_precision', 0.0),
                "recall": results.get('test_recall', 0.0),
                "confusion_matrix": results.get('confusion_matrix', [])
            }
            
            return {
                "training_results": results_dict,
                "model_checkpoint_path": model_checkpoint_path,
                "evaluation_metrics": evaluation_metrics
            }
            
        except Exception as e:
            # Save failure metrics
            save_metrics({"accuracy": 0.0})
            raise Exception(f"Training failed: {e}")
