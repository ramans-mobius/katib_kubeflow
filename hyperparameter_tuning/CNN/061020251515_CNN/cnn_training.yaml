name: dynamic_cnn_trainer_evaluator
version: 2.0
description: Dynamic CNN training and evaluation with universal data loading

inputs:
  - name: processed_data_path
    type: string
    description: Path to preprocessed data with loaders
  - name: model_config
    type: dict
    description: Model configuration including architecture and training parameters
  - name: output_path
    type: string
    description: Path for saving model outputs and checkpoints
    default: /tmp/output

outputs:
  - name: training_results
    type: dict
    description: Training results including accuracy and metrics
  - name: model_checkpoint_path
    type: string
    description: Path to saved model checkpoint
  - name: evaluation_metrics
    type: dict
    description: Detailed evaluation metrics

implementation:
  language: "python"
  source: |
    import os
    import pickle
    from pathlib import Path
    import torch
    from torch.utils.data import DataLoader, random_split
    import torchvision.transforms as transforms
    import torchvision
    from PIL import Image
    import numpy as np

    def get_universal_transforms(input_size=(224, 224), is_training=False):
        """Universal transforms that work for any image dataset"""
        
        # Generic normalization for most natural images
        mean = [0.485, 0.456, 0.406]  # ImageNet statistics (good default)
        std = [0.229, 0.224, 0.225]
        
        transform_list = []
        
        if is_training:
            # Universal training augmentations
            transform_list.extend([
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomRotation(degrees=15),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0))
            ])
        else:
            # Validation/Test transforms
            transform_list.extend([
                transforms.Resize(input_size),
                transforms.CenterCrop(input_size)
            ])
        
        # Common transforms for all
        transform_list.extend([
            transforms.ToTensor(),
            transforms.Normalize(mean, std)
        ])
        
        return transforms.Compose(transform_list)

    def detect_dataset_structure(data_path):
        """Auto-detect the structure of any dataset"""
        print(f"Scanning dataset structure in: {data_path}")
        
        # Check for common dataset structures
        structures = {
            'imagefolder_split': os.path.exists(os.path.join(data_path, 'train')) and 
                                os.path.exists(os.path.join(data_path, 'test')),
            'imagefolder_train_only': os.path.exists(os.path.join(data_path, 'train')) and 
                                     not os.path.exists(os.path.join(data_path, 'test')),
            'imagefolder_flat': any(os.path.isdir(os.path.join(data_path, d)) for d in os.listdir(data_path)),
            'single_directory': any(f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')) 
                                  for f in os.listdir(data_path))
        }
        
        # Determine the structure
        if structures['imagefolder_split']:
            return 'imagefolder_split'
        elif structures['imagefolder_train_only']:
            return 'imagefolder_train_only'
        elif structures['imagefolder_flat']:
            return 'imagefolder_flat'
        elif structures['single_directory']:
            return 'single_directory'
        else:
            return 'unknown'

    def get_num_classes_from_dataset(dataset):
        """Extract number of classes from any dataset type"""
        print("Detecting number of classes from dataset...")
        
        try:
            # Method 1: Direct targets access
            if hasattr(dataset, 'targets'):
                targets = dataset.targets
                if isinstance(targets, (list, np.ndarray)):
                    unique_labels = np.unique(targets)
                    num_classes = len(unique_labels)
                    print(f"  Found {num_classes} classes from targets: {unique_labels}")
                    return num_classes
            
            # Method 2: Classes attribute
            if hasattr(dataset, 'classes'):
                num_classes = len(dataset.classes)
                print(f"  Found {num_classes} classes from classes attribute")
                return num_classes
            
            # Method 3: For ImageFolder, check class_to_idx
            if hasattr(dataset, 'class_to_idx'):
                num_classes = len(dataset.class_to_idx)
                print(f"  Found {num_classes} classes from class_to_idx")
                return num_classes
            
            # Method 4: Sample dataset to find unique labels
            print("  Sampling dataset to detect classes...")
            all_labels = set()
            sample_size = min(1000, len(dataset))
            
            for i in range(sample_size):
                try:
                    _, label = dataset[i]
                    all_labels.add(label)
                except Exception as e:
                    continue
            
            if all_labels:
                num_classes = len(all_labels)
                print(f"  Found {num_classes} classes by sampling: {sorted(all_labels)}")
                return num_classes
            
            # Method 5: Default for single-class or unknown
            print("  Warning: Could not detect classes, defaulting to 2")
            return 2
            
        except Exception as e:
            print(f"  Warning: Error detecting classes: {e}, defaulting to 2")
            return 2

    def load_torchvision_dataset(dataset_name, data_path, is_training=True, input_size=(224, 224)):
        """Load torchvision datasets that use binary formats (CIFAR10, CIFAR100, MNIST, etc.)"""
        
        # Map dataset names to torchvision dataset classes
        dataset_map = {
            'cifar10': torchvision.datasets.CIFAR10,
            'cifar100': torchvision.datasets.CIFAR100,
            'mnist': torchvision.datasets.MNIST,
            'fashionmnist': torchvision.datasets.FashionMNIST,
            'kmnist': torchvision.datasets.KMNIST,
        }
        
        dataset_name_lower = dataset_name.lower()
        
        if dataset_name_lower in dataset_map:
            dataset_class = dataset_map[dataset_name_lower]
            print(f"Loading torchvision dataset: {dataset_name}")
            
            # For MNIST-like datasets, we need to convert to RGB
            if dataset_name_lower in ['mnist', 'fashionmnist', 'kmnist']:
                # Add conversion to RGB for grayscale datasets
                transform_list = []
                if is_training:
                    transform_list.extend([
                        transforms.RandomHorizontalFlip(p=0.5),
                        transforms.RandomRotation(degrees=15),
                    ])
                transform_list.extend([
                    transforms.Resize(input_size),
                    transforms.Grayscale(3),  # Convert to RGB
                    transforms.ToTensor(),
                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                ])
                transform = transforms.Compose(transform_list)
            else:
                # For CIFAR datasets, use standard transforms but resize to target size
                transform_list = []
                if is_training:
                    transform_list.extend([
                        transforms.RandomHorizontalFlip(p=0.5),
                        transforms.RandomCrop(32, padding=4),
                    ])
                transform_list.extend([
                    transforms.Resize(input_size),
                    transforms.ToTensor(),
                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                ])
                transform = transforms.Compose(transform_list)
            
            try:
                dataset = dataset_class(
                    root=data_path, 
                    train=is_training, 
                    download=False,  # Already downloaded by load_data
                    transform=transform
                )
                return dataset
            except Exception as e:
                print(f"Error loading {dataset_name}: {e}")
                raise
        
        return None

    def load_any_dataset(data_path, dataset_name=None, is_training=True, input_size=(224, 224)):
        """Load any dataset regardless of structure - enhanced for torchvision datasets"""
        
        # First, try to load as torchvision dataset if dataset_name is provided
        if dataset_name:
            torchvision_dataset = load_torchvision_dataset(dataset_name, data_path, is_training, input_size)
            if torchvision_dataset is not None:
                return torchvision_dataset
        
        structure = detect_dataset_structure(data_path)
        print(f"Detected dataset structure: {structure}")
        
        transform = get_universal_transforms(input_size, is_training=is_training)
        
        if structure == 'imagefolder_split':
            # Standard train/test split
            split_dir = 'train' if is_training else 'test'
            dataset_path = os.path.join(data_path, split_dir)
            return torchvision.datasets.ImageFolder(root=dataset_path, transform=transform)
        
        elif structure == 'imagefolder_train_only':
            # Only train directory exists
            if is_training:
                return torchvision.datasets.ImageFolder(
                    root=os.path.join(data_path, 'train'), 
                    transform=transform
                )
            else:
                # No test set, we'll split train later
                return torchvision.datasets.ImageFolder(
                    root=os.path.join(data_path, 'train'), 
                    transform=get_universal_transforms(input_size, is_training=False)
                )
        
        elif structure == 'imagefolder_flat':
            # Flat class directories
            return torchvision.datasets.ImageFolder(root=data_path, transform=transform)
        
        elif structure == 'single_directory':
            # All images in one directory (single class or unlabeled)
            class SingleClassDataset(torch.utils.data.Dataset):
                def __init__(self, data_path, transform=None):
                    self.data_path = data_path
                    self.transform = transform
                    self.image_files = [f for f in os.listdir(data_path) 
                                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]
                    print(f"  Found {len(self.image_files)} images in single directory")
                
                def __len__(self):
                    return len(self.image_files)
                
                def __getitem__(self, idx):
                    img_path = os.path.join(self.data_path, self.image_files[idx])
                    image = Image.open(img_path).convert('RGB')
                    if self.transform:
                        image = self.transform(image)
                    return image, 0  # Single class
            
            return SingleClassDataset(data_path, transform)
        
        else:
            raise ValueError(f"Unsupported dataset structure: {structure}")

    def execute(inputs):
        data_info = inputs["data_info"]
        dataset_name = inputs["dataset_name"]
        batch_size = inputs["batch_size"]
        validation_split = inputs["validation_split"]
        input_size = tuple(inputs["input_size"])
        
        data_path = data_info['data_path']
        
        print(f"=== UNIVERSAL DATASET PREPROCESSING ===")
        print(f"Dataset: {dataset_name}")
        print(f"Data path: {data_path}")
        print(f"Input size: {input_size}")
        print(f"Batch size: {batch_size}")
        print(f"Validation split: {validation_split}")
        
        try:
            # Load datasets - pass dataset_name for torchvision datasets
            print("Loading training data...")
            train_dataset = load_any_dataset(data_path, dataset_name=dataset_name, is_training=True, input_size=input_size)
            
            print("Loading test data...")
            test_dataset = load_any_dataset(data_path, dataset_name=dataset_name, is_training=False, input_size=input_size)
            
            # Auto-detect number of classes from actual dataset
            detected_num_classes = get_num_classes_from_dataset(train_dataset)
            data_info['num_classes'] = detected_num_classes
            data_info['dataset_structure'] = detect_dataset_structure(data_path)
            
            print(f"Dataset statistics:")
            print(f"  Training samples: {len(train_dataset)}")
            print(f"  Test samples: {len(test_dataset)}")
            print(f"  Detected classes: {detected_num_classes}")
            
            # Handle case where train and test are the same (no separate test set)
            if len(train_dataset) == len(test_dataset) and train_dataset == test_dataset:
                print("No separate test set found, splitting training data...")
                total_size = len(train_dataset)
                test_size = int(0.2 * total_size)  # 20% for test
                train_size = total_size - test_size
                
                train_dataset, test_dataset = random_split(
                    train_dataset, [train_size, test_size]
                )
            
            # Split train into train and validation
            val_size = int(len(train_dataset) * validation_split)
            train_size = len(train_dataset) - val_size
            
            print(f"Final dataset splits:")
            print(f"  Training: {train_size} samples")
            print(f"  Validation: {val_size} samples") 
            print(f"  Test: {len(test_dataset)} samples")
            
            train_subset, val_subset = random_split(train_dataset, [train_size, val_size])
            
            # Create data loaders
            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)
            val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
            
            # Save processed data
            output_path = os.path.join(data_path, 'preprocessed')
            Path(output_path).mkdir(parents=True, exist_ok=True)
            
            torch.save(train_loader, os.path.join(output_path, 'train_loader.pth'))
            torch.save(val_loader, os.path.join(output_path, 'val_loader.pth'))
            torch.save(test_loader, os.path.join(output_path, 'test_loader.pth'))
            
            # Update data info with actual input shape
            sample_batch = next(iter(train_loader))
            data_info['input_shape'] = sample_batch[0].shape[1:]
            data_info['preprocessed_path'] = output_path
            data_info['input_size'] = input_size
            data_info['batch_size'] = batch_size
            data_info['validation_split'] = validation_split
            
            # Save updated data info
            with open(os.path.join(output_path, 'data_info.pkl'), 'wb') as f:
                pickle.dump(data_info, f)
            
            print(f"=== PREPROCESSING COMPLETED ===")
            print(f"   Dataset: {dataset_name}")
            print(f"   Structure: {data_info['dataset_structure']}")
            print(f"   Classes: {detected_num_classes}")
            print(f"   Input shape: {data_info['input_shape']}")
            print(f"   Train batches: {len(train_loader)}")
            print(f"   Val batches: {len(val_loader)}")
            print(f"   Test batches: {len(test_loader)}")
            print(f"   Output path: {output_path}")
            
            preprocessing_info = {
                "dataset_structure": data_info['dataset_structure'],
                "num_classes": detected_num_classes,
                "input_shape": data_info['input_shape'],
                "train_samples": train_size,
                "val_samples": val_size,
                "test_samples": len(test_dataset),
                "batch_size": batch_size
            }
            
            return {
                "processed_data_path": output_path,
                "preprocessing_info": preprocessing_info
            }
            
        except Exception as e:
            raise Exception(f"Preprocessing failed: {e}")
