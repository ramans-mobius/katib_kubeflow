name: 5 CNN Data Preprocessing
description: Universal data preprocessing for CNN models

inputs:
  - name: data_path
    type: String
    description: Path to data
    
  - name: dataset_name
    type: String
    description: Name of the dataset
    
  - name: model_config
    type: String
    description: JSON string with model configuration

outputs:
  - name: processed_data_path
    type: string
    description: Path to processed data

  - name: num_classes
    type: Integer
    description: Number of classes detected

  - name: input_shape
    type: string
    description: Input shape as "channels,height,width"

implementation:
  container:
    image: nikhilv215/nesy-factory:v18
    command:
    - python3
    - -u
    - -c
    - |
      import os
      import sys
      import json
      import pickle
      from pathlib import Path
      import torchvision.transforms as transforms
      
      print("=== CNN DATA PREPROCESSING ===")
      
      data_path = sys.argv[1]
      dataset_name = sys.argv[2]
      model_config_str = sys.argv[3]
      processed_data_file = sys.argv[4]
      num_classes_file = sys.argv[5]
      input_shape_file = sys.argv[6]
      
      print(f"Data path: {data_path}")
      print(f"Dataset name: {dataset_name}")
      
      # Parse model config
      config = json.loads(model_config_str)
      batch_size = config.get('batch_size', 64)
      validation_split = config.get('validation_split', 0.2)
      input_size = (224, 224)
      
      print(f"Batch size from config: {batch_size}")
      print(f"Validation split from config: {validation_split}")
      
      # Add dataset structure detection
      def detect_dataset_structure(data_path):
          print(f"Scanning dataset structure in: {data_path}")
          
          if not os.path.exists(data_path):
              raise ValueError(f"Data path does not exist: {data_path}")
          
          structures = {
              'imagefolder_split': os.path.exists(os.path.join(data_path, 'train')) and 
                                  os.path.exists(os.path.join(data_path, 'test')),
              'imagefolder_train_only': os.path.exists(os.path.join(data_path, 'train')) and 
                                       not os.path.exists(os.path.join(data_path, 'test')),
              'imagefolder_flat': any(os.path.isdir(os.path.join(data_path, d)) for d in os.listdir(data_path) if not d.startswith('.')),
              'single_directory': any(f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')) 
                                    for f in os.listdir(data_path))
          }
          
          if structures['imagefolder_split']:
              return 'imagefolder_split'
          elif structures['imagefolder_train_only']:
              return 'imagefolder_train_only'
          elif structures['imagefolder_flat']:
              return 'imagefolder_flat'
          elif structures['single_directory']:
              return 'single_directory'
          else:
              return 'unknown'
      
      # Add transforms function
      def get_universal_transforms(input_size=(224, 224), is_training=False):
          mean = [0.485, 0.456, 0.406]
          std = [0.229, 0.224, 0.225]
          
          transform_list = []
          
          if is_training:
              transform_list.extend([
                  transforms.RandomHorizontalFlip(p=0.5),
                  transforms.RandomRotation(degrees=15),
                  transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                  transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0))
              ])
          else:
              transform_list.extend([
                  transforms.Resize(input_size),
                  transforms.CenterCrop(input_size)
              ])
          
          transform_list.extend([
              transforms.ToTensor(),
              transforms.Normalize(mean, std)
          ])
          
          return transforms.Compose(transform_list)
      
      # Detect structure
      structure = detect_dataset_structure(data_path)
      print(f"Detected dataset structure: {structure}")
      
      # Test transforms
      train_transform = get_universal_transforms(input_size, is_training=True)
      test_transform = get_universal_transforms(input_size, is_training=False)
      print("Transforms created successfully")
      
      # Create output directory
      output_path = os.path.join(data_path, 'preprocessed')
      Path(output_path).mkdir(parents=True, exist_ok=True)
      
      # For now, use dummy values - we'll add real preprocessing next
      num_classes = 10
      input_shape = "3,224,224"
      
      # Write outputs
      with open(processed_data_file, 'w') as f:
          f.write(output_path)
      
      with open(num_classes_file, 'w') as f:
          f.write(str(num_classes))
      
      with open(input_shape_file, 'w') as f:
          f.write(input_shape)
      
      print(f"Created output path: {output_path}")
      print(f"Number of classes: {num_classes}")
      print(f"Input shape: {input_shape}")
      print("=== PREPROCESSING COMPLETED ===")
    
    args:
    - --data_path
    - {inputValue: data_path}
    - --dataset_name
    - {inputValue: dataset_name}
    - --model_config
    - {inputValue: model_config}
    - --processed_data_path
    - {outputPath: processed_data_path}
    - --num_classes
    - {outputPath: num_classes}
    - --input_shape
    - {outputPath: input_shape}
