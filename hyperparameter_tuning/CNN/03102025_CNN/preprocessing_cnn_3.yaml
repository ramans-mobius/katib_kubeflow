name: 7 CNN Data Preprocessing
description: Universal data preprocessing for CNN models

inputs:
  - name: data_path
    type: String
    description: Path to data
    
  - name: dataset_name
    type: String
    description: Name of the dataset
    
  - name: model_config
    type: String
    description: JSON string with model configuration

outputs:
  - name: processed_data_path
    type: string
    description: Path to processed data

  - name: num_classes
    type: Integer
    description: Number of classes detected

  - name: input_shape
    type: string
    description: Input shape as "channels,height,width"

implementation:
  container:
    image: nikhilv215/nesy-factory:v18
    command:
    - python3
    - -u
    - -c
    - |
      import os
      import sys
      import json
      import pickle
      from pathlib import Path
      import torchvision.transforms as transforms
      from torchvision.datasets import ImageFolder
      
      print("=== CNN DATA PREPROCESSING ===")
      
      data_path = sys.argv[1]
      dataset_name = sys.argv[2]
      model_config_str = sys.argv[3]
      processed_data_file = sys.argv[4]
      num_classes_file = sys.argv[5]
      input_shape_file = sys.argv[6]
      
      # Parse model config
      config = json.loads(model_config_str)
      batch_size = config.get('batch_size', 64)
      validation_split = config.get('validation_split', 0.2)
      input_size = (224, 224)
      
      print(f"Data path: {data_path}")
      print(f"Dataset name: {dataset_name}")
      print(f"Batch size: {batch_size}")
      print(f"Validation split: {validation_split}")
      
      # Core functions
      def get_universal_transforms(input_size=(224, 224), is_training=False):
          mean = [0.485, 0.456, 0.406]
          std = [0.229, 0.224, 0.225]
          transform_list = []
          if is_training:
              transform_list.extend([transforms.RandomHorizontalFlip(p=0.5)])
          else:
              transform_list.extend([transforms.Resize(input_size)])
          transform_list.extend([transforms.ToTensor(), transforms.Normalize(mean, std)])
          return transforms.Compose(transform_list)
      
      def detect_dataset_structure(data_path):
          if not os.path.exists(data_path):
              raise ValueError(f"Data path does not exist: {data_path}")
          if os.path.exists(os.path.join(data_path, 'train')) and os.path.exists(os.path.join(data_path, 'test')):
              return 'imagefolder_split'
          elif os.path.exists(os.path.join(data_path, 'train')):
              return 'imagefolder_train_only'
          elif any(os.path.isdir(os.path.join(data_path, d)) for d in os.listdir(data_path) if not d.startswith('.')):
              return 'imagefolder_flat'
          else:
              return 'unknown'
      
      # Try to load dataset and get real num_classes
      try:
          structure = detect_dataset_structure(data_path)
          print(f"Detected dataset structure: {structure}")
          
          if structure == 'imagefolder_split':
              train_dataset = ImageFolder(root=os.path.join(data_path, 'train'), transform=get_universal_transforms(input_size, is_training=True))
              num_classes = len(train_dataset.classes)
              print(f"Successfully loaded dataset with {num_classes} classes")
          elif structure == 'imagefolder_train_only':
              train_dataset = ImageFolder(root=os.path.join(data_path, 'train'), transform=get_universal_transforms(input_size, is_training=True))
              num_classes = len(train_dataset.classes)
              print(f"Successfully loaded dataset with {num_classes} classes")
          elif structure == 'imagefolder_flat':
              train_dataset = ImageFolder(root=data_path, transform=get_universal_transforms(input_size, is_training=True))
              num_classes = len(train_dataset.classes)
              print(f"Successfully loaded dataset with {num_classes} classes")
          else:
              num_classes = 10
              print("Using default classes for unknown structure")
          
          # Create output directory
          output_path = os.path.join(data_path, 'preprocessed')
          Path(output_path).mkdir(parents=True, exist_ok=True)
          
          # For now, use detected or dummy values
          input_shape = "3,224,224"
          
          # Write outputs
          with open(processed_data_file, 'w') as f:
              f.write(output_path)
          
          with open(num_classes_file, 'w') as f:
              f.write(str(num_classes))
          
          with open(input_shape_file, 'w') as f:
              f.write(input_shape)
          
          print(f"Created output path: {output_path}")
          print(f"Number of classes: {num_classes}")
          print(f"Input shape: {input_shape}")
          print("=== PREPROCESSING COMPLETED ===")
          
      except Exception as e:
          print(f"Error loading dataset: {e}")
          # Fallback to dummy values
          output_path = os.path.join(data_path, 'preprocessed')
          Path(output_path).mkdir(parents=True, exist_ok=True)
          
          with open(processed_data_file, 'w') as f: f.write(output_path)
          with open(num_classes_file, 'w') as f: f.write("10")
          with open(input_shape_file, 'w') as f: f.write("3,224,224")
          print("Used fallback values due to error")
    
    args:
    - --data_path
    - {inputValue: data_path}
    - --dataset_name
    - {inputValue: dataset_name}
    - --model_config
    - {inputValue: model_config}
    - --processed_data_path
    - {outputPath: processed_data_path}
    - --num_classes
    - {outputPath: num_classes}
    - --input_shape
    - {outputPath: input_shape}
