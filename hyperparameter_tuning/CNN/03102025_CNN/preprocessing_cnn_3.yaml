name: 6 CNN Data Preprocessing
description: Universal data preprocessing for CNN models

inputs:
  - name: data_path
    type: String
    description: Path to data
    
  - name: dataset_name
    type: String
    description: Name of the dataset
    
  - name: model_config
    type: String
    description: JSON string with model configuration

outputs:
  - name: processed_data_path
    type: string
    description: Path to processed data

  - name: num_classes
    type: Integer
    description: Number of classes detected

  - name: input_shape
    type: string
    description: Input shape as "channels,height,width"

implementation:
  container:
    image: nikhilv215/nesy-factory:v18
    command:
    - python3
    - -u
    - -c
    - |
      import os
      import sys
      import json
      import pickle
      from pathlib import Path
      import torch
      from torch.utils.data import DataLoader, random_split
      import torchvision
      import torchvision.transforms as transforms
      from torchvision.datasets import ImageFolder
      import numpy as np
      from PIL import Image
      
      print("=== CNN DATA PREPROCESSING ===")
      
      data_path = sys.argv[1]
      dataset_name = sys.argv[2]
      model_config_str = sys.argv[3]
      processed_data_file = sys.argv[4]
      num_classes_file = sys.argv[5]
      input_shape_file = sys.argv[6]
      
      # Parse model config
      config = json.loads(model_config_str)
      batch_size = config.get('batch_size', 64)
      validation_split = config.get('validation_split', 0.2)
      input_size = (224, 224)
      
      print(f"Data path: {data_path}")
      print(f"Dataset name: {dataset_name}")
      print(f"Batch size: {batch_size}")
      print(f"Validation split: {validation_split}")
      
      # Core preprocessing functions
      def get_universal_transforms(input_size=(224, 224), is_training=False):
          mean = [0.485, 0.456, 0.406]
          std = [0.229, 0.224, 0.225]
          transform_list = []
          if is_training:
              transform_list.extend([
                  transforms.RandomHorizontalFlip(p=0.5),
                  transforms.RandomRotation(degrees=15),
                  transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0))
              ])
          else:
              transform_list.extend([
                  transforms.Resize(input_size),
                  transforms.CenterCrop(input_size)
              ])
          transform_list.extend([transforms.ToTensor(), transforms.Normalize(mean, std)])
          return transforms.Compose(transform_list)
      
      def detect_dataset_structure(data_path):
          if not os.path.exists(data_path):
              raise ValueError(f"Data path does not exist: {data_path}")
          if os.path.exists(os.path.join(data_path, 'train')) and os.path.exists(os.path.join(data_path, 'test')):
              return 'imagefolder_split'
          elif os.path.exists(os.path.join(data_path, 'train')):
              return 'imagefolder_train_only'
          elif any(os.path.isdir(os.path.join(data_path, d)) for d in os.listdir(data_path) if not d.startswith('.')):
              return 'imagefolder_flat'
          elif any(f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')) for f in os.listdir(data_path)):
              return 'single_directory'
          else:
              return 'unknown'
      
      def get_num_classes_from_dataset(dataset):
          if hasattr(dataset, 'classes'):
              return len(dataset.classes)
          if hasattr(dataset, 'class_to_idx'):
              return len(dataset.class_to_idx)
          return 2
      
      def load_any_dataset(data_path, dataset_name=None, is_training=True, input_size=(224, 224)):
          structure = detect_dataset_structure(data_path)
          print(f"Detected dataset structure: {structure}")
          transform = get_universal_transforms(input_size, is_training=is_training)
          
          if structure == 'imagefolder_split':
              split_dir = 'train' if is_training else 'test'
              return ImageFolder(root=os.path.join(data_path, split_dir), transform=transform)
          elif structure == 'imagefolder_train_only':
              return ImageFolder(root=os.path.join(data_path, 'train'), transform=transform)
          elif structure == 'imagefolder_flat':
              return ImageFolder(root=data_path, transform=transform)
          else:
              class SingleClassDataset(torch.utils.data.Dataset):
                  def __init__(self, data_path, transform=None):
                      self.data_path = data_path
                      self.transform = transform
                      self.image_files = [f for f in os.listdir(data_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]
                  
                  def __len__(self): return len(self.image_files)
                  def __getitem__(self, idx):
                      img_path = os.path.join(self.data_path, self.image_files[idx])
                      image = Image.open(img_path).convert('RGB')
                      if self.transform: image = self.transform(image)
                      return image, 0
              return SingleClassDataset(data_path, transform)
      
      # Main preprocessing logic
      try:
          print("Loading datasets...")
          train_dataset = load_any_dataset(data_path, dataset_name=dataset_name, is_training=True, input_size=input_size)
          test_dataset = load_any_dataset(data_path, dataset_name=dataset_name, is_training=False, input_size=input_size)
          
          num_classes = get_num_classes_from_dataset(train_dataset)
          print(f"Number of classes detected: {num_classes}")
          
          # Handle case where train and test are the same
          if len(train_dataset) == len(test_dataset):
              print("Splitting training data for test set...")
              total_size = len(train_dataset)
              test_size = int(0.2 * total_size)
              train_size = total_size - test_size
              train_dataset, test_dataset = random_split(train_dataset, [train_size, test_size])
          
          # Split train into train and validation
          val_size = int(len(train_dataset) * validation_split)
          train_size = len(train_dataset) - val_size
          train_subset, val_subset = random_split(train_dataset, [train_size, val_size])
          
          print(f"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {len(test_dataset)}")
          
          # Create data loaders
          train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)
          val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)
          test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
          
          # Get input shape
          sample_batch, _ = next(iter(train_loader))
          input_shape = sample_batch.shape[1:]
          input_shape_str = f"{input_shape[0]},{input_shape[1]},{input_shape[2]}"
          
          # Save processed data
          output_path = os.path.join(data_path, 'preprocessed')
          Path(output_path).mkdir(parents=True, exist_ok=True)
          
          torch.save(train_loader, os.path.join(output_path, 'train_loader.pth'))
          torch.save(val_loader, os.path.join(output_path, 'val_loader.pth'))
          torch.save(test_loader, os.path.join(output_path, 'test_loader.pth'))
          
          # Save data info
          data_info = {
              'dataset_name': dataset_name, 'data_path': data_path, 'num_classes': num_classes,
              'input_shape': input_shape, 'batch_size': batch_size, 'preprocessed_path': output_path
          }
          with open(os.path.join(output_path, 'data_info.pkl'), 'wb') as f:
              pickle.dump(data_info, f)
          
          # Write outputs
          with open(processed_data_file, 'w') as f: f.write(output_path)
          with open(num_classes_file, 'w') as f: f.write(str(num_classes))
          with open(input_shape_file, 'w') as f: f.write(input_shape_str)
          
          print(f"Output path: {output_path}")
          print(f"Number of classes: {num_classes}")
          print(f"Input shape: {input_shape_str}")
          print("=== PREPROCESSING COMPLETED ===")
          
      except Exception as e:
          print(f"Error: {e}")
          import traceback
          traceback.print_exc()
          sys.exit(1)
    
    args:
    - --data_path
    - {inputValue: data_path}
    - --dataset_name
    - {inputValue: dataset_name}
    - --model_config
    - {inputValue: model_config}
    - --processed_data_path
    - {outputPath: processed_data_path}
    - --num_classes
    - {outputPath: num_classes}
    - --input_shape
    - {outputPath: input_shape}
