name: 8 CNN Data Preprocessing
description: Universal data preprocessing for CNN models

inputs:
  - name: data_path
    type: String
    description: Path to data
    
  - name: dataset_name
    type: String
    description: Name of the dataset
    
  - name: model_config
    type: String
    description: JSON string with model configuration

outputs:
  - name: processed_data_path
    type: string
    description: Path to processed data

  - name: num_classes
    type: Integer
    description: Number of classes detected

  - name: input_shape
    type: string
    description: Input shape as "channels,height,width"

implementation:
  container:
    image: nikhilv215/nesy-factory:v18
    command:
    - python3
    - -u
    - -c
    - |
      import os
      import sys
      import json
      import pickle
      from pathlib import Path
      import torch
      from torch.utils.data import DataLoader, random_split
      import torchvision.transforms as transforms
      from torchvision.datasets import ImageFolder
      
      print("=== CNN DATA PREPROCESSING ===")
      
      data_path = sys.argv[1]
      dataset_name = sys.argv[2]
      model_config_str = sys.argv[3]
      processed_data_file = sys.argv[4]
      num_classes_file = sys.argv[5]
      input_shape_file = sys.argv[6]
      
      # Parse model config
      config = json.loads(model_config_str)
      batch_size = config.get('batch_size', 64)
      validation_split = config.get('validation_split', 0.2)
      input_size = (224, 224)
      
      print(f"Data path: {data_path}")
      print(f"Dataset name: {dataset_name}")
      print(f"Batch size: {batch_size}")
      print(f"Validation split: {validation_split}")
      
      def get_universal_transforms(input_size=(224, 224), is_training=False):
          mean = [0.485, 0.456, 0.406]
          std = [0.229, 0.224, 0.225]
          transform_list = []
          if is_training:
              transform_list.extend([transforms.RandomHorizontalFlip(p=0.5)])
          else:
              transform_list.extend([transforms.Resize(input_size)])
          transform_list.extend([transforms.ToTensor(), transforms.Normalize(mean, std)])
          return transforms.Compose(transform_list)
      
      def detect_dataset_structure(data_path):
          if not os.path.exists(data_path):
              raise ValueError(f"Data path does not exist: {data_path}")
          if os.path.exists(os.path.join(data_path, 'train')) and os.path.exists(os.path.join(data_path, 'test')):
              return 'imagefolder_split'
          elif os.path.exists(os.path.join(data_path, 'train')):
              return 'imagefolder_train_only'
          elif any(os.path.isdir(os.path.join(data_path, d)) for d in os.listdir(data_path) if not d.startswith('.')):
              return 'imagefolder_flat'
          else:
              return 'unknown'
      
      # Load datasets and create data loaders
      try:
          structure = detect_dataset_structure(data_path)
          print(f"Detected dataset structure: {structure}")
          
          # Load datasets
          if structure == 'imagefolder_split':
              train_dataset = ImageFolder(root=os.path.join(data_path, 'train'), transform=get_universal_transforms(input_size, is_training=True))
              test_dataset = ImageFolder(root=os.path.join(data_path, 'test'), transform=get_universal_transforms(input_size, is_training=False))
          elif structure == 'imagefolder_train_only':
              train_dataset = ImageFolder(root=os.path.join(data_path, 'train'), transform=get_universal_transforms(input_size, is_training=True))
              test_dataset = ImageFolder(root=os.path.join(data_path, 'train'), transform=get_universal_transforms(input_size, is_training=False))
          elif structure == 'imagefolder_flat':
              train_dataset = ImageFolder(root=data_path, transform=get_universal_transforms(input_size, is_training=True))
              test_dataset = ImageFolder(root=data_path, transform=get_universal_transforms(input_size, is_training=False))
          else:
              raise ValueError(f"Unsupported dataset structure: {structure}")
          
          num_classes = len(train_dataset.classes)
          print(f"Successfully loaded dataset with {num_classes} classes")
          
          # Split train into train and validation
          val_size = int(len(train_dataset) * validation_split)
          train_size = len(train_dataset) - val_size
          train_subset, val_subset = random_split(train_dataset, [train_size, val_size])
          
          print(f"Dataset splits - Train: {train_size}, Val: {val_size}, Test: {len(test_dataset)}")
          
          # Create data loaders
          train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
          val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)
          test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
          
          # Get real input shape from data
          sample_batch, _ = next(iter(train_loader))
          input_shape = sample_batch.shape[1:]
          input_shape_str = f"{input_shape[0]},{input_shape[1]},{input_shape[2]}"
          
          # Create output directory
          output_path = os.path.join(data_path, 'preprocessed')
          Path(output_path).mkdir(parents=True, exist_ok=True)
          
          # Save data loaders
          torch.save(train_loader, os.path.join(output_path, 'train_loader.pth'))
          torch.save(val_loader, os.path.join(output_path, 'val_loader.pth'))
          torch.save(test_loader, os.path.join(output_path, 'test_loader.pth'))
          
          # Save data info
          data_info = {
              'dataset_name': dataset_name,
              'data_path': data_path,
              'num_classes': num_classes,
              'input_shape': input_shape,
              'batch_size': batch_size,
              'preprocessed_path': output_path
          }
          with open(os.path.join(output_path, 'data_info.pkl'), 'wb') as f:
              pickle.dump(data_info, f)
          
          # Write outputs
          with open(processed_data_file, 'w') as f:
              f.write(output_path)
          
          with open(num_classes_file, 'w') as f:
              f.write(str(num_classes))
          
          with open(input_shape_file, 'w') as f:
              f.write(input_shape_str)
          
          print(f"Created output path: {output_path}")
          print(f"Number of classes: {num_classes}")
          print(f"Input shape: {input_shape_str}")
          print("=== PREPROCESSING COMPLETED ===")
          
      except Exception as e:
          print(f"Error during preprocessing: {e}")
          # Fallback to dummy values
          output_path = os.path.join(data_path, 'preprocessed')
          Path(output_path).mkdir(parents=True, exist_ok=True)
          
          with open(processed_data_file, 'w') as f: f.write(output_path)
          with open(num_classes_file, 'w') as f: f.write("10")
          with open(input_shape_file, 'w') as f: f.write("3,224,224")
          print("Used fallback values due to error")
    
    args:
    - --data_path
    - {inputValue: data_path}
    - --dataset_name
    - {inputValue: dataset_name}
    - --model_config
    - {inputValue: model_config}
    - --processed_data_path
    - {outputPath: processed_data_path}
    - --num_classes
    - {outputPath: num_classes}
    - --input_shape
    - {outputPath: input_shape}
