name: CNN Data Preprocessing
description: Universal data preprocessing for CNN models

inputs:
  - name: data_path
    type: String
    description: Path to raw data from data loading brick
    
  - name: dataset_name
    type: String
    description: Name of the dataset
    default: ""
    
  - name: batch_size
    type: Integer
    description: Batch size for data loaders
    default: 64
    
  - name: validation_split
    type: Float
    description: Fraction of data to use for validation
    default: 0.2
    
  - name: input_size
    type: String
    description: Input image size as "width,height"
    default: "224,224"

outputs:
  - name: processed_data_path
    type: String
    description: Path to processed data and data loaders

  - name: num_classes
    type: Integer
    description: Number of classes detected

  - name: input_shape
    type: String
    description: Input shape as "channels,height,width"

implementation:
  container:
    image: nikhilv215/nesy-factory:v18
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import json
        import pickle
        from pathlib import Path
        import torch
        from torch.utils.data import DataLoader, random_split
        import torchvision
        import torchvision.transforms as transforms
        from torchvision.datasets import ImageFolder
        import numpy as np
        from PIL import Image
        import argparse
        
        # Argument parsing
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", type=str, required=True)
        parser.add_argument("--dataset_name", type=str, required=True)
        parser.add_argument("--batch_size", type=int, required=True)
        parser.add_argument("--validation_split", type=float, required=True)
        parser.add_argument("--input_size", type=str, required=True)
        parser.add_argument("--processed_data_path", type=str, required=True)
        parser.add_argument("--num_classes", type=str, required=True)
        parser.add_argument("--input_shape", type=str, required=True)
        args = parser.parse_args()
        
        # Parse input size
        input_size = tuple(map(int, args.input_size.split(',')))
        
        print("=== CNN DATA PREPROCESSING ===")
        print(f"Data path: {args.data_path}")
        print(f"Dataset name: {args.dataset_name}")
        print(f"Input size: {input_size}")
        print(f"Batch size: {args.batch_size}")
        print(f"Validation split: {args.validation_split}")
        
        def get_transforms(input_size, is_training=True):
            transform_list = []
            
            if is_training:
                transform_list.extend([
                    transforms.RandomHorizontalFlip(p=0.5),
                    transforms.RandomRotation(degrees=15),
                    transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0))
                ])
            else:
                transform_list.extend([
                    transforms.Resize(input_size),
                    transforms.CenterCrop(input_size)
                ])
            
            transform_list.extend([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            
            return transforms.Compose(transform_list)
        
        def detect_dataset_structure(data_path):
            if not os.path.exists(data_path):
                raise ValueError(f"Data path does not exist: {data_path}")
                
            if os.path.exists(os.path.join(data_path, 'train')) and os.path.exists(os.path.join(data_path, 'test')):
                return 'train_test_split'
            elif os.path.exists(os.path.join(data_path, 'train')):
                return 'train_only'
            elif any(os.path.isdir(os.path.join(data_path, d)) for d in os.listdir(data_path)):
                return 'imagefolder'
            else:
                return 'unknown'
        
        def load_datasets(data_path, input_size):
            structure = detect_dataset_structure(data_path)
            print(f"Dataset structure: {structure}")
            
            if structure == 'train_test_split':
                train_dataset = ImageFolder(
                    root=os.path.join(data_path, 'train'), 
                    transform=get_transforms(input_size, is_training=True)
                )
                test_dataset = ImageFolder(
                    root=os.path.join(data_path, 'test'), 
                    transform=get_transforms(input_size, is_training=False)
                )
                num_classes = len(train_dataset.classes)
                
            elif structure == 'train_only':
                full_dataset = ImageFolder(
                    root=os.path.join(data_path, 'train'), 
                    transform=get_transforms(input_size, is_training=True)
                )
                num_classes = len(full_dataset.classes)
                
                # Split into train/test
                total_size = len(full_dataset)
                test_size = int(0.2 * total_size)
                train_size = total_size - test_size
                train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])
                
            elif structure == 'imagefolder':
                full_dataset = ImageFolder(
                    root=data_path, 
                    transform=get_transforms(input_size, is_training=True)
                )
                num_classes = len(full_dataset.classes)
                
                # Split into train/test
                total_size = len(full_dataset)
                test_size = int(0.2 * total_size)
                train_size = total_size - test_size
                train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])
                
            else:
                raise ValueError(f"Unsupported dataset structure: {structure}")
            
            return train_dataset, test_dataset, num_classes
        
        try:
          
            train_dataset, test_dataset, num_classes = load_datasets(args.data_path, input_size)
            print(f"Loaded datasets - Train: {len(train_dataset)}, Test: {len(test_dataset)}")
            print(f"Number of classes: {num_classes}")
            
      
            val_size = int(len(train_dataset) * args.validation_split)
            train_size = len(train_dataset) - val_size
            train_subset, val_subset = random_split(train_dataset, [train_size, val_size])
   
            train_loader = DataLoader(train_subset, batch_size=args.batch_size, shuffle=True, num_workers=2)
            val_loader = DataLoader(val_subset, batch_size=args.batch_size, shuffle=False, num_workers=2)
            test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)
            
            sample_batch, _ = next(iter(train_loader))
            input_shape = sample_batch.shape[1:]  # (channels, height, width)
            input_shape_str = f"{input_shape[0]},{input_shape[1]},{input_shape[2]}"

            output_path = os.path.join(args.data_path, 'preprocessed')
            Path(output_path).mkdir(parents=True, exist_ok=True)
            
            torch.save(train_loader, os.path.join(output_path, 'train_loader.pth'))
            torch.save(val_loader, os.path.join(output_path, 'val_loader.pth'))
            torch.save(test_loader, os.path.join(output_path, 'test_loader.pth'))
            
            # Save data info for training brick
            data_info = {
                'dataset_name': args.dataset_name,
                'data_path': args.data_path,
                'num_classes': num_classes,
                'input_shape': input_shape,
                'batch_size': args.batch_size
            }
            
            with open(os.path.join(output_path, 'data_info.json'), 'w') as f:
                json.dump(data_info, f, indent=2)
            
            # Save outputs for pipeline
            with open(args.processed_data_path, 'w') as f:
                f.write(output_path)
            
            with open(args.num_classes, 'w') as f:
                f.write(str(num_classes))
            
            with open(args.input_shape, 'w') as f:
                f.write(input_shape_str)
            
            print("=== PREPROCESSING COMPLETED ===")
            print(f"Output path: {output_path}")
            print(f"Number of classes: {num_classes}")
            print(f"Input shape: {input_shape}")
            
        except Exception as e:
            print(f"Preprocessing failed: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        print("Preprocessing brick completed!")
    args:
      - --data_path
      - {inputValue: data_path}
      - --dataset_name
      - {inputValue: dataset_name}
      - --batch_size
      - {inputValue: batch_size}
      - --validation_split
      - {inputValue: validation_split}
      - --input_size
      - {inputValue: input_size}
      - --processed_data_path
      - {outputPath: processed_data_path}
      - --num_classes
      - {outputPath: num_classes}
      - --input_shape
      - {outputPath: input_shape}
