name: CNN Data Preprocessing
description: Universal data preprocessing for any dataset structure

inputs:
  - name: dataset_name
    type: String
    description: Dataset name
    default: mnist
    
  - name: data_path
    type: String  
    description: Path from data loading brick
    default: /tmp/data
    
  - name: batch_size
    type: Integer
    description: Batch size for data loaders
    default: 64
    
  - name: validation_split
    type: Float
    description: Validation split ratio
    default: 0.2
    
  - name: input_size
    type: String
    description: Input image size as JSON string, e.g., "[224, 224]"
    default: "[224, 224]"

outputs:
  - name: processed_data_info
    type: JsonObject
    description: Preprocessed data information with loader paths

implementation:
  container:
    image: nikhilv215/nesy-factory:v18
    command:
      - python3
      - -u
      - -c
      - |
        import os
        import pickle
        from pathlib import Path
        import torch
        from torch.utils.data import DataLoader, random_split
        import torchvision.transforms as transforms
        import torchvision
        from PIL import Image
        import numpy as np
        import json
        import argparse

        # Argument parsing
        parser = argparse.ArgumentParser()
        parser.add_argument("--dataset_name", type=str, required=True)
        parser.add_argument("--data_path", type=str, required=True)
        parser.add_argument("--batch_size", type=int, required=True)
        parser.add_argument("--validation_split", type=float, required=True)
        parser.add_argument("--input_size", type=str, required=True)
        parser.add_argument("--processed_data_info", type=str, required=True)
        args = parser.parse_args()

        # Parse input_size from JSON string
        input_size = json.loads(args.input_size)

        # --- (rest of your preprocessing code stays the same) ---
        # Use 'input_size' as a tuple where needed
        input_size = tuple(input_size)

        # Main execution
        # Call your preprocess_data function here
        processed_data_info = preprocess_data(
            args.dataset_name, 
            args.data_path, 
            args.batch_size, 
            args.validation_split, 
            input_size
        )

        # Save output
        with open(args.processed_data_info, 'w') as f:
            json.dump(processed_data_info, f, indent=2)

        print("Data preprocessing brick completed successfully!")
    args:
      - --dataset_name
      - {inputValue: dataset_name}
      - --data_path
      - {inputValue: data_path}
      - --batch_size
      - {inputValue: batch_size}
      - --validation_split
      - {inputValue: validation_split}
      - --input_size
      - {inputValue: input_size}
      - --processed_data_info
      - {outputPath: processed_data_info}
