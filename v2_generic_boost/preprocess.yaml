name: 3 Unified Preprocessor
description: Preprocesses data using unified configuration
inputs:
  - name: train_data
    type: Dataset
  - name: test_data
    type: Dataset
  - name: target_data
    type: Dataset
  - name: config_str
    type: String
    description: 'Unified configuration JSON string'
outputs:
  - name: processed_data
    type: Dataset
  - name: preprocessing_pipeline
    type: Model
  - name: feature_info
    type: String

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        python3 -c "
        import pickle, numpy as np, json
        from sklearn.preprocessing import StandardScaler, MinMaxScaler
        from sklearn.feature_selection import SelectKBest, f_classif, f_regression
        
        # Load data and config
        with open('$0', 'rb') as f:
            train_dict = pickle.load(f)
        with open('$1', 'rb') as f:
            test_dict = pickle.load(f)
        with open('$2', 'rb') as f:
            y_test_original = pickle.load(f)
        
        config = json.loads('$3')
        preprocessing_config = config.get('preprocessing', {})
        pipeline_config = config.get('pipeline', {})
        
        X_train, y_train = train_dict['X'], train_dict['y']
        X_test, y_test = test_dict['X'], test_dict['y']
        feature_names = train_dict['feature_names']
        task_type = pipeline_config.get('task', 'classification')
        
        print(f'Preprocessing: {X_train.shape[1]} features, task={task_type}')

        # Extract preprocessing settings
        scaling = preprocessing_config.get('scaling', 'standard')
        feature_selection = preprocessing_config.get('feature_selection', 'auto')
        k_features = preprocessing_config.get('k_features', 'auto')
        
        # Apply preprocessing
        preprocessing_steps = []
        X_train_processed = X_train.copy()
        X_test_processed = X_test.copy()
        current_feature_names = feature_names.copy()

        # Handle missing values
        if np.isnan(X_train_processed).any():
            X_train_processed = np.nan_to_num(X_train_processed)
            X_test_processed = np.nan_to_num(X_test_processed)

        # Feature scaling
        if scaling != 'none':
            if scaling == 'standard':
                scaler = StandardScaler()
            else:
                scaler = MinMaxScaler()
            
            X_train_processed = scaler.fit_transform(X_train_processed)
            X_test_processed = scaler.transform(X_test_processed)
            preprocessing_steps.append(('scaler', scaler))
            print(f'Applied {scaling} scaling')

        # Feature selection
        if feature_selection != 'none':
            k = k_features
            if k == 'auto':
                k = min(50, X_train_processed.shape[1])
            
            if k != 'all' and X_train_processed.shape[1] > k:
                if task_type == 'classification':
                    selector = SelectKBest(score_func=f_classif, k=k)
                else:
                    selector = SelectKBest(score_func=f_regression, k=k)
                
                X_train_processed = selector.fit_transform(X_train_processed, y_train)
                X_test_processed = selector.transform(X_test_processed)
                
                if hasattr(selector, 'get_support'):
                    selected_mask = selector.get_support()
                    current_feature_names = [name for i, name in enumerate(current_feature_names) if selected_mask[i]]
                
                preprocessing_steps.append(('feature_selector', selector))
                print(f'Applied feature selection: {X_train_processed.shape[1]} features')

        # Create DataWrapper object
        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)
        
        processed_data_dict = {
            'X_train': X_train_processed, 
            'y_train': y_train,
            'X_test': X_test_processed, 
            'y_test': y_test,
            'feature_names': current_feature_names,
            'config': config,
            'dataset_info': {
                'dataset_name': 'Processed_Dataset',
                'problem_type': task_type,
                'num_features': len(current_feature_names),
                'preprocessing_steps': [step[0] for step in preprocessing_steps]
            }
        }
        
        data_wrapper = DataWrapper(processed_data_dict)
        
        # Create preprocessing pipeline
        class PreprocessingPipeline:
            def __init__(self, steps):
                self.steps = steps
            
            def transform(self, X):
                X_transformed = X.copy()
                for name, transformer in self.steps:
                    X_transformed = transformer.transform(X_transformed)
                return X_transformed
        
        preprocessing_pipeline = PreprocessingPipeline(preprocessing_steps)
        
        # Feature information
        feature_info = {
            'original_features': len(feature_names),
            'final_features': len(current_feature_names),
            'feature_names': current_feature_names,
            'task_type': task_type,
            'preprocessing_applied': [step[0] for step in preprocessing_steps],
            'train_shape': X_train_processed.shape,
            'test_shape': X_test_processed.shape
        }

        # Save outputs
        with open('$4', 'wb') as f:
            pickle.dump(data_wrapper, f)
        with open('$5', 'wb') as f:
            pickle.dump(preprocessing_pipeline, f)
        with open('$6', 'w') as f:
            json.dump(feature_info, f, indent=2)
        
        print('âœ… Preprocessing complete')
        print(f'Final features: {len(current_feature_names)}')
        "
    args:
      - {inputPath: train_data}
      - {inputPath: test_data}
      - {inputPath: target_data}
      - {inputValue: config_str}
      - {outputPath: processed_data}
      - {outputPath: preprocessing_pipeline}
      - {outputPath: feature_info}
