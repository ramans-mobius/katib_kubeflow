name: Data Loader
description: Fetches JSON data from API and prepares train/test splits with automatic feature detection
inputs:
  - {name: api_url, type: String, description: 'API URL to fetch JSON data'}
  - {name: access_token, type: string, description: 'Bearer access token for API auth'}
  - {name: target_column, type: String, description: 'Name of target column for supervised learning', default: ""}
  - {name: task_type, type: String, description: 'Type of task: classification or regression', default: "auto"}
outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: target_data, type: Dataset}
  - {name: data_info, type: String}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests pandas scikit-learn numpy || \
        python3 -m pip install --quiet requests pandas scikit-learn numpy --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        import pandas as pd
        import numpy as np
        import requests
        from sklearn.model_selection import train_test_split
        import json
        from sklearn.preprocessing import LabelEncoder

        parser = argparse.ArgumentParser()
        parser.add_argument('--api_url', type=str, required=True)
        parser.add_argument('--access_token', type=str, required=True)
        parser.add_argument('--target_column', type=str, required=True)
        parser.add_argument('--task_type', type=str, required=True)
        parser.add_argument('--train_data', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--target_data', type=str, required=True)
        parser.add_argument('--data_info', type=str, required=True)
        args = parser.parse_args()

        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()

        # Fetch data
        headers = {"Authorization": f"Bearer {access_token}"}
        try:
            # Try GET first, then POST if GET fails
            resp = requests.get(args.api_url, headers=headers, timeout=30)
            if resp.status_code == 405:  # Method not allowed
                payload = {"dbType": "TIDB", "filter": {}, "startTime": 0, "endTime": 0}
                resp = requests.post(args.api_url, headers=headers, json=payload, timeout=30)
            resp.raise_for_status()
            raw_data = resp.json()
        except Exception as e:
            print(f"API request failed: {e}")
            raise

        # Handle different response formats
        if isinstance(raw_data, dict) and 'content' in raw_data:
            df_data = raw_data['content']
        else:
            df_data = raw_data

        df = pd.DataFrame(df_data)
        print(f"Loaded {len(df)} records with {len(df.columns)} columns")
        print(f"Columns: {list(df.columns)}")

        # Automatic feature engineering for nested JSON structures
        def flatten_nested_columns(df):
            """Flatten nested JSON structures into flat columns"""
            flattened_df = df.copy()
            
            for col in df.columns:
                if isinstance(df[col].iloc[0], dict):
                    # Flatten dictionary columns
                    nested_df = pd.json_normalize(df[col])
                    nested_df.columns = [f"{col}_{subcol}" for subcol in nested_df.columns]
                    flattened_df = pd.concat([flattened_df.drop(columns=[col]), nested_df], axis=1)
                elif isinstance(df[col].iloc[0], list):
                    # Handle list columns (take length as feature)
                    flattened_df[f"{col}_count"] = df[col].apply(len)
                    flattened_df = flattened_df.drop(columns=[col])
            
            return flattened_df

        # Flatten nested structures
        df_flat = flatten_nested_columns(df)
        
        # Convert all columns to numeric where possible
        def auto_convert_to_numeric(df):
            converted_df = df.copy()
            for col in converted_df.columns:
                # Skip if already numeric
                if pd.api.types.is_numeric_dtype(converted_df[col]):
                    continue
                
                # Try to convert to numeric
                converted_col = pd.to_numeric(converted_df[col], errors='coerce')
                if not converted_col.isna().all():  # If conversion worked for some values
                    converted_df[col] = converted_col
                else:
                    # Encode categorical variables
                    if converted_df[col].nunique() < 50:  # Reasonable number of categories
                        le = LabelEncoder()
                        converted_df[col] = le.fit_transform(converted_df[col].astype(str))
                    else:
                        # Drop high-cardinality categorical columns
                        converted_df = converted_df.drop(columns=[col])
                        print(f"Dropped high-cardinality column: {col}")
            
            return converted_df

        df_numeric = auto_convert_to_numeric(df_flat)
        
        # Handle target column
        target_col = args.target_column if args.target_column else None
        if not target_col:
            # Auto-detect target: look for common target names
            common_targets = ['target', 'label', 'class', 'score', 'value', 'output', 
                             'reachable', 'memory_pressure', 'anomaly', 'failure']
            for col in df_numeric.columns:
                if any(target in col.lower() for target in common_targets):
                    target_col = col
                    break
            
            if not target_col and len(df_numeric.columns) > 0:
                # Use last column as target if none found
                target_col = df_numeric.columns[-1]
                print(f"Auto-selected target column: {target_col}")

        if target_col not in df_numeric.columns:
            raise ValueError(f"Target column '{target_col}' not found in data. Available columns: {list(df_numeric.columns)}")

        # Separate features and target
        X = df_numeric.drop(columns=[target_col])
        y = df_numeric[target_col]
        
        # Auto-detect task type if not specified
        task_type = args.task_type
        if task_type == "auto":
            unique_values = y.nunique()
            if unique_values == 2:
                task_type = "classification"
                print("Auto-detected: binary classification")
            elif unique_values < 20:
                task_type = "classification" 
                print(f"Auto-detected: multi-class classification ({unique_values} classes)")
            else:
                task_type = "regression"
                print("Auto-detected: regression")
        else:
            task_type = task_type.lower()

        # For classification, ensure target is integer
        if task_type == "classification":
            y = y.astype(int)
            print(f"Classification classes: {sorted(y.unique())}")

        # Handle missing values
        X = X.fillna(0)
        y = y.fillna(0) if task_type == "regression" else y.fillna(y.mode()[0] if len(y.mode()) > 0 else 0)

        # Train-test split
        test_size = 0.2
        stratify = y if task_type == "classification" and y.nunique() > 1 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=stratify
        )
        
        # Prepare output data
        train_dict = {
            'X': X_train.values, 
            'y': y_train, 
            'feature_names': X.columns.tolist(),
            'task_type': task_type,
            'target_name': target_col
        }
        test_dict = {
            'X': X_test.values, 
            'y': y_test, 
            'feature_names': X.columns.tolist(),
            'task_type': task_type,
            'target_name': target_col
        }
        
        # Data info
        data_info = {
            'original_samples': len(df),
            'final_samples': len(X),
            'features_count': len(X.columns),
            'task_type': task_type,
            'target_column': target_col,
            'target_statistics': {
                'min': float(y.min()),
                'max': float(y.max()),
                'mean': float(y.mean()),
                'std': float(y.std())
            } if task_type == 'regression' else {
                'classes': int(y.nunique()),
                'class_distribution': y.value_counts().to_dict()
            },
            'feature_names': X.columns.tolist(),
            'train_samples': len(X_train),
            'test_samples': len(X_test)
        }

        # Save datasets
        os.makedirs(os.path.dirname(args.train_data) or ".", exist_ok=True)
        with open(args.train_data, "wb") as f:
            pickle.dump(train_dict, f)
        with open(args.test_data, "wb") as f:
            pickle.dump(test_dict, f)
        with open(args.target_data, "wb") as f:
            pickle.dump(y_test, f)
        with open(args.data_info, "w") as f:
            json.dump(data_info, f, indent=2)
        
        print(f"Data loading complete")
        print(f"Task: {task_type}")
        print(f"Target: {target_col}")
        print(f"Features: {len(X.columns)}")
        print(f"Train samples: {len(X_train)}, Test samples: {len(X_test)}")
        if task_type == "classification":
            print(f"Classes: {sorted(y.unique())}")
            
    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --target_column
      - {inputValue: target_column}
      - --task_type
      - {inputValue: task_type}
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --target_data
      - {outputPath: target_data}
      - --data_info
      - {outputPath: data_info}
