name: Unified Boosting Evaluator
description: Evaluates trained boosting model using unified configuration
inputs:
  - name: trained_model
    type: Model
    description: Trained model from trainer
  - name: processed_data
    type: Dataset
    description: Preprocessed data from preprocessor
  - name: training_history
    type: String
    description: Training history from trainer
  - name: config_str
    type: String
    description: 'Unified configuration JSON string'
outputs:
  - name: metrics
    type: Metrics
  - name: metrics_json
    type: String
  - name: evaluation_report
    type: String

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import sys, os, pickle, json, pandas as pd, numpy as np
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
        import argparse

        # Define DataWrapper class FIRST before unpickling
        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)

        print("STARTING UNIFIED BOOSTING EVALUATOR")

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--processed_data', type=str, required=True)
        parser.add_argument('--training_history', type=str, required=True)
        parser.add_argument('--config_str', type=str, required=True)
        parser.add_argument('--metrics', type=str, required=True)
        parser.add_argument('--metrics_json', type=str, required=True)
        parser.add_argument('--evaluation_report', type=str, required=True)
        args = parser.parse_args()

        # Load model, data, and history
        with open(args.trained_model, 'rb') as f:
            model = pickle.load(f)
        with open(args.processed_data, 'rb') as f:
            data_wrapper = pickle.load(f)
        with open(args.training_history, 'r') as f:
            training_history = json.load(f)
        config = json.loads(args.config_str)

        # Extract information
        X_test = data_wrapper.X_test
        y_test = data_wrapper.y_test
        feature_names = data_wrapper.feature_names
        dataset_info = getattr(data_wrapper, 'dataset_info', {})
        
        task = training_history.get('task', 'classification')
        algorithm = training_history.get('algorithm', 'Unknown')
        test_metrics = training_history.get('test_metrics', {})

        print(f'Evaluating {algorithm} for {task} on {X_test.shape[0]} test samples')

        # Make predictions
        y_pred = model.predict(X_test)
        
        # Calculate additional metrics
        if task == 'regression':
            metrics_dict = {
                'r2_score': float(r2_score(y_test, y_pred)),
                'mae': float(mean_absolute_error(y_test, y_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_test, y_pred)))
            }
            if np.any(y_test == 0):
                mape = np.mean(np.abs((y_test - y_pred) / (np.where(y_test != 0, y_test, 1)))) * 100
            else:
                mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            metrics_dict['mape'] = float(mape)
        else:
            metrics_dict = {
                'accuracy': float(accuracy_score(y_test, y_pred)),
                'precision': float(precision_score(y_test, y_pred, zero_division=0)),
                'recall': float(recall_score(y_test, y_pred, zero_division=0)),
                'f1_score': float(f1_score(y_test, y_pred, zero_division=0))
            }
            if hasattr(model, 'predict_proba'):
                y_pred_proba = model.predict_proba(X_test)[:, 1]
                metrics_dict['roc_auc'] = float(roc_auc_score(y_test, y_pred_proba))
            cm = confusion_matrix(y_test, y_pred)
            metrics_dict['confusion_matrix'] = cm.tolist()

        # Create comprehensive metrics
        metrics = {
            'algorithm': algorithm,
            'task': task,
            'dataset': dataset_info.get('dataset_name', 'Unknown'),
            'test_metrics': metrics_dict,
            'training_metrics': test_metrics,
            'target_statistics': {
                'actual_mean': float(np.mean(y_test)),
                'actual_std': float(np.std(y_test)),
                'predicted_mean': float(np.mean(y_pred)),
                'predicted_std': float(np.std(y_pred))
            },
            'model_info': {
                'test_samples': len(y_test),
                'features_used': X_test.shape[1],
                'feature_names': feature_names
            }
        }

        # Create evaluation report
        if task == 'regression':
            evaluation_report = f'''
            REGRESSION EVALUATION REPORT
            ============================
            Algorithm: {algorithm}
            Dataset: {metrics["dataset"]}
            
            PERFORMANCE METRICS:
            RÂ² Score: {metrics_dict['r2_score']:.4f}
            MAE: {metrics_dict['mae']:.4f}
            RMSE: {metrics_dict['rmse']:.4f}
            MAPE: {metrics_dict.get('mape', 0):.2f}%
            
            TARGET STATISTICS:
            Actual - Mean: {np.mean(y_test):.4f}, Std: {np.std(y_test):.4f}
            Predicted - Mean: {np.mean(y_pred):.4f}, Std: {np.std(y_pred):.4f}
            
            TEST SAMPLES: {len(y_test)}
            FEATURES: {len(feature_names)}
            '''
        else:
            evaluation_report = f'''
            CLASSIFICATION EVALUATION REPORT
            ===============================
            Algorithm: {algorithm}
            Dataset: {metrics["dataset"]}
            
            PERFORMANCE METRICS:
            Accuracy: {metrics_dict['accuracy']:.4f}
            Precision: {metrics_dict['precision']:.4f}
            Recall: {metrics_dict['recall']:.4f}
            F1-Score: {metrics_dict['f1_score']:.4f}
            {f"AUC-ROC: {metrics_dict.get('roc_auc', 0):.4f}" if 'roc_auc' in metrics_dict else ''}
            
            TARGET DISTRIBUTION:
            Class 0: {np.sum(y_test == 0)} samples
            Class 1: {np.sum(y_test == 1)} samples
            
            TEST SAMPLES: {len(y_test)}
            FEATURES: {len(feature_names)}
            '''

        # Save outputs
        os.makedirs(os.path.dirname(args.metrics) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.metrics_json) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.evaluation_report) or ".", exist_ok=True)
        
        with open(args.metrics, 'w') as f:
            json.dump(metrics, f, indent=2)
        with open(args.metrics_json, 'w') as f:
            json.dump(metrics, f, indent=2)
        with open(args.evaluation_report, 'w') as f:
            f.write(evaluation_report)
        
        print(f"Evaluation completed for {task}")
        print(f"Final metrics: {metrics_dict}")
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --processed_data
      - {inputPath: processed_data}
      - --training_history
      - {inputPath: training_history}
      - --config_str
      - {inputValue: config_str}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
      - --evaluation_report
      - {outputPath: evaluation_report}
