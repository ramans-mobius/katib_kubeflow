name: Generic Boosting Data Loader
description: Downloads data from CDN and prepares for boosting algorithms

inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download data file'}
  - {name: dataset_name, type: String, description: 'Dataset name to extract'}
  - {name: target_column, type: String, description: 'Target column name for prediction'}
  - {name: train_split, type: Float, description: 'Train split ratio'}
  - {name: shuffle_seed, type: Integer, description: 'Random seed for shuffling'}

outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: dataset_info, type: DatasetInfo}

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas numpy requests >/dev/null 2>&1
        python -c "
        import os
        import sys
        import pandas as pd
        import requests
        import pickle
        import json
        from urllib.parse import unquote

        cdn_url = sys.argv[1]
        dataset_name = sys.argv[2]
        target_column = sys.argv[3]
        train_split = float(sys.argv[4])
        shuffle_seed = int(sys.argv[5])
        train_data_path = sys.argv[6]
        test_data_path = sys.argv[7]
        dataset_info_path = sys.argv[8]

        print('Generic Boosting Data Loader Started')
        print(f'Dataset: {dataset_name}')

        decoded_url = unquote(cdn_url)
        response = requests.get(decoded_url, timeout=30)
        response.raise_for_status()
        content = response.text

        lines = content.strip().split('
')
        current_section = None
        json_buffer = []
        in_target_section = False
        in_json_array = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            if line.endswith('Data:'):
                current_section = line.replace('Data:', '').strip()
                if dataset_name in current_section:
                    in_target_section = True
                    continue
                else:
                    in_target_section = False
                    continue
            if in_target_section:
                if line == '[':
                    in_json_array = True
                    continue
                if in_json_array:
                    if line == ']':
                        in_json_array = False
                        break
                    json_buffer.append(line)

        if not json_buffer:
            raise ValueError(f'No data found for dataset: {dataset_name}')

        json_str = '[' + ','.join(json_buffer) + ']'
        data = json.loads(json_str)
        df = pd.json_normalize(data)

        if target_column not in df.columns:
            available_targets = [col for col in df.columns if col not in ['id', 'timestamp', 'tenantid']]
            if available_targets:
                target_column = available_targets[0]
            else:
                target_column = df.columns[0]

        df = df.sample(frac=1, random_state=shuffle_seed).reset_index(drop=True)
        train_size = int(len(df) * train_split)
        train_df = df.iloc[:train_size]
        test_df = df.iloc[train_size:]

        feature_columns = [col for col in df.columns if col != target_column]
        
        dataset_info = {
            'dataset_name': dataset_name,
            'total_samples': len(df),
            'train_samples': len(train_df),
            'test_samples': len(test_df),
            'target_column': target_column,
            'feature_columns': feature_columns,
            'all_columns': list(df.columns),
            'train_split_ratio': train_split,
            'shuffle_seed': shuffle_seed
        }

        os.makedirs(os.path.dirname(train_data_path) or '.', exist_ok=True)
        train_df.to_csv(train_data_path, index=False)

        os.makedirs(os.path.dirname(test_data_path) or '.', exist_ok=True)
        test_df.to_csv(test_data_path, index=False)

        os.makedirs(os.path.dirname(dataset_info_path) or '.', exist_ok=True)
        with open(dataset_info_path, 'wb') as f:
            pickle.dump(dataset_info, f)

        print('Data Loading Complete')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7"
    args:
      - {inputValue: cdn_url}
      - {inputValue: dataset_name}
      - {inputValue: target_column}
      - {inputValue: train_split}
      - {inputValue: shuffle_seed}
      - {outputPath: train_data}
      - {outputPath: test_data}
      - {outputPath: dataset_info}
