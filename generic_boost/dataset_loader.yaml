name: 1 Load Dataset from Data.txt
description: Loads specific datasets from data.txt file based on dataset name, extracts relevant features, and splits into train/test datasets.

inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download data.txt file'}
  - {name: dataset_name, type: String, description: 'Dataset name to extract (e.g., GRM_SERVER_REACHABLE, LineChart-Generation-Dbaas, etc.)'}
  - {name: target_column, type: String, description: 'Target column name for prediction'}
  - {name: train_split, type: Float, description: 'Train split ratio (default 0.7)'}
  - {name: shuffle_seed, type: Integer, description: 'Random seed for shuffling'}

outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: dataset_info, type: DatasetInfo}

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas numpy requests scikit-learn >/dev/null 2>&1
        python -c "
        import os
        import sys
        import io
        import json
        import pandas as pd
        import numpy as np
        import requests
        import pickle
        from urllib.parse import unquote
        from sklearn.model_selection import train_test_split

        cdn_url = sys.argv[1]
        dataset_name = sys.argv[2]
        target_column = sys.argv[3]
        train_split = float(sys.argv[4])
        shuffle_seed = int(sys.argv[5])
        train_data_path = sys.argv[6]
        test_data_path = sys.argv[7]
        dataset_info_path = sys.argv[8]

        print('Generic Dataset Loader from data.txt')
        print(f'Dataset: {dataset_name}')
        print(f'Target column: {target_column}')
        print(f'Train split: {train_split}')

        # Dataset configuration mapping
        DATASET_CONFIGS = {
            'GRM_SERVER_REACHABLE': {
                'section': 'Server Reach-ability Data',
                'features': ['nodeip', 'nodeType', 'reachable', 'nodename', 'timestamp'],
                'target': 'reachable',
                'parser': 'server_reachability'
            },
            'LineChart-Generation-Dbaas': {
                'section': 'API Performance QPS Data', 
                'features': ['value', 'timestamp', 'serviceName', 'metricsName'],
                'target': 'value',
                'parser': 'chart_data'
            },
            'LLM-MODEL-BUILDING-TRAINING': {
                'section': 'DSML data',
                'features': ['name', 'description', 'neuronType', 'inputs', 'outputs'],
                'target': 'description',
                'parser': 'dsml_neuron'
            },
            'Mig-Setup-Single-Playbook': {
                'section': 'DSML data',
                'features': ['name', 'description', 'playbookNodeActivateMetadata'],
                'target': 'description',
                'parser': 'mig_setup'
            },
            'GNN-STGNN': {
                'section': 'Cluster Health Data',
                'features': ['node', 'used', 'capacity', 'uptime', 'conditions'],
                'target': 'used',
                'parser': 'cluster_health'
            }
        }

        def safe_float_conversion(value, default=0.0):
          
            try:
                return float(value)
            except (ValueError, TypeError):
                return default

        def parse_server_reachability(data):

            records = []
            for item in data:
                record = {
                    'node': item.get('nodename', ''),
                    'nodeip': item.get('nodeip', ''),
                    'nodeType': item.get('nodeType', ''),
                    'reachable': 1 if str(item.get('reachable', 'False')).lower() == 'true' else 0,
                    'timestamp': item.get('timestamp', '')
                }
                records.append(record)
            return pd.DataFrame(records)

        def parse_chart_data(data):
       
            records = []
            for item in data:
                try:
                    # Safely convert value to float
                    value_str = item.get('value', '0')
                    value = safe_float_conversion(value_str)
                    
                    records.append({
                        'service': item.get('serviceName', ''),
                        'metric': item.get('metricsName', ''),
                        'value': value,
                        'timestamp': item.get('timestamp', ''),
                        'test_suite': item.get('testSuiteId', '')
                    })
                except Exception as e:
                    print(f'Warning: Skipping record due to error: {e}')
                    continue
            return pd.DataFrame(records)

        def parse_dsml_neuron(data):
   
            records = []
            for node in data.get('nodes', []):
                props = node.get('properties', {})
                try:
                    inputs_list = json.loads(props.get('inputs', '[]')) if props.get('inputs') else []
                    outputs_list = json.loads(props.get('outputs', '[]')) if props.get('outputs') else []
                except:
                    inputs_list = []
                    outputs_list = []
                
                records.append({
                    'name': props.get('name', ''),
                    'description': props.get('description', ''),
                    'neuron_type': props.get('neuronType', ''),
                    'inputs_count': len(inputs_list),
                    'outputs_count': len(outputs_list)
                })
            return pd.DataFrame(records)

        def parse_mig_setup(data):
    
            records = []
            for node in data.get('nodes', []):
                props = node.get('properties', {})
                metadata = props.get('playbookNodeActivateMetadata', '{}')
                try:
                    # Clean and parse metadata
                    cleaned_metadata = metadata.replace('\\\\n', '').replace('\\\\t', '')
                    metadata_dict = json.loads(cleaned_metadata) if cleaned_metadata else {}
                    body = metadata_dict.get('body', {})
                    
                    records.append({
                        'name': props.get('name', ''),
                        'description': props.get('description', ''),
                        'gpu_type': 'A100' if 'A100' in str(body) else 'H100' if 'H100' in str(body) else 'Unknown',
                        'mig_profile': '1g.10gb' if '1g.10gb' in str(body) else 'other',
                        'batch_size': body.get('batch', 8) if isinstance(body, dict) else 8
                    })
                except Exception as e:
                    print(f'Warning: Error parsing MIG setup: {e}')
                    records.append({
                        'name': props.get('name', ''),
                        'description': props.get('description', ''),
                        'gpu_type': 'Unknown',
                        'mig_profile': 'unknown',
                        'batch_size': 8
                    })
            return pd.DataFrame(records)

        def parse_cluster_health(data):
       
            records = []
            for item in data:
                used = item.get('used', {})
                capacity = item.get('capacity', {})
                
                # Safely parse CPU values
                cpu_used_str = used.get('cpu', '0')
                cpu_capacity_str = capacity.get('cpu', '0')
                
                records.append({
                    'node': item.get('node', ''),
                    'cpu_used': safe_float_conversion(cpu_used_str.replace('c', '')),
                    'memory_used': used.get('memory', '0B'),
                    'pods_used': used.get('pods', 0),
                    'cpu_capacity': safe_float_conversion(cpu_capacity_str.replace('c', '')),
                    'memory_capacity': capacity.get('memory', '0B'),
                    'uptime': item.get('uptime', '0d')
                })
            return pd.DataFrame(records)

        # Download and parse data.txt
        decoded_url = unquote(cdn_url)
        try:
            print('Downloading data.txt')
            response = requests.get(decoded_url, timeout=30)
            response.raise_for_status()
            content = response.text
            print(f'Downloaded {len(content)} characters')
        except Exception as e:
            print(f'Failed to download data.txt: {e}')
            sys.exit(1)

        # Parse the structured data from data.txt
        datasets = {}
        current_section = None
        current_data = []
        
        lines = content.split('\\n')
        print(f'Processing {len(lines)} lines')
        
        for line in lines:
            line = line.strip()
            if line.endswith('Data:'):
                if current_section and current_data:
                    datasets[current_section] = current_data
                    print(f'Stored section: {current_section} with {len(current_data)} items')
                current_section = line.replace(' Data:', '').strip()
                current_data = []
                print(f'Found section: {current_section}')
            elif line.startswith('[') or line.startswith('{'):
                try:
                    # Clean the line for JSON parsing
                    clean_line = line.strip()
                    if clean_line.startswith('[') and not clean_line.endswith(']'):
                        continue
                    if clean_line.startswith('{') and not clean_line.endswith('}'):
                        continue
                        
                    data = json.loads(clean_line)
                    if isinstance(data, list):
                        current_data.extend(data)
                    else:
                        current_data.append(data)
                except json.JSONDecodeError as e:
                    print(f'Warning: Could not parse line: {line[:100]}...')
                    continue
                except Exception as e:
                    print(f'Warning: Unexpected error parsing line: {e}')
                    continue
        
        if current_section and current_data:
            datasets[current_section] = current_data
            print(f'Stored final section: {current_section} with {len(current_data)} items')

        print(f'Found sections: {list(datasets.keys())}')

        # Get dataset config
        config = DATASET_CONFIGS.get(dataset_name)
        if not config:
            raise ValueError(f'Dataset {dataset_name} not found in configuration. Available: {list(DATASET_CONFIGS.keys())}')

        section_data = datasets.get(config['section'], [])
        if not section_data:
            raise ValueError(f'Section {config[\\\"section\\\"]} not found in data.txt. Available sections: {list(datasets.keys())}')

        print(f'Processing {len(section_data)} items from section {config[\\\"section\\\"]}')

        # Parse the specific dataset
        parser_func = globals()[f'parse_{config[\\\"parser\\\"]}']
        df = parser_func(section_data)

        if df.empty:
            raise ValueError(f'No data could be parsed for dataset {dataset_name}')

        print(f'Parsed dataset shape: {df.shape}')
        print(f'Columns: {list(df.columns)}')
        print(f'First few rows:')
        print(df.head())

        # Check if target column exists
        if target_column not in df.columns:
            raise ValueError(f'Target column {target_column} not found in dataset. Available columns: {list(df.columns)}')

        # Split data
        train_df, test_df = train_test_split(
            df, 
            train_size=train_split, 
            random_state=shuffle_seed,
            shuffle=True
        )

        print(f'Train shape: {train_df.shape}, Test shape: {test_df.shape}')

        # Create dataset info
        feature_columns = [col for col in df.columns if col != target_column]
        dataset_info = {
            'dataset_name': dataset_name,
            'total_samples': len(df),
            'train_samples': len(train_df),
            'test_samples': len(test_df),
            'target_column': target_column,
            'feature_columns': feature_columns,
            'train_split_ratio': train_split,
            'shuffle_seed': shuffle_seed,
            'columns': list(df.columns),
            'dtypes': {col: str(df[col].dtype) for col in df.columns},
            'config_used': config
        }

        # Save outputs
        os.makedirs(os.path.dirname(train_data_path) or '.', exist_ok=True)
        train_df.to_csv(train_data_path, index=False)

        os.makedirs(os.path.dirname(test_data_path) or '.', exist_ok=True)
        test_df.to_csv(test_data_path, index=False)

        os.makedirs(os.path.dirname(dataset_info_path) or '.', exist_ok=True)
        with open(dataset_info_path, 'wb') as f:
            pickle.dump(dataset_info, f)

        print('Dataset loading complete!')
        print(f'Train data saved to: {train_data_path}')
        print(f'Test data saved to: {test_data_path}')
        print(f'Dataset info saved to: {dataset_info_path}')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7"
    args:
      - {inputValue: cdn_url}
      - {inputValue: dataset_name}
      - {inputValue: target_column}
      - {inputValue: train_split}
      - {inputValue: shuffle_seed}
      - {outputPath: train_data}
      - {outputPath: test_data}
      - {outputPath: dataset_info}
