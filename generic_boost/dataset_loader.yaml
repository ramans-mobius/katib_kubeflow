name: Load Dataset from Datatxt
description: Loads specific datasets from data.txt file based on dataset name, extracts relevant features, and splits into train/test datasets.

inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download data.txt file'}
  - {name: dataset_name, type: String, description: 'Dataset name to extract (e.g., GRM_SERVER_REACHABLE, LineChart-Generation-Dbaas, etc.)'}
  - {name: target_column, type: String, description: 'Target column name for prediction'}
  - {name: train_split, type: Float, description: 'Train split ratio (default 0.7)'}
  - {name: shuffle_seed, type: Integer, description: 'Random seed for shuffling'}

outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: dataset_info, type: DatasetInfo}

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas numpy requests scikit-learn >/dev/null 2>&1
        python -c "
        import os
        import sys
        import io
        import json
        import pandas as pd
        import numpy as np
        import requests
        import pickle
        from urllib.parse import unquote
        from sklearn.model_selection import train_test_split

        cdn_url = sys.argv[1]
        dataset_name = sys.argv[2]
        target_column = sys.argv[3]
        train_split = float(sys.argv[4])
        shuffle_seed = int(sys.argv[5])
        train_data_path = sys.argv[6]
        test_data_path = sys.argv[7]
        dataset_info_path = sys.argv[8]

        print('Generic Dataset Loader from data.txt')
        print(f'Dataset: {dataset_name}')
        print(f'Target column: {target_column}')
        print(f'Train split: {train_split}')

        # Dataset configuration mapping
        DATASET_CONFIGS = {
            'GRM_SERVER_REACHABLE': {
                'section': 'Server Reach-ability Data',
                'features': ['nodeip', 'nodeType', 'reachable', 'nodename', 'timestamp'],
                'target': 'reachable',
                'parser': 'server_reachability'
            },
            'LineChart-Generation-Dbaas': {
                'section': 'API Performance QPS Data', 
                'features': ['value', 'timestamp', 'serviceName', 'metricsName'],
                'target': 'value',
                'parser': 'chart_data'
            },
            'LLM-MODEL-BUILDING-TRAINING': {
                'section': 'DSML data',
                'features': ['name', 'description', 'neuronType', 'inputs', 'outputs'],
                'target': 'description',
                'parser': 'dsml_neuron'
            },
            'Mig-Setup-Single-Playbook': {
                'section': 'DSML data',
                'features': ['name', 'description', 'playbookNodeActivateMetadata'],
                'target': 'description',
                'parser': 'mig_setup'
            },
            'GNN-STGNN': {
                'section': 'Cluster Health Data',
                'features': ['node', 'used', 'capacity', 'uptime', 'conditions'],
                'target': 'used',
                'parser': 'cluster_health'
            }
        }

        def parse_server_reachability(data):
            \"\"\"Parse server reachability data\"\"\"
            records = []
            for item in data:
                record = {
                    'node': item.get('nodename', ''),
                    'nodeip': item.get('nodeip', ''),
                    'nodeType': item.get('nodeType', ''),
                    'reachable': 1 if str(item.get('reachable', 'False')).lower() == 'true' else 0,
                    'timestamp': item.get('timestamp', '')
                }
                records.append(record)
            return pd.DataFrame(records)

        def parse_chart_data(data):
            \"\"\"Parse chart/QPS data\"\"\"
            records = []
            for item in data:
                try:
                    value = float(item.get('value', 0))
                    records.append({
                        'service': item.get('serviceName', ''),
                        'metric': item.get('metricsName', ''),
                        'value': value,
                        'timestamp': item.get('timestamp', ''),
                        'test_suite': item.get('testSuiteId', '')
                    })
                except (ValueError, TypeError):
                    continue
            return pd.DataFrame(records)

        def parse_dsml_neuron(data):
            \"\"\"Parse DSML neuron data\"\"\"
            records = []
            for node in data.get('nodes', []):
                props = node.get('properties', {})
                records.append({
                    'name': props.get('name', ''),
                    'description': props.get('description', ''),
                    'neuron_type': props.get('neuronType', ''),
                    'inputs': len(json.loads(props.get('inputs', '[]'))),
                    'outputs': len(json.loads(props.get('outputs', '[]')))
                })
            return pd.DataFrame(records)

        def parse_mig_setup(data):
            \"\"\"Parse MIG setup data\"\"\"
            records = []
            for node in data.get('nodes', []):
                props = node.get('properties', {})
                metadata = props.get('playbookNodeActivateMetadata', '{}')
                try:
                    metadata_dict = json.loads(metadata.replace('\\n', '')) if metadata else {}
                    body = metadata_dict.get('body', {})
                    records.append({
                        'name': props.get('name', ''),
                        'description': props.get('description', ''),
                        'gpu_type': 'A100' if 'A100' in str(body) else 'H100' if 'H100' in str(body) else 'Unknown',
                        'mig_profile': '1g.10gb' if '1g.10gb' in str(body) else 'other',
                        'batch_size': body.get('batch', 8) if isinstance(body, dict) else 8
                    })
                except:
                    records.append({
                        'name': props.get('name', ''),
                        'description': props.get('description', ''),
                        'gpu_type': 'Unknown',
                        'mig_profile': 'unknown',
                        'batch_size': 8
                    })
            return pd.DataFrame(records)

        def parse_cluster_health(data):
            \"\"\"Parse cluster health data\"\"\"
            records = []
            for item in data:
                used = item.get('used', {})
                capacity = item.get('capacity', {})
                records.append({
                    'node': item.get('node', ''),
                    'cpu_used': int(used.get('cpu', '0').replace('c', '') or 0),
                    'memory_used': used.get('memory', '0B'),
                    'pods_used': used.get('pods', 0),
                    'cpu_capacity': int(capacity.get('cpu', '0').replace('c', '') or 0),
                    'memory_capacity': capacity.get('memory', '0B'),
                    'uptime': item.get('uptime', '0d')
                })
            return pd.DataFrame(records)

        # Download and parse data.txt
        decoded_url = unquote(cdn_url)
        try:
            print('Downloading data.txt')
            response = requests.get(decoded_url, timeout=30)
            response.raise_for_status()
            content = response.text
        except Exception as e:
            print(f'Failed to download data.txt: {e}')
            raise

        # Parse the structured data from data.txt
        datasets = {}
        current_section = None
        current_data = []
        
        for line in content.split('\\n'):
            line = line.strip()
            if line.endswith('Data:'):
                if current_section and current_data:
                    datasets[current_section] = current_data
                current_section = line.replace(' Data:', '').strip()
                current_data = []
            elif line.startswith('[') or line.startswith('{'):
                try:
                    if line.startswith('['):
                        data = json.loads(line)
                    else:
                        data = json.loads(line)
                    current_data.extend(data if isinstance(data, list) else [data])
                except:
                    continue
        
        if current_section and current_data:
            datasets[current_section] = current_data

        print(f'Found sections: {list(datasets.keys())}')

        # Get dataset config
        config = DATASET_CONFIGS.get(dataset_name)
        if not config:
            raise ValueError(f'Dataset {dataset_name} not found in configuration')

        section_data = datasets.get(config['section'], [])
        if not section_data:
            raise ValueError(f'Section {config[\\\"section\\\"]} not found in data.txt')

        # Parse the specific dataset
        parser_func = globals()[f'parse_{config[\\\"parser\\\"]}']
        df = parser_func(section_data)

        print(f'Parsed dataset shape: {df.shape}')
        print(f'Columns: {list(df.columns)}')

        # Split data
        train_df, test_df = train_test_split(
            df, 
            train_size=train_split, 
            random_state=shuffle_seed,
            shuffle=True
        )

        print(f'Train shape: {train_df.shape}, Test shape: {test_df.shape}')

        # Create dataset info
        feature_columns = [col for col in df.columns if col != target_column]
        dataset_info = {
            'dataset_name': dataset_name,
            'total_samples': len(df),
            'train_samples': len(train_df),
            'test_samples': len(test_df),
            'target_column': target_column,
            'feature_columns': feature_columns,
            'train_split_ratio': train_split,
            'shuffle_seed': shuffle_seed,
            'columns': list(df.columns),
            'dtypes': {col: str(df[col].dtype) for col in df.columns},
            'config_used': config
        }

        # Save outputs
        os.makedirs(os.path.dirname(train_data_path) or '.', exist_ok=True)
        train_df.to_csv(train_data_path, index=False)

        os.makedirs(os.path.dirname(test_data_path) or '.', exist_ok=True)
        test_df.to_csv(test_data_path, index=False)

        os.makedirs(os.path.dirname(dataset_info_path) or '.', exist_ok=True)
        with open(dataset_info_path, 'wb') as f:
            pickle.dump(dataset_info, f)

        print('Dataset loading complete!')
        print(f'Train data: {train_data_path}')
        print(f'Test data: {test_data_path}')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7"
    args:
      - {inputValue: cdn_url}
      - {inputValue: dataset_name}
      - {inputValue: target_column}
      - {inputValue: train_split}
      - {inputValue: shuffle_seed}
      - {outputPath: train_data}
      - {outputPath: test_data}
      - {outputPath: dataset_info}
