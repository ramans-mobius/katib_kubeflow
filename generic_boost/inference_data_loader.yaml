name: Inference Data & Model Loader
description: Loads trained model and inference data for prediction
inputs:
  - name: trained_model
    type: Model
    description: Trained model from training pipeline
  - name: preprocessing_pipeline
    type: Model
    description: Preprocessing pipeline from training
  - name: test_data
    type: Dataset
    description: Test data from training (fallback)
  - name: inference_data_url
    type: String
    description: URL to new inference data (optional)
outputs:
  - name: loaded_model
    type: Model
    description: Loaded trained model
  - name: processed_inference_data
    type: Dataset
    description: Preprocessed data ready for inference
  - name: data_source_info
    type: String
    description: Information about data source used

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - python3
      - -u
      - -c
      - |
        import sys, os, pickle, json, pandas as pd, numpy as np, requests
        import argparse
        import subprocess

        print("STARTING INFERENCE DATA & MODEL LOADER")

        # Install required packages for model loading
        print("Installing required packages...")
        packages = ['lightgbm', 'xgboost', 'catboost']
        for package in packages:
            try:
                subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', package])
                print(f"Installed {package}")
            except subprocess.CalledProcessError:
                print(f"Warning: Failed to install {package}")

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--preprocessing_pipeline', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--inference_data_url', type=str, required=True)
        parser.add_argument('--loaded_model', type=str, required=True)
        parser.add_argument('--processed_inference_data', type=str, required=True)
        parser.add_argument('--data_source_info', type=str, required=True)
        args = parser.parse_args()

        print("Arguments loaded")

        # Load trained model
        try:
            with open(args.trained_model, 'rb') as f:
                model = pickle.load(f)
            print(f"Model loaded successfully: {type(model).__name__}")
        except Exception as e:
            print("ERROR loading model: " + str(e))
            sys.exit(1)

        # Load preprocessing pipeline
        try:
            with open(args.preprocessing_pipeline, 'rb') as f:
                preprocessor = pickle.load(f)
            print("Preprocessing pipeline loaded successfully")
        except Exception as e:
            print("ERROR loading preprocessor: " + str(e))
            sys.exit(1)

        # Load inference data (new data or test data fallback)
        inference_data = []
        data_source = "test_data"

        if args.inference_data_url and args.inference_data_url != "none":
            try:
                response = requests.get(args.inference_data_url)
                response.raise_for_status()
                content = response.text
                print(f"Downloaded new inference data: {len(content)} characters")
                
                # Parse JSON data
                lines = content.splitlines()
                json_buffer = []
                in_json_section = False
                
                for line in lines:
                    line = line.strip()
                    if "GNN-STGNN Spatio-Temporal Data:" in line:
                        in_json_section = True
                        continue
                    if in_json_section and line == '[':
                        continue
                    if in_json_section and line == ']':
                        break
                    if in_json_section and line:
                        json_buffer.append(line)
                
                if json_buffer:
                    json_str = '[' + ','.join(json_buffer) + ']'
                    inference_data = json.loads(json_str)
                    data_source = "new_data"
                    print(f"Successfully loaded {len(inference_data)} new inference samples")
                else:
                    print("No new inference data found, falling back to test data")
                    
            except Exception as e:
                print(f"Failed to load new data: {e}, falling back to test data")

        # If no new data, use test data
        if not inference_data:
            try:
                test_df = pd.read_csv(args.test_data)
                inference_data = test_df.to_dict('records')
                print(f"Using test data: {len(inference_data)} samples")
            except Exception as e:
                print("ERROR loading test data: " + str(e))
                sys.exit(1)

        # Process data
        try:
            df = pd.DataFrame(inference_data)
            target_column = preprocessor.get('target_column', 'temporal_signal')
            feature_columns = [col for col in df.columns if col != target_column]
            X_inference = df[feature_columns].copy()
            
            class InferenceDataWrapper:
                def __init__(self, X_data, original_data, source):
                    self.X_inference = X_data
                    self.original_data = original_data
                    self.feature_columns = list(X_data.columns)
                    self.data_source = source
            
            data_wrapper = InferenceDataWrapper(X_inference, inference_data, data_source)
            
            # Save outputs
            os.makedirs(os.path.dirname(args.loaded_model), exist_ok=True)
            with open(args.loaded_model, 'wb') as f:
                pickle.dump(model, f)
            
            os.makedirs(os.path.dirname(args.processed_inference_data), exist_ok=True)
            with open(args.processed_inference_data, 'wb') as f:
                pickle.dump(data_wrapper, f)
            
            os.makedirs(os.path.dirname(args.data_source_info), exist_ok=True)
            with open(args.data_source_info, 'w') as f:
                json.dump({
                    "data_source": data_source,
                    "samples": len(inference_data),
                    "features": len(X_inference.columns),
                    "model_type": type(model).__name__
                }, f, indent=2)
                
            print("Inference data & model loading completed successfully")
            print(f"Data source: {data_source}")
            print(f"Samples: {len(inference_data)}")
            print(f"Model: {type(model).__name__}")
            
        except Exception as e:
            print("ERROR processing inference data: " + str(e))
            sys.exit(1)
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --preprocessing_pipeline
      - {inputPath: preprocessing_pipeline}
      - --test_data
      - {inputPath: test_data}
      - --inference_data_url
      - {inputValue: inference_data_url}
      - --loaded_model
      - {outputPath: loaded_model}
      - --processed_inference_data
      - {outputPath: processed_inference_data}
      - --data_source_info
      - {outputPath: data_source_info}
