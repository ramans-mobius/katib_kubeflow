name: 2 Generic Boosting Evaluator
description: Evaluates trained boosting model with algorithm-specific evaluation metrics
inputs:
  - name: trained_model
    type: Model
    description: Trained model from trainer
  - name: data_path
    type: Dataset
    description: Preprocessed data from preprocessor
  - name: model_coefficients
    type: String
    description: Feature importances from trainer
outputs:
  - name: metrics
    type: Metrics
    description: Evaluation metrics
  - name: metrics_json
    type: String
    description: Metrics as JSON string
  - name: evaluation_report
    type: String
    description: Comprehensive evaluation report

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - python3
      - -u
      - -c
      - |
        import sys, os, pickle, json, pandas as pd, numpy as np
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
        import argparse

        print("STARTING BOOSTING EVALUATOR")

        class DataWrapper:
            def __init__(self, data_dict=None):
                if data_dict:
                    self.__dict__.update(data_dict)

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--data_path', type=str, required=True)
        parser.add_argument('--model_coefficients', type=str, required=True)
        parser.add_argument('--metrics', type=str, required=True)
        parser.add_argument('--metrics_json', type=str, required=True)
        parser.add_argument('--evaluation_report', type=str, required=True)
        args = parser.parse_args()

        print("Arguments loaded")

        if not os.path.exists(args.trained_model):
            print("ERROR: trained_model path does not exist!")
            sys.exit(1)
        if not os.path.exists(args.data_path):
            print("ERROR: data_path does not exist!")
            sys.exit(1)
        if not os.path.exists(args.model_coefficients):
            print("ERROR: model_coefficients path does not exist!")
            sys.exit(1)

        # Load model and data
        try:
            with open(args.trained_model, 'rb') as f:
                model = pickle.load(f)
            print("Model loaded successfully")
        except Exception as e:
            print("ERROR loading model: " + str(e))
            sys.exit(1)

        try:
            with open(args.data_path, 'rb') as f:
                data_wrapper = pickle.load(f)
            print("Data loaded successfully")
        except Exception as e:
            print("ERROR loading data: " + str(e))
            sys.exit(1)

        # Extract data from wrapper
        X_test = data_wrapper.X_test
        y_test = data_wrapper.y_test
        
        # Get dataset info safely
        dataset_info = getattr(data_wrapper, 'dataset_info', {})
        algorithm = type(model).__name__
        dataset_name = dataset_info.get('dataset_name', 'Unknown')

        print(f'Evaluating {algorithm} on {X_test.shape[0]} test samples')
        print(f'Dataset: {dataset_name}')

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate metrics
        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))

        # Safe MAPE calculation
        if np.any(y_test == 0):
            mape = np.mean(np.abs((y_test - y_pred) / (np.where(y_test != 0, y_test, 1)))) * 100
        else:
            mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

        # Create metrics
        metrics = {
            'algorithm': algorithm,
            'dataset': dataset_name,
            'test_metrics': {
                'r2_score': float(r2),
                'mae': float(mae),
                'rmse': float(rmse),
                'mape': float(mape)
            },
            'target_statistics': {
                'actual_mean': float(np.mean(y_test)),
                'actual_std': float(np.std(y_test)),
                'predicted_mean': float(np.mean(y_pred)),
                'predicted_std': float(np.std(y_pred)),
                'actual_min': float(np.min(y_test)),
                'actual_max': float(np.max(y_test)),
                'predicted_min': float(np.min(y_pred)),
                'predicted_max': float(np.max(y_pred))
            },
            'model_info': {
                'test_samples': len(y_test),
                'features_used': X_test.shape[1] if hasattr(X_test, 'shape') else 'unknown',
                'algorithm_type': algorithm
            }
        }

        # Create report
        evaluation_report = f'''BOOSTING ALGORITHM EVALUATION REPORT
        =========================================
        Algorithm: {algorithm}
        Dataset: {dataset_name}
        
        PERFORMANCE METRICS:
        R² Score: {r2:.4f}
        MAE: {mae:.4f}
        RMSE: {rmse:.4f}
        MAPE: {mape:.2f}%
        
        TARGET STATISTICS:
        Actual - Mean: {np.mean(y_test):.4f}, Std: {np.std(y_test):.4f}
        Predicted - Mean: {np.mean(y_pred):.4f}, Std: {np.std(y_pred):.4f}
        Actual Range: [{np.min(y_test):.4f}, {np.max(y_test):.4f}]
        Predicted Range: [{np.min(y_pred):.4f}, {np.max(y_pred):.4f}]
        
        TEST SAMPLES: {len(y_test)}
        FEATURES USED: {X_test.shape[1] if hasattr(X_test, 'shape') else 'unknown'}
        '''

        # Save outputs
        try:
            os.makedirs(os.path.dirname(args.metrics), exist_ok=True)
            os.makedirs(os.path.dirname(args.metrics_json), exist_ok=True)
            os.makedirs(os.path.dirname(args.evaluation_report), exist_ok=True)
            
            with open(args.metrics, 'w') as f:
                json.dump(metrics, f, indent=2)
            
            with open(args.metrics_json, 'w') as f:
                json.dump(metrics, f, indent=2)
            
            with open(args.evaluation_report, 'w') as f:
                f.write(evaluation_report)
            
            print("Evaluation completed successfully")
            print(f'Test R²: {r2:.4f}')
            print(f'Test RMSE: {rmse:.4f}')
        except Exception as e:
            print("ERROR saving results: " + str(e))
            sys.exit(1)
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --data_path
      - {inputPath: data_path}
      - --model_coefficients
      - {inputPath: model_coefficients}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
      - --evaluation_report
      - {outputPath: evaluation_report}
