name: 2 Boosting Inference Results Storage
description: Stores inference prediction results in database
inputs:
  - name: prediction_results
    type: String
    description: Detailed prediction results
  - name: data_source_info
    type: String
    description: Information about data source
  - name: schema_id
    type: String
  - name: model_id
    type: String
  - name: execution_id
    type: String
  - name: tenant_id
    type: string
  - name: project_id
    type: String
  - name: algorithm_type
    type: String
  - name: bearer_auth_token
    type: string
  - name: domain
    type: String
outputs:
  - name: storage_confirmation
    type: String
    description: Confirmation of successful storage

implementation:
  container:
    image: python:3.9-slim
    command:
      - sh
      - -c
      - |
        pip install requests numpy
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import json
        import argparse
        import requests
        import numpy as np
        import os

        parser = argparse.ArgumentParser()
        parser.add_argument('--prediction_results', type=str, required=True)
        parser.add_argument('--data_source_info', type=str, required=True)
        parser.add_argument('--schema_id', type=str, required=True)
        parser.add_argument('--model_id', type=str, required=True)
        parser.add_argument('--execution_id', type=str, required=True)
        parser.add_argument('--tenant_id', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        parser.add_argument('--algorithm_type', type=str, required=True)
        parser.add_argument('--bearer_auth_token', type=str, required=True)
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--storage_confirmation', type=str, required=True)
        args = parser.parse_args()

        # Read auth token and tenant_id
        with open(args.bearer_auth_token, 'r') as f:
            bearer_auth_token = f.read().strip()
        with open(args.tenant_id, 'r') as f:
            tenant_id = f.read().strip()

        # Load prediction results and data source info
        with open(args.prediction_results, 'r') as f:
            prediction_results = json.load(f)
        with open(args.data_source_info, 'r') as f:
            data_source_info = json.load(f)

        print("=== Storing Inference Results ===")

        # Calculate statistics
        predictions = [result['predicted_value'] for result in prediction_results]
        confidence_scores = [result['confidence_score'] for result in prediction_results]
        
        # Get most common prediction for predicted_class
        if predictions:
            # For binary classification, convert to class labels
            predicted_classes = ['class_1' if pred > 0.5 else 'class_0' for pred in predictions]
            most_common_class = max(set(predicted_classes), key=predicted_classes.count)
        else:
            most_common_class = "unknown"

        # Prepare schema data - ONLY FIELDS THAT EXIST IN SCHEMA
        schema_data = {
            # REQUIRED FIELDS (must be present)
            "tenant_id": tenant_id,
            "execution_id": args.execution_id,
            "infernce_input": "spatio_temporal_forecasting",
            "inference_score_rouge": 0.0,
            
            # OPTIONAL FIELDS (that exist in schema)
            "projectId": args.project_id,
            "infernce_output": "predictions_completed",
            "infernce_score_blue": 0.0,
            "model_id": args.model_id,
            "architecture_type": args.algorithm_type,  # Map algorithm_type to architecture_type
            "inference_confidence": float(np.mean(confidence_scores)) if confidence_scores else 0.0,
            "predicted_class": most_common_class,
            "total_samples": len(predictions),
            "ModelName": data_source_info.get('model_type', 'XGBoost_Model')
        }

        print("=== Prepared Schema Data ===")
        print(json.dumps(schema_data, indent=2))

        # Validate required fields
        required_fields = ["tenant_id", "execution_id", "infernce_input", "inference_score_rouge"]
        missing_fields = [field for field in required_fields if field not in schema_data or schema_data[field] is None]
        
        if missing_fields:
            error_msg = f"Missing required fields: {missing_fields}"
            print(f"ERROR: {error_msg}")
            output_dir = os.path.dirname(args.storage_confirmation)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            with open(args.storage_confirmation, 'w') as f:
                json.dump({
                    "status": "error",
                    "message": error_msg,
                    "execution_id": args.execution_id
                }, f, indent=2)
            exit(1)

        # API call to store data
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {bearer_auth_token}'
        }

        create_url = f"{args.domain}/pi-entity-instances-service/v2.0/schemas/{args.schema_id}/instances"
        create_payload = {
            "data": [schema_data]
        }

        print("=== Sending Request ===")
        print(f"URL: {create_url}")
        print(f"Schema ID: {args.schema_id}")

        try:
            response = requests.post(create_url, headers=headers, json=create_payload, timeout=60)
            response.raise_for_status()
            
            # Save confirmation
            output_dir = os.path.dirname(args.storage_confirmation)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            with open(args.storage_confirmation, 'w') as f:
                json.dump({
                    "status": "success",
                    "message": f"Stored {len(predictions)} predictions successfully",
                    "response": response.json(),
                    "execution_id": args.execution_id
                }, f, indent=2)
                
            print("Successfully stored inference results")
            print(f"Stored {len(predictions)} predictions")
            
        except requests.exceptions.RequestException as e:
            error_msg = f"Error storing results: {e}"
            if hasattr(e, 'response') and e.response is not None:
                error_msg += f"\nResponse status: {e.response.status_code}"
                error_msg += f"\nResponse text: {e.response.text}"
                try:
                    error_details = e.response.json()
                    error_msg += f"\nError details: {json.dumps(error_details, indent=2)}"
                except:
                    error_msg += f"\nResponse content: {e.response.content}"
            
            print(error_msg)
            
            # Save error confirmation
            output_dir = os.path.dirname(args.storage_confirmation)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            with open(args.storage_confirmation, 'w') as f:
                json.dump({
                    "status": "error",
                    "message": error_msg,
                    "execution_id": args.execution_id,
                    "url": create_url
                }, f, indent=2)
            exit(1)
    args:
      - --prediction_results
      - {inputPath: prediction_results}
      - --data_source_info
      - {inputPath: data_source_info}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --execution_id
      - {inputValue: execution_id}
      - --tenant_id
      - {inputPath: tenant_id}
      - --project_id
      - {inputValue: project_id}
      - --algorithm_type
      - {inputValue: algorithm_type}
      - --bearer_auth_token
      - {inputPath: bearer_auth_token}
      - --domain
      - {inputValue: domain}
      - --storage_confirmation
      - {outputPath: storage_confirmation}
