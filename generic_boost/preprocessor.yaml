name: 2 Generic Boosting Preprocessor
description: Preprocesses data for boosting algorithms with robust encoding
inputs:
  - name: train_data
    type: Dataset
    description: Training dataset from extractor
  - name: test_data
    type: Dataset
    description: Testing dataset from extractor
  - name: dataset_info
    type: DatasetInfo
    description: Dataset metadata from extractor
  - name: model_config
    type: String
    description: Boosting algorithm configuration
outputs:
  - name: processed_data_pickle
    type: Dataset
    description: Preprocessed data for training
  - name: preprocessing_pipeline
    type: Model
    description: Fitted preprocessing pipeline
  - name: weight_out
    type: String
    description: Preprocessing configuration

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        set -e
        python3 -m pip install --quiet pandas numpy scikit-learn category_encoders
        
        # Create output directories
        mkdir -p /tmp/outputs/processed_data_pickle
        mkdir -p /tmp/outputs/preprocessing_pipeline
        mkdir -p /tmp/outputs/weight_out
        
        python3 -c "
        import sys, os, pickle, json, pandas as pd, numpy as np
        from sklearn.preprocessing import LabelEncoder, StandardScaler
        from sklearn.compose import ColumnTransformer
        from sklearn.pipeline import Pipeline
        import warnings
        warnings.filterwarnings('ignore', category=UserWarning)
        
        print('Number of arguments:', len(sys.argv))
        print('Arguments:', sys.argv)
        
        train_path = sys.argv[1]
        test_path = sys.argv[2]
        info_path = sys.argv[3]
        config_str = sys.argv[4]
        out_path = sys.argv[5]
        pipeline_path = sys.argv[6]
        weight_path = sys.argv[7]
        
        print('Starting preprocessing...')
        
        # Load data
        train_df = pd.read_csv(train_path)
        test_df = pd.read_csv(test_path)
        with open(info_path, 'rb') as f:
            dataset_info = pickle.load(f)
        
        config = json.loads(config_str)
        algorithm = config.get('algorithm', 'GradientBoosting')
        target_col = dataset_info.get('target_column')
        feature_columns = dataset_info.get('feature_columns', [])
        
        print(f'Algorithm: {algorithm}')
        print(f'Dataset: {dataset_info.get(\\\"dataset_name\\\", \\\"Unknown\\\")}')
        print(f'Target: {target_col}')
        
        # Prepare features and target
        X_train = train_df[feature_columns].copy() if feature_columns else train_df.drop(columns=[target_col]).copy()
        y_train = train_df[target_col].copy()
        X_test = test_df[feature_columns].copy() if feature_columns else test_df.drop(columns=[target_col]).copy()
        y_test = test_df[target_col].copy()
        
        print(f'Original shapes - Train: {X_train.shape}, Test: {X_test.shape}')
        
        # Identify column types
        numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()
        
        print(f'Numeric features: {numeric_features}')
        print(f'Categorical features: {categorical_features}')
        
        # Handle timestamp columns specially
        timestamp_features = [col for col in categorical_features if any(keyword in col.lower() for keyword in ['timestamp', 'time', 'date'])]
        regular_categorical = [col for col in categorical_features if col not in timestamp_features]
        
        if timestamp_features:
            print(f'Timestamp features detected: {timestamp_features}')
            # Convert timestamps to numeric (Unix timestamp)
            for col in timestamp_features:
                try:
                    X_train[col] = pd.to_datetime(X_train[col]).astype('int64') // 10**9
                    X_test[col] = pd.to_datetime(X_test[col]).astype('int64') // 10**9
                    # Move from categorical to numeric
                    numeric_features.append(col)
                    if col in regular_categorical:
                        regular_categorical.remove(col)
                except:
                    # If conversion fails, keep as categorical but use robust encoding
                    print(f'Could not convert {col} to timestamp, keeping as categorical')
        
        # Robust categorical encoding that handles unseen values
        class RobustLabelEncoder:
            def __init__(self):
                self.encoder = LabelEncoder()
                self.unknown_token = 'UNKNOWN'
                
            def fit(self, X):
                # Add unknown token to handle unseen values
                unique_vals = list(X.unique()) + [self.unknown_token]
                self.encoder.fit(unique_vals)
                return self
                
            def transform(self, X):
                # Replace unseen values with unknown token
                X_transformed = X.copy()
                unseen_mask = ~X_transformed.isin(self.encoder.classes_[:-1])  # Exclude unknown token
                X_transformed[unseen_mask] = self.unknown_token
                return self.encoder.transform(X_transformed)
                
            def fit_transform(self, X):
                return self.fit(X).transform(X)
        
        # Apply preprocessing
        processed_X_train = X_train.copy()
        processed_X_test = X_test.copy()
        
        # Scale numeric features
        if numeric_features:
            scaler = StandardScaler()
            processed_X_train[numeric_features] = scaler.fit_transform(processed_X_train[numeric_features])
            processed_X_test[numeric_features] = scaler.transform(processed_X_test[numeric_features])
        
        # Encode categorical features
        categorical_encoders = {}
        for col in regular_categorical:
            encoder = RobustLabelEncoder()
            processed_X_train[col] = encoder.fit_transform(processed_X_train[col])
            processed_X_test[col] = encoder.transform(processed_X_test[col])
            categorical_encoders[col] = encoder
        
        print(f'Processed shapes - Train: {processed_X_train.shape}, Test: {processed_X_test.shape}')
        
        # Create data wrapper
        class DataWrapper:
            def __init__(self, data_dict): 
                self.__dict__.update(data_dict)
        
        data_wrapper = DataWrapper({
            'X_train': processed_X_train,
            'y_train': y_train,
            'X_test': processed_X_test,
            'y_test': y_test,
            'dataset_info': dataset_info,
            'algorithm': algorithm,
            'preprocessing_info': {
                'numeric_features': numeric_features,
                'categorical_features': regular_categorical,
                'timestamp_features': timestamp_features
            }
        })
        
        # Create preprocessing pipeline info
        preprocessing_info = {
            'numeric_features': numeric_features,
            'categorical_features': regular_categorical,
            'timestamp_features': timestamp_features,
            'target_column': target_col,
            'algorithm': algorithm
        }
        
        # Save outputs
        with open(out_path, 'wb') as f:
            pickle.dump(data_wrapper, f)
        
        with open(pipeline_path, 'wb') as f:
            pickle.dump(preprocessing_info, f)
        
        with open(weight_path, 'w') as f:
            json.dump(preprocessing_info, f, indent=2)
        
        print('Preprocessing complete!')
        print(f'Final shapes - Train: {processed_X_train.shape}, Test: {processed_X_test.shape}')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6" "$7"
    args:
      - {inputPath: train_data}
      - {inputPath: test_data}
      - {inputPath: dataset_info}
      - {inputValue: model_config}
      - {outputPath: processed_data_pickle}
      - {outputPath: preprocessing_pipeline}
      - {outputPath: weight_out}
